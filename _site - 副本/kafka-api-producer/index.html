<!DOCTYPE html>
<html xmlns:wb="http://open.weibo.com/wb">
<head>
    <!--   Author: NingG   -->
    <meta charset="utf-8" />
	<meta property="wb:webmaster" content="7ea7f3d5f3edad6a" />
    <title>Kafka 0.8.1：Producer API and Producer Configs | ningg.top</title>
    <meta name="author" content="NingG" />
    <meta name="renderer" content="webkit">
    <meta name="description" content=" Kafka是一个消息队列，那就要允许往其中存入数据，存入数据的这个实体，就是Kafka的Producer  " />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">

    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="" />
	
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
	<script src="http://tjs.sjs.sinajs.cn/open/api/js/wb.js" type="text/javascript" charset="utf-8"></script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
	
    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">NingG</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="http://weibo.com/ggningg/" target="_blank" style="margin-left:-5px;"><img src="http://www.weibo.com/favicon.ico" alt="" width="25"/></a>
            <a href="http://www.douban.com/people/ggningg/" target="_blank" style="text-align:center;"><img src="http://www.douban.com/favicon.ico" alt="" width="22"/></a>
			<a href="http://github.com/ningg" alt="" target="_blank" style="text-align:right;"><img src="/images/icon-cube-smaller.png" width="22"/></a>
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/kafka-api-producer" title="Kafka 0.8.1：Producer API and Producer Configs">Kafka 0.8.1：Producer API and Producer Configs</a></h1>
        <p class="entry-date">2014-11-07</p>
        <h2 id="section">背景</h2>

<p>最近在做Flume与Kafka的整合，其中用到了一个工程：<a href="https://github.com/thilinamb/flume-ng-kafka-sink">flume-ng-kafka-sink</a>，本质上就是Flume的一个插件：Kafka sink。遇到一个问题：Kafka sink通过设置kafak broker的<code>ip:port</code>来寻找broker，那就有一个问题，如果设置连接的kafka broker 宕掉了，flume的数据是不是就送不出去了？</p>

<h2 id="producer">Producer</h2>

<p>开始介绍Producer之前，说个小问题：上面<strong>背景</strong>中一直在说Flume的Sink：Kafka Sink，那与Kafka producer什么关系呢？为什么这次标题是<strong>Kafka Producer</strong>，而丝毫未提<strong>Flume Sink</strong>？这个问题很好，说明读者在思考，大概说几点：</p>

<ul>
  <li>Flume架构中有Sink，是用来将Flume收集到的数据送出去的；Flume下的Kafka Sink插件，在Flume看来，就是个Sink；</li>
  <li>Kafka架构中有Producer，是用来向Kafka broker中送入数据的；Flume下的Kafka Sink插件，在Kafka看来，就是个Producer；</li>
</ul>

<p>此次，主要站在Kafka的角度来看一个Producer可以进行的配置。</p>

<h3 id="kafka-producer-api">Kafka Producer API</h3>

<p>下面是<code>kafka.javaapi.producer.Producer</code>类的java API，实际上这个类是scala编写的，</p>

<pre><code>/**
 *  V: type of the message
 *  K: type of the optional key associated with the message
 */
 
class kafka.javaapi.producer.Producer&lt;K,V&gt; {

  public Producer(ProducerConfig config);

  /**
   * Sends the data to a single topic, partitioned by key, using either the
   * synchronous or the asynchronous producer
   * @param message the producer data object that encapsulates the topic, key and message data
   */
  public void send(KeyedMessage&lt;K,V&gt; message);

  /**
   * Use this API to send data to multiple topics
   * @param messages list of producer data objects that encapsulate the topic, key and message data
   */
  public void send(List&lt;KeyedMessage&lt;K,V&gt;&gt; messages);

  /**
   * Close API to close the producer pool connections to all Kafka brokers.
   */
  public void close();
}
</code></pre>

<p>具体如何使用上述Producer API，可参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example">0.8.0 Producer Example</a>。</p>

<h3 id="producer-example">0.8.0 Producer Example</h3>

<p>研究要深入，上面提到的<a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example">0.8.0 Producer Example</a>，下面简要介绍一下。</p>

<p>The Producer class is used to create new messages for a specific Topic and optional Partition.</p>

<p>If using Java you need to include a few packages for the Producer and supporting classes:</p>

<pre><code>import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
</code></pre>

<p>The first step in your code is to define properties for how the Producer finds the cluster, serializes the messages and if appropriate directs the message to a specific Partition.</p>

<p>代码本质体现的是逻辑，首先需要确定几个问题：</p>

<ul>
  <li>Producer如何找到Kafka Cluster；</li>
  <li>message传输的格式；（serialize，序列化）</li>
  <li>如何将message存入指定的Partition中；</li>
</ul>

<p>These properties are defined in the standard Java Properties object:</p>

<pre><code>Properties props = new Properties();
 
props.put("metadata.broker.list", "broker1:9092,broker2:9092");
props.put("serializer.class", "kafka.serializer.StringEncoder");
props.put("partitioner.class", "example.producer.SimplePartitioner");
props.put("request.required.acks", "1");
 
ProducerConfig config = new ProducerConfig(props);
</code></pre>

<p>The first property, “metadata.broker.list” defines where the Producer can find a one or more Brokers to determine the Leader for each topic. This does not need to be the full set of Brokers in your cluster but should include at least two in case the first Broker is not available. No need to worry about figuring out which Broker is the leader for the topic (and partition), the Producer knows how to connect to the Broker and ask for the meta data then connect to the correct Broker.</p>

<p><strong>第一项参数</strong><code>metadata.broker.list</code>，用于配置可用的broker列表，可以只配置一个broker，不过建议最好至少配置2个broker，这样即使有一个broker宕机了，另一个也能及时接替工作；这些broker中，也不用指定不同topic的leader，因为Producer会主动连接Broker并且请求到meta数据，然后连接到topic的leader。</p>

<p>The second property “serializer.class” defines what Serializer to use when preparing the message for transmission to the Broker. In our example we use a simple String encoder provided as part of Kafka. Note that the encoder must accept the same type as defined in the KeyedMessage object in the next step.</p>

<p><strong>第二项参数</strong><code>serializer.class</code>，设定了将message从Producer发送到Broker的序列化方式。</p>

<p><strong>notes(ningg)</strong>：“Note that the encoder must accept the same type as defined in the KeyedMessage object in the next step.” 什么含义？ <code>KeyedMessage</code>。</p>

<p>It is possible to change the Serializer for the Key (see below) of the message by defining “key.serializer.class” appropriately. By default it is set to the same value as “serializer.class”.</p>

<p>参数<code>key.serializer.class</code>用于设置key序列化的方法，key将在序列化之后，与message一同从Producer发送到Broker；<code>key.serializer.class</code>的默认值与<code>serializer.class</code>相同。</p>

<p><strong>notes(ningg)</strong>：Kafka是按照key进行partition的，每个message绑定的key也是需要传输到broker的，传输过程中也需要进行序列化，</p>

<p>The third property  “partitioner.class” defines what class to use to determine which Partition in the Topic the message is to be sent to. This is optional, but for any non-trivial implementation you are going to want to implement a partitioning scheme. More about the implementation of this class later. If you include a value for the key but haven’t defined a partitioner.class Kafka will use the default partitioner. If the key is null, then the Producer will assign the message to a random Partition.</p>

<p><strong>第三项参数</strong><code>partitioner.class</code>用于设定message与Partition的映射关系，简单来说，每个message都发送给broker的某个对应的Topic，但message真正存储对应的是Topic下的partition，那么，参数<code>partitioner.class</code>就是用于设定message–partition之间映射关系的。</p>

<p>The last property “request.required.acks” tells Kafka that you want your Producer to require an acknowledgement from the Broker that the message was received. Without this setting the Producer will ‘fire and forget’ possibly leading to data loss. Additional information can be found <a href="http://kafka.apache.org/08/configuration.html">here</a>.</p>

<p><strong>最后一项参数</strong><code>request.required.acks</code>，设定Broker在接收到message之后，是否返回一个确认信息（ack）。如果没有这个信息，那么很有可能<code>fire and forget</code>并且丢失数据。更多Kafka的相关配置信息，参考：<a href="http://kafka.apache.org/08/configuration.html">Kafka Configuration</a>。</p>

<p><strong>notes(ningg)</strong>：有个问题，即使Broker在接收到message之后，返回了ack信息，那Producer提供了重发机制吗？还是Producer只是进行登记？</p>

<p>Next you define the Producer object itself:</p>

<pre><code>Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config);
</code></pre>

<p>Note that the Producer is a Java Generic and you need to tell it the type of two parameters. The first is the type of the Partition key, the second the type of the message. In this example they are both Strings, which also matches to what we defined in the Properties above.</p>

<p><code>Producer</code>是一个Java Generic（泛型），需要输入两个参数，<code>&lt;String, String&gt;</code>，第一个参数是Partition key的类型，第二个是message的类型</p>

<p><strong>notes(ningg)</strong>：java中Generic的用法、注意事项有哪些？上面说的Partition key，到底指什么？是properties中的属性和属性值吗？不是的，查看源代码，Partition key就是按照key进行partition的key。</p>

<p>Now build your message:</p>

<pre><code>Random rnd = new Random();
long runtime = new Date().getTime();
String ip = “192.168.2.” + rnd.nextInt(255);
String msg = runtime + “,www.example.com,” + ip;
</code></pre>

<p>In this example we are faking a message for a website visit by IP address. First part of the comma-separated message is the timestamp of the event, the second is the website and the third is the IP address of the requester. We use the Java Random class here to make the last octet of the IP vary so we can see how Partitioning works.（上面<code>msg</code>中是伪造的一个网站访问记录）</p>

<p>Finally write the message to the Broker:</p>

<pre><code>KeyedMessage&lt;String, String&gt; data = new KeyedMessage&lt;String, String&gt;("page_visits", ip, msg);
 
producer.send(data);
</code></pre>

<p>The “page_visits” is the Topic to write to. Here we are passing the IP as the partition key. Note that if you do not include a key, even if you’ve defined a partitioner class, Kafka will assign the message to a random partition.</p>

<p><code>KeyedMessage&lt;String, String&gt;(topic, message)</code>或者<code>KeyedMessage&lt;String, String&gt;(topic, key, message)</code>，如果没输入key，那么即使设定了<code>partitioner.class</code>也不会对message分发到相应partition的，原因很简单，因为真的没有key。</p>

<p>Full Source:</p>

<pre><code>import java.util.*;
 
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
 
public class TestProducer {
	public static void main(String[] args) {
		long events = Long.parseLong(args[0]);
		Random rnd = new Random();
 
		Properties props = new Properties();
		props.put("metadata.broker.list", "broker1:9092,broker2:9092 ");
		props.put("serializer.class", "kafka.serializer.StringEncoder");
		props.put("partitioner.class", "example.producer.SimplePartitioner");
		props.put("request.required.acks", "1");
 
		ProducerConfig config = new ProducerConfig(props);
 
		Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config);
 
		for (long nEvents = 0; nEvents &lt; events; nEvents++) { 
			   long runtime = new Date().getTime();  
			   String ip = “192.168.2.” + rnd.nextInt(255); 
			   String msg = runtime + “,www.example.com,” + ip; 
			   KeyedMessage&lt;String, String&gt; data = new KeyedMessage&lt;String, String&gt;("page_visits", ip, msg);
			   producer.send(data);
		}
		producer.close();
	}
}
</code></pre>

<p>Partitioning Code:</p>

<pre><code>import kafka.producer.Partitioner;
import kafka.utils.VerifiableProperties;
 
public class SimplePartitioner implements Partitioner {
	public SimplePartitioner (VerifiableProperties props) {
 
	}
 
	public int partition(Object key, int a_numPartitions) {
		int partition = 0;
		String stringKey = (String) key;
		int offset = stringKey.lastIndexOf('.');
		if (offset &gt; 0) {
		   partition = Integer.parseInt( stringKey.substring(offset+1)) % a_numPartitions;
		}
	   return partition;
  }
 
}
</code></pre>

<p>The logic takes the key, which we expect to be the IP address, finds the last octet and does a modulo operation on the number of partitions defined within Kafka for the topic. The benefit of this partitioning logic is all web visits from the same source IP end up in the same Partition. Of course so do other IPs, but your consumer logic will need to know how to handle that.
（将有时间顺序的message放到同一个partition中）</p>

<p>Before running this, make sure you have created the Topic page_visits. From the command line:</p>

<pre><code>bin/kafka-create-topic.sh --topic page_visits --replica 3 --zookeeper localhost:2181 --partition 5
</code></pre>

<p>Make sure you include a <code>--partition</code> option so you create more than one.
（要使用<code>--partition</code>来创建多个partition，否则可能只有一个）</p>

<p>Now compile and run your Producer and data will be written to Kafka.</p>

<p>To confirm you have data, use the command line tool to see what was written:</p>

<pre><code>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic page_visits --from-beginning
</code></pre>

<p>利用Maven进行Producer开发时，需要添加的POM配置如下：</p>

<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;kafka_2.9.2&lt;/artifactId&gt;
  &lt;version&gt;0.8.1.1&lt;/version&gt;
  &lt;scope&gt;compile&lt;/scope&gt;
  &lt;exclusions&gt;
	&lt;exclusion&gt;
	  &lt;artifactId&gt;jmxri&lt;/artifactId&gt;
	  &lt;groupId&gt;com.sun.jmx&lt;/groupId&gt;
	&lt;/exclusion&gt;
	&lt;exclusion&gt;
	  &lt;artifactId&gt;jms&lt;/artifactId&gt;
	  &lt;groupId&gt;javax.jms&lt;/groupId&gt;
	&lt;/exclusion&gt;
	&lt;exclusion&gt;
	  &lt;artifactId&gt;jmxtools&lt;/artifactId&gt;
	  &lt;groupId&gt;com.sun.jdmk&lt;/groupId&gt;
	&lt;/exclusion&gt;
  &lt;/exclusions&gt;
&lt;/dependency&gt;
</code></pre>

<h3 id="section-1">几个情况</h3>

<p><strong>思考1</strong>：Kafka 0.7.2版本中，直接在Producer中配置Zookeeper，Producer通过Zookeeper来获知Broker的位置，简单来说，应用与Kafka之间是解耦的，可以在不修改Producer信息的情况下，动态增减Broker。</p>

<p>当前，通过<code>metadata.broker.list</code>来设置broker的列表，有几个问题，稍微梳理一下：</p>

<ul>
  <li>如果只在<code>metadata.broker.list</code>中配置一个broker，那么Producer能够识别出其他broker吗？</li>
  <li>如果能够识别出未配置的broker，那么，只配置一个broker不就行了吗？</li>
  <li>如果不能识别出未配置的broker，那Kafka集群中动态增加了broker，岂不是需要重新启动flume？（因为<code>metadata.broker.list</code>实际上是flume的配置，要更新这一参数配置，就需要重启flume）</li>
</ul>

<p><strong>思考2</strong>：如果network interrupt，producer会如何动作？记录log？还是抛出异常？</p>

<p><strong>思考3</strong>：如果某一个flume挂了，能否能自动重启？</p>

<h2 id="producer-1">Producer配置的详细参数</h2>

<p>针对Kafka 0.8.1版本，这一部分介绍的Producer配置信息，主要参考两个地方：</p>

<ul>
  <li><a href="http://kafka.apache.org/08/configuration.html">Kafka Configuration</a></li>
  <li><a href="http://kafka.apache.org/documentation.html#producerconfigs">Kafka Producer Config</a></li>
</ul>

<p>Essential configuration properties for the producer include:（Producer必须的参数有几个，如下）</p>

<ul>
  <li>metadata.broker.list，broker列表；</li>
  <li>request.required.acks，broker收到producer发来的message后，是否ack？</li>
  <li>producer.type，这个什么滴干活？</li>
  <li>serializer.class，message从producer发往broker的过程中，需要序列化；</li>
</ul>

<p>具体参数如下：</p>

<ul>
  <li>Property
    <ul>
      <li>Default</li>
      <li>Description</li>
    </ul>
  </li>
  <li>metadata.broker.list
    <ul>
      <li>null</li>
      <li>This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas). The socket connections for sending the actual data will be established based on the broker information returned in the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a subset of brokers.</li>
    </ul>
  </li>
  <li>request.required.acks
    <ul>
      <li>0	</li>
      <li>This value controls when a produce request is considered completed. Specifically, how many other brokers must have committed the data to their log and acknowledged this to the leader? Typical values are</li>
      <li>0, which means that the producer never waits for an acknowledgement from the broker (the same behavior as 0.7). This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails).（不等待ack信息）</li>
      <li>1, which means that the producer gets an acknowledgement after the leader replica has received the data. This option provides better durability as the client waits until the server acknowledges the request as successful (only messages that were written to the now-dead leader but not yet replicated will be lost).（leader完成数据写入）</li>
      <li>-1, which means that the producer gets an acknowledgement after all in-sync replicas have received the data. This option provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.（所有replica都完成数据写入）</li>
    </ul>
  </li>
  <li>request.timeout.ms
    <ul>
      <li>10000</li>
      <li>The amount of time the broker will wait trying to meet the <code>request.required.acks</code> requirement before sending back an error to the client.（broker收到producer发来的message后，如果需要返回ack信息，那这个参数设定了broker返回ack信息的时间限制，如果超过这个时间，则broker向producer返回一个error信息）</li>
    </ul>
  </li>
  <li>producer.type	
    <ul>
      <li>sync</li>
      <li>This parameter specifies whether the messages are sent asynchronously in a background thread. Valid values are (1) <code>async</code> for asynchronous send and (2) <code>sync</code> for synchronous send. By setting the producer to async we allow batching together of requests (which is great for throughput) but open the possibility of a failure of the client machine dropping unsent data.（producer发送message的方式：同步、异步；设置为异步时，producer处理的吞吐量会提升，但可能丢失数据）</li>
    </ul>
  </li>
  <li>serializer.class	
    <ul>
      <li>kafka.serializer.DefaultEncoder</li>
      <li>The serializer class for messages. The default encoder takes a <code>byte[]</code> and returns the same <code>byte[]</code>.（message的序列化方法，默认是<code>byte[]</code>）</li>
    </ul>
  </li>
  <li>key.serializer.class
    <ul>
      <li>defaults to the same as for messages if nothing is given.</li>
      <li>The serializer class for keys .</li>
    </ul>
  </li>
  <li>partitioner.class
    <ul>
      <li>kafka.producer.DefaultPartitioner（The default partitioner is based on the hash of the key.）</li>
      <li>The partitioner class for partitioning messages amongst sub-topics. （将message放入哪个partition中）</li>
    </ul>
  </li>
  <li>compression.codec	
    <ul>
      <li>none	</li>
      <li>This parameter allows you to specify the compression codec for all data generated by this producer. Valid values are “none”, “gzip” and “snappy”.（producer向broker发送的信息，是否进行压缩，包含：key、message信息。）</li>
    </ul>
  </li>
  <li>compressed.topics	
    <ul>
      <li>null</li>
      <li>This parameter allows you to set whether compression should be turned on for particular topics. If the compression codec is anything other than NoCompressionCodec, enable compression only for specified topics if any. If the list of compressed topics is empty, then enable the specified compression codec for all topics. If the compression codec is NoCompressionCodec, compression is disabled for all topics（当开启<code>compression.codec</code>时，通过设置<code>compressed.topics</code>，设置只针对某些特定的topic进行压缩，默认，对所有的topic都进行压缩）</li>
    </ul>
  </li>
  <li>message.send.max.retries
    <ul>
      <li>3	</li>
      <li>This property will cause the producer to automatically retry a failed send request. This property specifies the number of retries when such failures occur. Note that setting a non-zero value here can lead to duplicates in the case of network errors that cause a message to be sent but the acknowledgement to be lost.（当producer发送message失败后，尝试重新发送的次数；<strong>特别说明</strong>：如果message发送成功，但broker返回的ack信息丢失时，会有message重发，即，此处有消息重复发送）</li>
    </ul>
  </li>
  <li>retry.backoff.ms
    <ul>
      <li>100</li>
      <li>Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.（producer在进行重新发送message之前，都会refresh metadata，主要目标，查看是否更新了topic的leader；因为leader election需要一段时间，因此，在refresh metadata之前，需要等待一段时间，<code>retry.backoff.ms</code>参数设置的就是等待的时间，等待选出新的topic leader）</li>
    </ul>
  </li>
  <li>topic.metadata.refresh.interval.ms	
    <ul>
      <li>600 * 1000	</li>
      <li>The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). （正常情况，多长时间刷新一次broker metadata，即，刷新间隔）</li>
      <li>If you set this to a <code>negative value</code>, metadata will only get refreshed on failure. （<code>&lt;0</code>时，仅当发送message失败时，才刷新）</li>
      <li>If you set this to zero, the metadata will get refreshed after each message sent (not recommended). （<code>0</code>，每次发送完message之后，都刷新，<strong>不推荐</strong>）</li>
      <li>Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed（<strong>重要提示</strong>：无论设置刷新间隔为多少，具体刷新metadata都发生在producer发送message之后，因此，如果一直没有message发送，就不会有metadata刷新）</li>
    </ul>
  </li>
  <li>queue.buffering.max.ms
    <ul>
      <li>5000</li>
      <li>Maximum time to buffer data when using <code>async</code> mode. For example a setting of 100 will try to batch together 100ms of messages to send at once. This will improve throughput but adds message delivery latency due to the buffering.（当使用<code>producer.type</code>为async模式时，这一参数才有用，含义：一时间为单位，将这一时间单位内的message一起发送给broker，这样有利于提高throughput，但会增加时延。）</li>
    </ul>
  </li>
  <li>queue.buffering.max.messages
    <ul>
      <li>10000</li>
      <li>The maximum number of unsent messages that can be queued up the producer when using async mode before either the producer must be blocked or data must be dropped.（当<code>producer.type</code>使用async时，producer能够缓存的unsent message的数量，如果超过这一数量，producer就会blocked？message就会被丢弃？具体什么情况？）</li>
    </ul>
  </li>
  <li>queue.enqueue.timeout.ms	
    <ul>
      <li>-1	</li>
      <li>The amount of time to block before dropping messages when running in async mode and the buffer has reached queue.buffering.max.messages. （当<code>queue.buffering.max.message</code>设定的值已经触顶，等待多久block，之后就开始丢弃message）</li>
      <li>If set to 0 events will be enqueued immediately or dropped if the queue is full (the producer send call will never block). </li>
      <li>If set to -1 the producer will block indefinitely and never willingly drop a send.</li>
    </ul>
  </li>
  <li>batch.num.messages
    <ul>
      <li>200</li>
      <li>The number of messages to send in one batch when using async mode. The producer will wait until either this number of messages are ready to send or queue.buffer.max.ms is reached.（在<code>async</code>模式下，当message数量达到<code>batch.num.messages</code>时，或者，当等待时间达到<code>queue.buffer.max.ms</code>时，producer都会发送一次缓存的message）</li>
    </ul>
  </li>
  <li>send.buffer.bytes
    <ul>
      <li>100 * 1024</li>
      <li>Socket write buffer size（socket写缓存的大小）</li>
    </ul>
  </li>
  <li>client.id	
    <ul>
      <li>””</li>
      <li>The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.（用户自己定义的producer标识，会伴随发送的message一起发送，用于追踪message的来源）</li>
    </ul>
  </li>
</ul>

<p>More details about producer configuration can be found in the scala class <code>kafka.producer.ProducerConfig</code>.</p>

<p><strong>notes(ningg)</strong>：几个新的理解：</p>

<ul>
  <li>metadata.broker.list：本质是从一些broker中请求metadata（topic、partition、replicas），而真正的socket链接，是根据收到的metadata来进行的；因此，可以只配置一部分的broker，或者说只配置部分VIP broker，必要时，探查此深层的原因；每一个broker上都保存了整个Kafka cluster的完整metadata吗？</li>
  <li>producer.type：sync、async，当设置为async时，能够提升吞吐量，但是会丢失数据？丢失，不能重发吗？</li>
  <li>request.required.acks：设置是否需要broker在完成数据写入后，向producer返回ack信息；一个问题：如果broker上数据写入失败，那，producer会进行重发吗？有没有类似的机制？</li>
  <li>queue.enqueue.timeout.me：其中说明的producer block什么含义？还会继续缓存未发送的message吗？</li>
</ul>

<p><strong>疑问</strong>：突然想到一个问题，记录一下：</p>

<ul>
  <li>Server上，有进程监听port后，在服务器上无法再启动一个进程来监听这一port；</li>
  <li>在远端通过telnet能够与server上这一port建立连接，并且，多个client都能与server上这一port建立连接；</li>
</ul>

<p>这个问题，我不知到深层原因，归根结底是对socket建立的底层原因不清晰。</p>

<h2 id="new-producer-configs">New Producer Configs（补充）</h2>

<p>下面是今后Kafka Producer会采用的新的配置参数，当前，可以有一个基本的了解。</p>

<p>We are working on a replacement for our existing producer. The code is available in trunk now and can be considered beta quality. Below is the configuration for the new producer.</p>

<ul>
  <li>Name
    <ul>
      <li>Type</li>
      <li>Default</li>
      <li>Importance</li>
      <li>Description</li>
    </ul>
  </li>
  <li>bootstrap.servers
    <ul>
      <li>list</li>
      <li>null</li>
      <li>high</li>
      <li>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. Data will be load balanced over all servers irrespective of which servers are specified here for bootstrapping—this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form host1:port1,host2:port2,…. Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down). If no server in this list is available sending data will fail until on becomes available.</li>
    </ul>
  </li>
  <li>acks
    <ul>
      <li>string</li>
      <li>1</li>
      <li>high</li>
      <li>The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are common:</li>
      <li><code>acks=0</code> If set to zero then the producer will not wait for any acknowledgment from the server at all. The record will be immediately added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won’t generally know of any failures). The offset given back for each record will always be set to -1.</li>
      <li><code>acks=1</code> This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.（leader将message写入local log后，直接返回ack信息；如果leader，返回ack信息后，leader宕机了，那其他follwer上并没有这条message，将导致message丢失）</li>
      <li><code>acks=all</code> This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.</li>
      <li>Other settings such as acks=2 are also possible, and will require the given number of acknowledgements but this is generally less useful.</li>
    </ul>
  </li>
  <li>buffer.memory
    <ul>
      <li>long</li>
      <li>33554432</li>
      <li>high</li>
      <li>The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will either block or throw an exception based on the preference specified by <code>block.on.buffer.full</code>.（用于存储未发送出去的message，当producer接收到的message速度大于发送message速度时，producer will block，或者抛出异常）</li>
      <li>This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests.（<strong>什么含义</strong>？需仔细琢磨）</li>
    </ul>
  </li>
  <li>compression.type
    <ul>
      <li>string</li>
      <li>none</li>
      <li>high</li>
      <li>The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, or snappy. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).</li>
    </ul>
  </li>
  <li>retries
    <ul>
      <li>int</li>
      <li>0</li>
      <li>high</li>
      <li>Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries will potentially change the ordering of records because if two records are sent to a single partition, and the first fails and is retried but the second succeeds, then the second record may appear first.（设定，message尝试重发的次数；这个重发机制，可能会改变message之间的相互顺序）</li>
    </ul>
  </li>
  <li>batch.size
    <ul>
      <li>int</li>
      <li>16384</li>
      <li>medium</li>
      <li>The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes.（将发送到同一partition的多条message集中起来发送，构成一个batch）</li>
      <li>No attempt will be made to batch records larger than this size.</li>
      <li>Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent.（发送给broker的request包含多个batch？每一个batch对应一个partition）</li>
      <li>A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.</li>
    </ul>
  </li>
  <li>client.id
    <ul>
      <li>string</li>
      <li>null</li>
      <li>medium</li>
      <li>The id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included with the request. The application can set any string it wants as this has no functional purpose other than in logging and metrics.</li>
    </ul>
  </li>
  <li>linger.ms
    <ul>
      <li>long</li>
      <li>0</li>
      <li>medium</li>
      <li>The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay—that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle’s algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will ‘linger’ for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absense of load.（当producer收到一个message后，不直接发送出去，而是，等待<code>linger.ms</code>时间，目的：相同partition的多个message同时发送。）</li>
    </ul>
  </li>
  <li>max.request.size
    <ul>
      <li>int</li>
      <li>1048576</li>
      <li>medium</li>
      <li>The maximum size of a request. This is also effectively a cap on the maximum record size. Note that the server has its own cap on record size which may be different from this. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.（限制单个request的大小）</li>
    </ul>
  </li>
  <li>receive.buffer.bytes
    <ul>
      <li>int</li>
      <li>32768</li>
      <li>medium</li>
      <li>The size of the TCP receive buffer to use when reading data（上层读取TCP数据时，一次读取的缓冲单元？）</li>
    </ul>
  </li>
  <li>send.buffer.bytes
    <ul>
      <li>int</li>
      <li>131072</li>
      <li>medium</li>
      <li>The size of the TCP send buffer to use when sending data</li>
    </ul>
  </li>
  <li>timeout.ms
    <ul>
      <li>int</li>
      <li>30000</li>
      <li>medium</li>
      <li>The configuration controls the maximum amount of time the server will wait for acknowledgments from followers to meet the acknowledgment requirements the producer has specified with the acks configuration. If the requested number of acknowledgments are not met when the timeout elapses an error will be returned. This timeout is measured on the server side and does not include the network latency of the request.（server等待follower返回ack信息的时间，这个时间是指server端的时间）</li>
    </ul>
  </li>
  <li>block.on.buffer.full
    <ul>
      <li>boolean</li>
      <li>true</li>
      <li>low</li>
      <li>When our memory buffer is exhausted we must either stop accepting new records (block) or throw errors. By default this setting is true and we block, however in some scenarios blocking is not desirable and it is better to immediately give an error. Setting this to false will accomplish that: the producer will throw a BufferExhaustedException if a recrord is sent and the buffer space is full.（默认<code>true</code>，当memory buffer中内容满了之后，producer不再接收新的message；如果设置为<code>false</code>，则当memory buffer中内容满了之后，producer会直接抛出异常<code>BufferExhaustedException</code>）</li>
    </ul>
  </li>
  <li>metadata.fetch.timeout.ms
    <ul>
      <li>long</li>
      <li>60000</li>
      <li>low</li>
      <li>The first time data is sent to a topic we must fetch metadata about that topic to know which servers host the topic’s partitions. This configuration controls the maximum amount of time we will block waiting for the metadata fetch to succeed before throwing an exception back to the client.（当第一次向topic中传入数据时，需要从server请求metadata，参数<code>metadata.fetch.timeout.ms</code>设定了发送metadata请求后，producer等待的时间，如果超时，则抛出异常。）</li>
    </ul>
  </li>
  <li>metadata.max.age.ms
    <ul>
      <li>long</li>
      <li>300000</li>
      <li>low</li>
      <li>The period of time in milliseconds after which we force a refresh of metadata even if we haven’t seen any partition leadership changes to proactively discover any new brokers or partitions.（定期请求metadata的时常）</li>
    </ul>
  </li>
  <li>metric.reporters
    <ul>
      <li>list</li>
      <li><code>[]</code></li>
      <li>low</li>
      <li>A list of classes to use as metrics reporters. Implementing the <code>MetricReporter</code> interface allows plugging in classes that will be notified of new metric creation. The <code>JmxReporter</code> is always included to register <code>JMX</code> statistics.（<strong>什么含义</strong>？生成测试报告？测试什么？为什么要测试？）</li>
    </ul>
  </li>
  <li>metrics.num.samples
    <ul>
      <li>int</li>
      <li>2</li>
      <li>low</li>
      <li>The number of samples maintained to compute metrics.（计算指标时，保留的samples的个数）</li>
    </ul>
  </li>
  <li>metrics.sample.window.ms
    <ul>
      <li>long</li>
      <li>30000</li>
      <li>low</li>
      <li>The metrics system maintains a configurable number of samples over a fixed window size. This configuration controls the size of the window. For example we might maintain two samples each measured over a 30 second period. When a window expires we erase and overwrite the oldest window.（选出sample的window大小）</li>
    </ul>
  </li>
  <li>reconnect.backoff.ms
    <ul>
      <li>long</li>
      <li>10</li>
      <li>low</li>
      <li>The amount of time to wait before attempting to reconnect to a given host when a connection fails. This avoids a scenario where the client repeatedly attempts to connect to a host in a tight loop.（当与一个host断开连接后，等待多长时间，再去进行连接，避免过于频繁的无效连接）</li>
    </ul>
  </li>
  <li>retry.backoff.ms
    <ul>
      <li>long</li>
      <li>100</li>
      <li>low</li>
      <li>The amount of time to wait before attempting to retry a failed produce request to a given topic partition. This avoids repeated sending-and-failing in a tight loop.</li>
    </ul>
  </li>
</ul>

<p><strong>notes(ningg)</strong>：<code>metrics</code>的含义？为什么有这个？干什么的？</p>

<h2 id="section-2">参考来源</h2>

<ul>
  <li><a href="https://github.com/thilinamb/flume-ng-kafka-sink">flume-ng-kafka-sink</a></li>
  <li><a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example">0.8.0 Producer Example</a></li>
  <li><a href="http://kafka.apache.org/08/configuration.html">Kafka Configuration</a></li>
</ul>

<h2 id="section-3">杂谈</h2>

<p>人是有差异的，特别是视野上的差异，有些东西，如果一个人没有见识过，同时想象力也不行，或者说胆小如鼠不敢想象，这样的人脑袋不行、胆子也不行，年轻人脑袋行不行，至少胆子要大；另，信任是金子，别人对我的绝对信任，我对别人的绝对信任，都是很难建立的，要如同珍惜脑袋一样，珍惜这些信任。（注：绝对信任：无论做什么事，都相信是在做一件值得做的事，无论怎样，都是信任，甚至当有流言蜚语产生时，都能力排众议对其信任。这种信任，大都是建立在对人格的熟知上。）</p>

<p>整理东西，突然想到：做一件事，怎样才能做成？做事情需要几个条件：</p>

<ul>
  <li>做事的方向对不对？</li>
  <li>做事的人脑袋是否灵光？</li>
  <li>做事的人，是否投入了足够的时间？</li>
</ul>

<p>针对上面的几点，大概说一下：</p>

<ul>
  <li><strong>做事的方向对不对</strong>，只要针对做的事情，当前能够达到基本一致，方向基本确定，而不是一团乱麻，那就可以开始做下去了，而在后期的过程中，可能会涉及到多次调整、迭代，这些都是可以预见的；</li>
  <li><strong>做事的人脑袋是否灵光</strong>，事情是由人来做的，做的人脑袋行不行？基础理论、基本操作技能，基本的世界观：劳动创造价值，获得报酬；还是，跟着大牛有肉吃？（这本质是希望拿牛人的劳动价值，换取自己的报酬）</li>
  <li><strong>做事的人时间投入</strong>，天资尚可的人就行了，团队高低档次的人都需要，但是，有一点，如果不投入时间，或者时间很少，那又如何保证产出？特别是针对以前就没有涉足的领域，需要不畏艰险、持续的投入时间，才能有所理解、有所深入；脑袋还可以，但做事不投入充足时间的人，这个就不太好。</li>
</ul>

<p>今天突然想起一件事，几年前，跟某位好友一起走路，无意间说起坚韧这种性格，我就问道：如果要在午门城墙上打一个洞，如何才能做到？谈到锲而不舍，如果一个人没有这种精神，那遇到困难的事情，就难办了；后来又说起，今后工作的打算，我们基本达成一致：精挑细选公司，一旦入门后，就当自己是公司的创始人，然后，返老还童，恢复到20多岁年轻小伙儿的年纪 ，只不过，返老还童的代价是放弃对于公司的所有权、职务等，以这种心态去工作，重塑自己的公司、再造辉煌，可以说想象还是比较大胆的；基于这种定位，每次做事，都是创始人心态，全力做好。<em>（每次整理blog都会到夜里12点才睡，略累呀，如果下班就走，那就有时间了，只不过工作要做好，要投入时间，下班很难按时走，权衡吧）</em></p>



        <div id="disqus_container">
            <div style="margin-bottom:20px" class="right">
				<wb:share-button appkey="2306590132" addition="simple" type="button" ralateUid="2356527183"></wb:share-button>
            </div>
            <!--<a href="#" class="comment" onclick="return false;">点击查看评论</a>-->
            <div id="disqus_thread"></div>
        </div>
    </div>

	
    <div class="sidenav">
		<iframe width="100%" height="75" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=0&skin=1&isTitle=0&noborder=0&isWeibo=0&isFans=0&uid=2356527183&verifier=1ccfb61e&colors=d6f3f7,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe>
    </div>

</div>

<script src="/js/post.js" type="text/javascript"></script>


	<div id="gotoTop">Top</div>


    <!--*********************************************************-->
    <!--****** 宝贝儿，看见这个时候，删掉下面的统计代码哦~ ******-->
    <!--*********************************************************-->
	<script>
	  (function(i,s,o,g,r,a,m){
		  i['GoogleAnalyticsObject']=r;i[r]=i[r] || function(){
			(i[r].q=i[r].q||[]).push(arguments)}, i[r].l=1*new Date();
			a=s.createElement(o), m=s.getElementsByTagName(o)[0];
			a.async=1;
			a.src=g;
			m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-48535193-1', 'ningg.github.io');
	  ga('send', 'pageview');
	</script>
    
	
    <script type="text/javascript">
		function gotoTop(min_height){
			$("#gotoTop").click(
				function(){$('html,body').animate({scrollTop:0},700);
			}).hover(
				function(){$(this).addClass("hover");},
				function(){$(this).removeClass("hover");
			});
			min_height ? min_height = min_height : min_height = 600;
			$(window).scroll(function(){
				
				var s = $(window).scrollTop();
				
				if( s > min_height){
					$("#gotoTop").fadeIn(100);
				}else{
					$("#gotoTop").fadeOut(200);
				};
			});
		};
		gotoTop();

        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })

    </script>
</body>
</html>
