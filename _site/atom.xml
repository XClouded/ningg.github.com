<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>NingG.github.com</title>
   <link href="http://ningg.github.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://ningg.github.com" rel="alternate" type="text/html" />
   <updated>2014-10-18T23:26:33+08:00</updated>
   <id>http://ningg.github.com</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>Kafka 0.8.1 Documentation：Design</title>
     <link href="http://ningg.github.com/kafka-documentation-design"/>
     <updated>2014-10-18T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-documentation-design</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1 Documentation：Getting Started</title>
     <link href="http://ningg.github.com/kafka-documentation"/>
     <updated>2014-10-18T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-documentation</id>
     <content type="html">&lt;h2 id=&quot;getting-started&quot;&gt;1. Getting Started&lt;/h2&gt;

&lt;h3 id=&quot;introduction&quot;&gt;1.1 Introduction&lt;/h3&gt;

&lt;p&gt;Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：distributed, partitioned, replicated commit log service?&lt;/p&gt;

&lt;p&gt;First let’s review some basic messaging terminology:（几个messaging概念）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka maintains feeds of messages in categories called topics.（按topic来分类message？）&lt;/li&gt;
  &lt;li&gt;We’ll call processes that publish messages to a Kafka topic producers.（调用process，向topic producer中写message）&lt;/li&gt;
  &lt;li&gt;We’ll call processes that subscribe to topics and process the feed of published messages consumers..&lt;/li&gt;
  &lt;li&gt;Kafka is run as a cluster comprised of one or more servers each of which is called a broker.（kafka集群由borker构成）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:（producer向kafka集群写入message，consumer从kafka集群中读取message）&lt;/p&gt;

&lt;p&gt;![/images/kafa-documentation/producer_consumer.png]&lt;/p&gt;

&lt;p&gt;Communication between the clients and the servers is done with a simple, high-performance, language agnostic &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol&quot;&gt;TCP protocol&lt;/a&gt;. We provide a Java client for Kafka, but clients are available in &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Clients&quot;&gt;many languages&lt;/a&gt;.（client与server之间通过TCP协议通信，默认为kafka提供了java client，当然也可以用其他语言实现client）&lt;/p&gt;

&lt;h4 id=&quot;topics-and-logs&quot;&gt;Topics and Logs&lt;/h4&gt;

&lt;p&gt;A topic is a category or feed name to which messages are published. For each topic, the Kafka cluster maintains a partitioned log that looks like this:（topic，就是category、feed name，message按此分开存放；每个topic，对应一个partitioned log）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/log_anatomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each partition is an ordered, immutable sequence of messages that is continually appended to—a commit log. The messages in the partitions are each assigned a sequential id number called the offset that uniquely identifies each message within the partition.（partition是ordered、immutable sequence of message，其中的message被唯一标识，partition对应 a commit log）&lt;/p&gt;

&lt;p&gt;The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. For example if the log retention is set to two days, then for the two days after a message is published it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so retaining lots of data is not a problem.（在一段可配置的时间内，kafka始终保存所有的published messages，即使message已经被consume；Kafka对data size不敏感，lots of data对performance造成太大影响）&lt;/p&gt;

&lt;p&gt;In fact the only metadata retained on a per-consumer basis is the position of the consumer in the log, called the “offset”. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages, but in fact the position is controlled by the consumer and it can consume messages in any order it likes. For example a consumer can reset to an older offset to reprocess.
（on a per-consumer basis，只需保存元数据：consumer在log中的position，即，offset；这个offset完全由consumer自己决定，offset默认是顺序递增，但实际上consumer可以任意调整。）&lt;/p&gt;

&lt;p&gt;This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers.（总之，consumer在kafka中非常cheap：随意的come and go，对系统影响很小，consumer相互之间的影响也很小）&lt;/p&gt;

&lt;p&gt;The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit.（对log分partition，有几点目的：1.single server支撑较大的log，单个partition受到server的限制，但partition的数量不受限；2.多partition可以支撑并发处理，每个partition作为一个unit。）&lt;/p&gt;

&lt;h4 id=&quot;distribution&quot;&gt;Distribution&lt;/h4&gt;

&lt;p&gt;The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.
（partition分布式存储，方便共享；同时可配置每个patition的复制份数，以提升系统可靠性）&lt;/p&gt;

&lt;p&gt;Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.
（每个partition都对应一个server担当”leader”角色，也可能有其他server担当”follower”角色；leader负责所有的Read、write；follower只replicate the leader；如果leader崩溃，则自动推选一个follower升级为leader；server只对其上的部分partition充当leader角色，方便cluster的均衡。）&lt;/p&gt;

&lt;h4 id=&quot;producers&quot;&gt;Producers&lt;/h4&gt;

&lt;p&gt;Producers publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the message). More on the use of partitioning in a second.
（producer复制将message分发到相应的topic，具体：1.将message分发到哪个topic的哪个partition，常用方式，轮询、函数；）&lt;/p&gt;

&lt;h4 id=&quot;consumers&quot;&gt;Consumers&lt;/h4&gt;

&lt;p&gt;Messaging traditionally has two models: &lt;a href=&quot;http://en.wikipedia.org/wiki/Message_queue&quot;&gt;queuing&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern&quot;&gt;publish-subscribe&lt;/a&gt;. In a queue, a pool of consumers may read from a server and each message goes to one of them; in publish-subscribe the message is broadcast to all consumers. Kafka offers a single consumer abstraction that generalizes both of these—the consumer group.
（messaging，消息发送，由两种方式：queuing、publish-subscribe。Queuing，message发送到某一个consumer；publish-subscribe，message广播到所有的consumers。Kafka，通过将consumer泛化为consumer group，来支持这两种方式）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：publish-subscribe，发布-订阅模式的含义？&lt;/p&gt;

&lt;p&gt;Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.
（每个consumer都标记有consumer group name，每个message都被分发给subscribing consumer group中的一个consumer instance，consumer instances可以是不同的进程，也可以分布在不同的物理机器上。）&lt;/p&gt;

&lt;p&gt;If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.（若所有的consumer都属于同一个consumer group，则，情况变为：queue的负载均衡？）&lt;/p&gt;

&lt;p&gt;If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.
（若所有的consumer都属不同的consumer group，则，情况变为：publish-subscribe，message广播发送到所有consumer）&lt;/p&gt;

&lt;p&gt;More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is cluster of consumers instead of a single process.
（topics只对应少数的consumer group，即，consumer group类似&lt;code&gt;logical subscriber&lt;/code&gt;；每个group中有多个consumer，目的是提升可扩展性、容错能力）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：consumer group下有多个consumer？这些consumer怎么调用的？相互之间有什么差异？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/consumer-groups.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kafka has stronger ordering guarantees than a traditional messaging system, too.（kafka有strong ordering guarantees）&lt;/p&gt;

&lt;p&gt;A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored. However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the messages is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.
（message queue中存放的message，按照顺序发送到不同的consumers，但是这些发送是异步的，因此，后发送的message可能先到达consumer，即，并行处理时，有可能message乱序。现有的Messaging system，常用&lt;code&gt;exclusive consumer&lt;/code&gt;，独占消费，仅仅启动一个process来读取一个queue中的数据，此时，就无法实现并行处理。）&lt;/p&gt;

&lt;p&gt;Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances than partitions.
（Kafka，采用&lt;code&gt;partition&lt;/code&gt;的方式解决上述问题：每个partition被指定给topic对应的consumer group中的特定的consumer，这样能保证一点：一个partition中的message被顺序处理。由于有多个partition，并且对应多个consumer instance来处理，从而实现负载均衡；特别注意：consumer instance个数不能多于partitions个数）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message是怎么分配到topic对应的partition中的？consumer instance为什么不能多于partition个数？&lt;/p&gt;

&lt;p&gt;Kafka only provides a total order over messages within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process.
（Kafka只保证partition内mesaage的顺序处理，不保证partition之间的处理顺序。per-partition ordering和partition data by key，满足了大部分需求。如果要保证所有message顺序处理，则，将topic设置为only one partition，此时，变为串行处理。）&lt;/p&gt;

&lt;h4 id=&quot;guarantees&quot;&gt;Guarantees&lt;/h4&gt;

&lt;p&gt;At a high-level Kafka gives the following guarantees:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.（同一个producer发送到a particular topic partition的message，保证在partition中是有序的）&lt;/li&gt;
  &lt;li&gt;A consumer instance sees messages in the order they are stored in the log.（partition对应的commit log中message是有序的）&lt;/li&gt;
  &lt;li&gt;For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.（复制N份的topic，保证N-1份都丢失的情况下能够恢复。）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details on these guarantees are given in the design section of the documentation.&lt;/p&gt;

&lt;h3 id=&quot;use-cases&quot;&gt;1.2 Use Cases&lt;/h3&gt;

&lt;p&gt;Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see &lt;a href=&quot;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;this blog post&lt;/a&gt;.
（使用Kafka的典型场景，详细应用参考&lt;a href=&quot;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;this blog post&lt;/a&gt;）&lt;/p&gt;

&lt;h4 id=&quot;messaging&quot;&gt;Messaging&lt;/h4&gt;

&lt;p&gt;Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.
（替换传统的message broker/消息代理，其基本用途：解耦processing和data producer，缓存message，etc。）&lt;/p&gt;

&lt;p&gt;In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.
（实验发现messaging过程中，对broker的吞吐量要求不高，但要求低延迟、高可靠，这些kafka都满足。）&lt;/p&gt;

&lt;p&gt;In this domain Kafka is comparable to traditional messaging systems such as &lt;a href=&quot;http://activemq.apache.org/&quot;&gt;ActiveMQ&lt;/a&gt; or &lt;a href=&quot;https://www.rabbitmq.com/&quot;&gt;RabbitMQ&lt;/a&gt;.
（在messaging方面，Kafka的性能可与ActiveMQ、RabbitMQ相匹敌。）&lt;/p&gt;

&lt;h4 id=&quot;website-activity-tracking&quot;&gt;Website Activity Tracking&lt;/h4&gt;

&lt;p&gt;The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.&lt;/p&gt;

&lt;p&gt;Activity tracking is often very high volume as many activity messages are generated for each user page view.
（活动追踪，数据流量很大）&lt;/p&gt;

&lt;h4 id=&quot;metrics&quot;&gt;Metrics&lt;/h4&gt;

&lt;p&gt;Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.
（运行状态监控系统，从分布式应用中，汇总统计数据，形成集中的运行监控数据）&lt;/p&gt;

&lt;h4 id=&quot;log-aggregation&quot;&gt;Log Aggregation&lt;/h4&gt;

&lt;p&gt;Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.
（收集不同物理机器上的log，汇总到a central place：a file server or HDFS。与 Scribe or Flume相比，Kafka提供相当的performance、可靠性、低延迟。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：日志收集方面，Kafka的性能与Flume相当？Kafka能取代掉Flume吗？&lt;/p&gt;

&lt;h4 id=&quot;stream-processing&quot;&gt;Stream Processing&lt;/h4&gt;

&lt;p&gt;Many users end up doing stage-wise processing of data where data is consumed from topics of raw data and then aggregated, enriched, or otherwise transformed into new Kafka topics for further consumption. For example a processing flow for article recommendation might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might help normalize or deduplicate this content to a topic of cleaned article content; a final stage might attempt to match this content to users. This creates a graph of real-time data flow out of the individual topics. &lt;a href=&quot;https://github.com/nathanmarz/storm&quot;&gt;Storm&lt;/a&gt; and &lt;a href=&quot;http://samza.incubator.apache.org/&quot;&gt;Samza&lt;/a&gt; are popular frameworks for implementing these kinds of transformations.
（在Stream Processing中，Kafka担当data存储功能，即，raw data存储到Kafka中，consumer处理后的结果存储到new kafka topics中）&lt;/p&gt;

&lt;h4 id=&quot;event-sourcing&quot;&gt;Event Sourcing&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/eaaDev/EventSourcing.html&quot;&gt;Event sourcing&lt;/a&gt; is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka’s support for very large stored log data makes it an excellent backend for an application built in this style.
（Event sourcing，事件溯源，记录不同时间点的应用状态变化，通常log数据很大，Kafka满足此需求）&lt;/p&gt;

&lt;h4 id=&quot;commit-log&quot;&gt;Commit Log&lt;/h4&gt;

&lt;p&gt;Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The &lt;a href=&quot;http://kafka.apache.org/documentation.html#compaction&quot;&gt;log compaction&lt;/a&gt; feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.&lt;/p&gt;

&lt;h3 id=&quot;quick-start&quot;&gt;1.3 Quick Start&lt;/h3&gt;

&lt;p&gt;This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
（新手入门，对Kafka、Zookeeper一知半解的人，看这儿就对了）&lt;/p&gt;

&lt;h4 id=&quot;step-1-download-the-code&quot;&gt;Step 1: Download the code&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz&quot;&gt;Download&lt;/a&gt; the 0.8.1.1 release and un-tar it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; tar -xzf kafka_2.9.2-0.8.1.1.tgz
&amp;gt; cd kafka_2.9.2-0.8.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-2-start-the-server&quot;&gt;Step 2: Start the server&lt;/h4&gt;

&lt;p&gt;Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.
（kafka自带了ZooKeeper，不推荐使用）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/zookeeper-server-start.sh config/zookeeper.properties
[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now start the Kafka server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-server-start.sh config/server.properties
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-3-create-a-topic&quot;&gt;Step 3: Create a topic&lt;/h4&gt;

&lt;p&gt;Let’s create a topic named “test” with a single partition and only one replica:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see that topic if we run the list topic command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --list --zookeeper localhost:2181
test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.
（可通过配置文件，让broker自动创建topic）&lt;/p&gt;

&lt;h4 id=&quot;step-4-send-some-messages&quot;&gt;Step 4: Send some messages&lt;/h4&gt;

&lt;p&gt;Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.
（kafka自带了一个工具，能够将file或者standard input作为输入，按行传送到kafka cluster中。）&lt;/p&gt;

&lt;p&gt;Run the producer and then type a few messages into the console to send to the server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
This is a message
This is another message
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-5-start-a-consumer&quot;&gt;Step 5: Start a consumer&lt;/h4&gt;

&lt;p&gt;Kafka also has a command line consumer that will dump out messages to standard output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
This is a message
This is another message
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.&lt;/p&gt;

&lt;p&gt;All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.
（所有命令行，不夹带参数启动时，会自动弹出usage info）&lt;/p&gt;

&lt;h4 id=&quot;step-6-setting-up-a-multi-broker-cluster&quot;&gt;Step 6: Setting up a multi-broker cluster&lt;/h4&gt;

&lt;p&gt;So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).
（上述例子中，只启动了一个broker，其最多能够启动几个broker instances。下面说一下如何启动多个broker，构造cluster）&lt;/p&gt;

&lt;p&gt;First we make a config file for each of the brokers:（为每个broker，设定属性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; cp config/server.properties config/server-1.properties 
&amp;gt; cp config/server.properties config/server-2.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now edit these new files and set the following properties:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config/server-1.properties:
	broker.id=1
	port=9093
	log.dir=/tmp/kafka-logs-1
 
config/server-2.properties:
	broker.id=2
	port=9094
	log.dir=/tmp/kafka-logs-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;broker.id&lt;/code&gt; property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.
（不同的broker，应该设置不同的&lt;code&gt;port&lt;/code&gt;和&lt;code&gt;log.dir&lt;/code&gt;，否则，broker的数据会相互覆盖。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：同一台物理主机上，可以启动多个node，每个node通过&lt;code&gt;broker.id&lt;/code&gt;唯一标识。&lt;/p&gt;

&lt;p&gt;We already have Zookeeper and our single node started, so we just need to start the two new nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-server-start.sh config/server-1.properties &amp;amp;
...
&amp;gt; bin/kafka-server-start.sh config/server-2.properties &amp;amp;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a new topic with a replication factor of three:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay but now that we have a cluster how can we know which broker is doing what? To see that run the &lt;code&gt;describe topics&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.
（&lt;code&gt;describe topics&lt;/code&gt;命令的输出结果说明：first line，partition的汇总信息；remaining lines 分别说明每个partition的详细信息）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;leader&lt;/strong&gt; is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;replicas&lt;/strong&gt; is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive. （备份当前partition的node列表，包含当前已经不再存活的node）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;isr&lt;/strong&gt; is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.（&lt;code&gt;replicas&lt;/code&gt;内的node中，存活的node列表）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;leader&lt;/code&gt;后的数字&lt;code&gt;1&lt;/code&gt;，对应的含义？leader是怎么标识的？node怎么标识的？&lt;/p&gt;

&lt;p&gt;Note that in my example node 1 is the leader for the only partition of the topic.
We can run the same command on the original topic we created to see where it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.&lt;/p&gt;

&lt;p&gt;Let’s publish a few messages to our new topic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
...
my test message 1
my test message 2
^C 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s consume these messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:（验证kafka的容错性：kill leader）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ps | grep server-1.properties
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java...
&amp;gt; kill -9 7564
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:（leader终止后，slave自动升级为leader）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But the messages are still be available for consumption even though the leader that took the writes originally is down:（新选出的leader，对用户是透明的，consumer感觉不到异常）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ecosystem&quot;&gt;1.4 Ecosystem&lt;/h3&gt;

&lt;p&gt;There are a plethora of tools that integrate with Kafka outside the main distribution. The &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;ecosystem page&lt;/a&gt; lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.
（有很多工具与Kafka集成，参考&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;页面&lt;/a&gt;）&lt;/p&gt;

&lt;h3 id=&quot;upgrading-from-previous-versions&quot;&gt;1.5 Upgrading From Previous Versions&lt;/h3&gt;

&lt;h4 id=&quot;upgrading-from-080-to-081&quot;&gt;Upgrading from 0.8.0 to 0.8.1&lt;/h4&gt;

&lt;p&gt;0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.&lt;/p&gt;

&lt;h4 id=&quot;upgrading-from-07&quot;&gt;Upgrading from 0.7&lt;/h4&gt;

&lt;p&gt;0.8, the release in which added replication, was our first backwards-incompatible release: major changes were made to the API, ZooKeeper data structures, and protocol, and configuration. The upgrade from 0.7 to 0.8.x requires a &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8&quot;&gt;special tool&lt;/a&gt; for migration. This migration can be done without downtime.&lt;/p&gt;

&lt;h2 id=&quot;api&quot;&gt;2. API&lt;/h2&gt;

&lt;h3 id=&quot;producer-api&quot;&gt;2.1 Producer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;high-level-consumer-api&quot;&gt;2.2 High Level Consumer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;simple-consumer-api&quot;&gt;2.3 Simple Consumer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;kafka-hadoop-consumer-api&quot;&gt;2.4 Kafka Hadoop Consumer API&lt;/h3&gt;

&lt;p&gt;Providing a horizontally scalable solution for aggregating and loading data into Hadoop was one of our basic use cases. To support this use case, we provide a Hadoop-based consumer which spawns off many map tasks to pull data from the Kafka cluster in parallel. This provides extremely fast pull-based Hadoop data load capabilities (we were able to fully saturate the network with only a handful of Kafka servers).
（Hadoop-based consumer，并行的从Kafka cluster中pull data，速度很快）&lt;/p&gt;

&lt;p&gt;Usage information on the hadoop consumer can be found &lt;a href=&quot;https://github.com/linkedin/camus/tree/camus-kafka-0.8/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;3. Configuration&lt;/h2&gt;

&lt;p&gt;Kafka uses key-value pairs in the &lt;a href=&quot;http://en.wikipedia.org/wiki/.properties&quot;&gt;property file format&lt;/a&gt; for configuration. These values can be supplied either from a file or programmatically.&lt;/p&gt;

&lt;h3 id=&quot;broker-configs&quot;&gt;3.1 Broker Configs&lt;/h3&gt;

&lt;p&gt;The essential configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;broker.id&lt;/li&gt;
  &lt;li&gt;log.dirs&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#brokerconfigs&quot;&gt;Broker Configs&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;consumer-configs&quot;&gt;3.2 Consumer Configs&lt;/h3&gt;

&lt;p&gt;The essential consumer configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;group.id&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#consumerconfigs&quot;&gt;Consumer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;producer-configs&quot;&gt;3.3 Producer Configs&lt;/h3&gt;

&lt;p&gt;Essential configuration properties for the producer include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list&lt;/li&gt;
  &lt;li&gt;request.required.acks&lt;/li&gt;
  &lt;li&gt;producer.type&lt;/li&gt;
  &lt;li&gt;serializer.class&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#producerconfigs&quot;&gt;Producer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;new-producer-configs&quot;&gt;3.4 New Producer Configs&lt;/h3&gt;

&lt;p&gt;We are working on a replacement for our existing producer. The code is available in trunk now and can be considered beta quality. Below is the configuration for the new producer.&lt;/p&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#newproducerconfigs&quot;&gt;New Producer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume 1.5.0.1 User Guide：Flume Sources</title>
     <link href="http://ningg.github.com/flume-user-guide-source"/>
     <updated>2014-10-17T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-user-guide-source</id>
     <content type="html">&lt;h2 id=&quot;avro-source&quot;&gt;Avro Source&lt;/h2&gt;

&lt;p&gt;Listens on Avro port and receives events from external Avro client streams. When paired with the built-in Avro Sink on another (previous hop) Flume agent, it can create tiered collection topologies. Required properties are in bold.（必须属性加黑了）&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be &lt;code&gt;avro&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;bind&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;hostname or IP address to listen on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;port&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Port # to bind to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;threads&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Maximum number of worker threads to spawn&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space-separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;compression-type&lt;/td&gt;
      &lt;td&gt;none&lt;/td&gt;
      &lt;td&gt;This can be “none” or “deflate”. The compression-type must match the compression-type of matching AvroSource&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ssl&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Set this to true to enable SSL encryption. You must also specify a “keystore” and a “keystore-password”.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;This is the path to a Java keystore file. Required for SSL.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore-password&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The password for the Java keystore. Required for SSL.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore-type&lt;/td&gt;
      &lt;td&gt;JKS	The type of the Java keystore. This can be “JKS” or “PKCS12”.&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ipFilter&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Set this to true to enable ipFiltering for netty&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ipFilter.rules&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Define N netty ipFilter pattern rules with this config.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example of ipFilter.rules&lt;/p&gt;

&lt;p&gt;ipFilter.rules defines N netty ipFilters separated by a comma(&lt;code&gt;,&lt;/code&gt;) a pattern rule must be in this format.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;`allow` or `deny`&amp;gt;:&amp;lt;`ip` or `name` for computer name&amp;gt;:&amp;lt;`pattern`&amp;gt; 
allow/deny:ip/name:pattern

# example
ipFilter.rules=allow:ip:127.*,allow:name:localhost,deny:ip:*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the first rule to match will apply as the example below shows from a client on the localhost（从左向右，第一个匹配出的rules生效）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# This will Allow the client on localhost be deny clients from any other ip 
ipFilter.rules = allow:name:localhost,deny:ip:

# This will deny the client on localhost be allow clients from any other ip 
ipFilter.rules = deny:name:localhost,allow:ip:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;thrift-source&quot;&gt;Thrift Source&lt;/h2&gt;

&lt;p&gt;Listens on Thrift port and receives events from external Thrift client streams. When paired with the built-in ThriftSink on another (previous hop) Flume agent, it can create tiered collection topologies. Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be thrift&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;bind&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;hostname or IP address to listen on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;port&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Port # to bind to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;threads&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Maximum number of worker threads to spawn&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = thrift
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;exec-source&quot;&gt;Exec Source&lt;/h2&gt;

&lt;p&gt;Exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as &lt;code&gt;cat [named pipe]&lt;/code&gt; or &lt;code&gt;tail -F [file]&lt;/code&gt; are going to produce the desired results where as &lt;code&gt;date&lt;/code&gt; will probably not - the former two commands produce streams of data where as the latter produces a single event and exits.（捕获命令的输出，并按行处理，当&lt;code&gt;logStdErr&lt;/code&gt;设为true时，也将捕获stderr的输出；）&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be exec&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;command&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The command to execute&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shell&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;A shell invocation used to run the command. e.g. /bin/sh -c. Required only for commands relying on shell features like wildcards, back ticks, pipes etc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restartThrottle&lt;/td&gt;
      &lt;td&gt;10000&lt;/td&gt;
      &lt;td&gt;Amount of time (in millis) to wait before attempting a restart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restart&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Whether the executed cmd should be restarted if it dies&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;logStdErr&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Whether the command’s stderr should be logged&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batchSize&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;The max number of lines to read and send to the channel at a time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt;replicating&lt;/td&gt;
      &lt;td&gt;replicating or multiplexing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Depends on the selector.type value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space-separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;： The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost. As a for instance, one of the most commonly requested features is the &lt;code&gt;tail -F [file]&lt;/code&gt;-like use case where an application writes to a log file on disk and Flume tails the file, sending each line as an event. While this is possible, there’s an obvious problem; what happens if the channel fills up and Flume can’t send an event? Flume has no way of indicating to the application writing the log file that it needs to retain the log or that the event hasn’t been sent, for some reason. If this doesn’t make sense, you need only know this: Your application can never guarantee data has been received when using a unidirectional asynchronous interface such as ExecSource! As an extension of this warning - and to be completely clear - there is absolutely zero guarantee of event delivery when using this source. For stronger reliability guarantees, consider the Spooling Directory Source or direct integration with Flume via the SDK.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：ExecSource方式，当command异常退出后，会丢失数据。解决办法：考虑Spooling Directory Source或者通过SDK直接与Flume集成。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;：You can use ExecSource to emulate TailSource from Flume 0.9x (flume og). Just use unix command tail -F /full/path/to/your/file. Parameter -F is better in this case than -f as it will also follow file rotation.（Flume 0.9x版本中，可以使用 &lt;code&gt;tail -F path&lt;/code&gt;命令模仿 TailSource）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = exec
# follow file rotation
a1.sources.r1.command = tail -F /var/log/secure
a1.sources.r1.channels = c1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ‘shell’ config is used to invoke the ‘command’ through a command shell (such as Bash or Powershell). The ‘command’ is passed as an argument to ‘shell’ for execution. This allows the ‘command’ to use features from the shell such as wildcards, back ticks, pipes, loops, conditionals etc. In the absence of the ‘shell’ config, the ‘command’ will be invoked directly. Common values for ‘shell’ : ‘/bin/sh -c’, ‘/bin/ksh -c’, ‘cmd /c’, ‘powershell -Command’, etc.（启用shell选项时，系统会将command当作参数，传送给shell执行，此时，能利用不同shell的特性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;agent_foo.sources.tailsource-1.type = exec
agent_foo.sources.tailsource-1.shell = /bin/bash -c
agent_foo.sources.tailsource-1.command = for i in /path/*.txt; do cat $i; done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jms-source&quot;&gt;JMS Source&lt;/h2&gt;

&lt;p&gt;JMS Source reads messages from a JMS destination such as a queue or topic. Being a JMS application it should work with any JMS provider but has only been tested with ActiveMQ. The JMS source provides configurable batch size, message selector, user/pass, and message to flume event converter. Note that the vendor provided JMS jars should be included in the Flume classpath using plugins.d directory (preferred), –classpath on command line, or via FLUME_CLASSPATH variable in flume-env.sh.&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be jms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;initialContextFactory&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Inital Context Factory, e.g: org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;connectionFactory&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The JNDI name the connection factory shoulld appear as&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;providerURL&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The JMS provider URL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;destinationName&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Destination name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;destinationType&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Destination type (queue or topic)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;messageSelector&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Message selector to use when creating the consumer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;userName&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Username for the destination/provider&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;passwordFile&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;File containing the password for the destination/provider&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batchSize&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;Number of messages to consume in one batch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.type&lt;/td&gt;
      &lt;td&gt;DEFAULT&lt;/td&gt;
      &lt;td&gt;Class to use to convert messages to flume events. See below.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.*&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Converter properties.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.charset&lt;/td&gt;
      &lt;td&gt;UTF-8&lt;/td&gt;
      &lt;td&gt;Default converter only. Charset to use when converting JMS TextMessages to byte arrays.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：JMS是java方面的消息队列？做几年java了，对这个我还不清楚~~&lt;/p&gt;

&lt;h3 id=&quot;converter&quot;&gt;Converter&lt;/h3&gt;

&lt;p&gt;The JMS source allows pluggable converters, though it’s likely the default converter will work for most purposes. The default converter is able to convert Bytes, Text, and Object messages to FlumeEvents. In all cases, the properties in the message are added as headers to the FlumeEvent.（默认，message的properties会转换为FlumeEvent的headers）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BytesMessage: Bytes of message are copied to body of the FlumeEvent. Cannot convert more than 2GB of data per message.&lt;/li&gt;
  &lt;li&gt;TextMessage: Text of message is converted to a byte array and copied to the body of the FlumeEvent. The default converter uses UTF-8 by default but this is configurable.&lt;/li&gt;
  &lt;li&gt;ObjectMessage: Object is written out to a ByteArrayOutputStream wrapped in an ObjectOutputStream and the resulting array is copied to the body of the FlumeEvent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = jms
a1.sources.r1.channels = c1
a1.sources.r1.initialContextFactory = org.apache.activemq.jndi.ActiveMQInitialContextFactory
a1.sources.r1.connectionFactory = GenericConnectionFactory
a1.sources.r1.providerURL = tcp://mqserver:61616
a1.sources.r1.destinationName = BUSINESS_DATA
a1.sources.r1.destinationType = QUEUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spooling-directory-source&quot;&gt;Spooling Directory Source&lt;/h2&gt;

&lt;p&gt;This source lets you ingest data by placing files to be ingested into a “spooling” directory on disk. This source will watch the specified directory for new files, and will parse events out of new files as they appear. The event parsing logic is pluggable. After a given file has been fully read into the channel, it is renamed to indicate completion (or optionally deleted).&lt;/p&gt;

&lt;p&gt;Unlike the Exec source, this source is reliable and will not miss data, even if Flume is restarted or killed. In exchange for this reliability, only immutable, uniquely-named files must be dropped into the spooling directory. Flume tries to detect these problem conditions and will fail loudly if they are violated:&lt;/p&gt;

&lt;p&gt;If a file is written to after being placed into the spooling directory, Flume will print an error to its log file and stop processing.
If a file name is reused at a later time, Flume will print an error to its log file and stop processing.
To avoid the above issues, it may be useful to add a unique identifier (such as a timestamp) to log file names when they are moved into the spooling directory.&lt;/p&gt;

&lt;p&gt;Despite the reliability guarantees of this source, there are still cases in which events may be duplicated if certain downstream failures occur. This is consistent with the guarantees offered by other Flume components.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be spooldir.
spoolDir	–	The directory from which to read files from.
fileSuffix	.COMPLETED	Suffix to append to completely ingested files
deletePolicy	never	When to delete completed files: never or immediate
fileHeader	false	Whether to add a header storing the absolute path filename.
fileHeaderKey	file	Header key to use when appending absolute path filename to event header.
basenameHeader	false	Whether to add a header storing the basename of the file.
basenameHeaderKey	basename	Header Key to use when appending basename of file to event header.
ignorePattern	^$	Regular expression specifying which files to ignore (skip)
trackerDir	.flumespool	Directory to store metadata related to processing of files. If this path is not an absolute path, then it is interpreted as relative to the spoolDir.
consumeOrder	oldest	In which order files in the spooling directory will be consumed oldest, youngest and random. In case of oldest and youngest, the last modified time of the files will be used to compare the files. In case of a tie, the file with smallest laxicographical order will be consumed first. In case of random any file will be picked randomly. When using oldest and youngest the whole directory will be scanned to pick the oldest/youngest file, which might be slow if there are a large number of files, while using random may cause old files to be consumed very late if new files keep coming in the spooling directory.
maxBackoff	4000	The maximum time (in millis) to wait between consecutive attempts to write to the channel(s) if the channel is full. The source will start at a low backoff and increase it exponentially each time the channel throws a ChannelException, upto the value specified by this parameter.
batchSize	100	Granularity at which to batch transfer to the channel
inputCharset	UTF-8	Character set used by deserializers that treat the input file as text.
decodeErrorPolicy	FAIL	What to do when we see a non-decodable character in the input file. FAIL: Throw an exception and fail to parse the file. REPLACE: Replace the unparseable character with the “replacement character” char, typically Unicode U+FFFD. IGNORE: Drop the unparseable character sequence.
deserializer	LINE	Specify the deserializer used to parse the file into events. Defaults to parsing each line as an event. The class specified must implement EventDeserializer.Builder.
deserializer.*	 	Varies per event deserializer.
bufferMaxLines	–	(Obselete) This option is now ignored.
bufferMaxLineLength	5000	(Deprecated) Maximum length of a line in the commit buffer. Use deserializer.maxLineLength instead.
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for an agent named agent-1:&lt;/p&gt;

&lt;p&gt;agent-1.channels = ch-1
agent-1.sources = src-1&lt;/p&gt;

&lt;p&gt;agent-1.sources.src-1.type = spooldir
agent-1.sources.src-1.channels = ch-1
agent-1.sources.src-1.spoolDir = /var/log/apache/flumeSpool
agent-1.sources.src-1.fileHeader = true
Twitter 1% firehose Source (experimental)&lt;/p&gt;

&lt;p&gt;Warning This source is hightly experimental and may change between minor versions of Flume. Use at your own risk.
Experimental source that connects via Streaming API to the 1% sample twitter firehose, continously downloads tweets, converts them to Avro format and sends Avro events to a downstream Flume sink. Requires the consumer and access tokens and secrets of a Twitter developer account. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.twitter.TwitterSource
consumerKey	–	OAuth consumer key
consumerSecret	–	OAuth consumer secret
accessToken	–	OAuth access token
accessTokenSecret	–	OAuth toekn secret
maxBatchSize	1000	Maximum number of twitter messages to put in a single batch
maxBatchDurationMillis	1000	Maximum number of milliseconds to wait before closing a batch
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.twitter.TwitterSource
a1.sources.r1.channels = c1
a1.sources.r1.consumerKey = YOUR_TWITTER_CONSUMER_KEY
a1.sources.r1.consumerSecret = YOUR_TWITTER_CONSUMER_SECRET
a1.sources.r1.accessToken = YOUR_TWITTER_ACCESS_TOKEN
a1.sources.r1.accessTokenSecret = YOUR_TWITTER_ACCESS_TOKEN_SECRET
a1.sources.r1.maxBatchSize = 10
a1.sources.r1.maxBatchDurationMillis = 200
Event Deserializers&lt;/p&gt;

&lt;p&gt;The following event deserializers ship with Flume.&lt;/p&gt;

&lt;p&gt;LINE&lt;/p&gt;

&lt;p&gt;This deserializer generates one event per line of text input.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer.maxLineLength	2048	Maximum number of characters to include in a single event. If a line exceeds this length, it is truncated, and the remaining characters on the line will appear in a subsequent event.
deserializer.outputCharset	UTF-8	Charset to use for encoding events put into the channel.
AVRO&lt;/p&gt;

&lt;p&gt;This deserializer is able to read an Avro container file, and it generates one event per Avro record in the file. Each event is annotated with a header that indicates the schema used. The body of the event is the binary Avro record data, not including the schema or the rest of the container file elements.&lt;/p&gt;

&lt;p&gt;Note that if the spool directory source must retry putting one of these events onto a channel (for example, because the channel is full), then it will reset and retry from the most recent Avro container file sync point. To reduce potential event duplication in such a failure scenario, write sync markers more frequently in your Avro input files.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer.schemaType	HASH	How the schema is represented. By default, or when the value HASH is specified, the Avro schema is hashed and the hash is stored in every event in the event header “flume.avro.schema.hash”. If LITERAL is specified, the JSON-encoded schema itself is stored in every event in the event header “flume.avro.schema.literal”. Using LITERAL mode is relatively inefficient compared to HASH mode.
BlobDeserializer&lt;/p&gt;

&lt;p&gt;This deserializer reads a Binary Large Object (BLOB) per event, typically one BLOB per file. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because the entire BLOB is buffered in RAM.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder
deserializer.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request
NetCat Source&lt;/p&gt;

&lt;p&gt;A netcat-like source that listens on a given port and turns each line of text into an event. Acts like nc -k -l [host] [port]. In other words, it opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel.&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be netcat
bind	–	Host name or IP address to bind to
port	–	Port # to bind to
max-line-length	512	Max line length per event body (in bytes)
ack-every-event	true	Respond with an “OK” for every event received
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Sequence Generator Source&lt;/p&gt;

&lt;p&gt;A simple sequence generator that continuously generates events with a counter that starts from 0 and increments by 1. Useful mainly for testing. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be seq
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
batchSize	1	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = seq
a1.sources.r1.channels = c1
Syslog Sources&lt;/p&gt;

&lt;p&gt;Reads syslog data and generate Flume events. The UDP source treats an entire message as a single event. The TCP sources create a new event for each string of characters separated by a newline (‘n’).&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Syslog TCP Source&lt;/p&gt;

&lt;p&gt;The original, tried-and-true syslog TCP source.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be syslogtcp
host	–	Host name or IP address to bind to
port	–	Port # to bind to
eventSize	2500	Maximum size of a single event line, in bytes
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
For example, a syslog TCP source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = syslogtcp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
Multiport Syslog TCP Source&lt;/p&gt;

&lt;p&gt;This is a newer, faster, multi-port capable version of the Syslog TCP source. Note that the ports configuration setting has replaced port. Multi-port capability means that it can listen on many ports at once in an efficient manner. This source uses the Apache Mina library to do that. Provides support for RFC-3164 and many common RFC-5424 formatted messages. Also provides the capability to configure the character set used on a per-port basis.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be multiport_syslogtcp
host	–	Host name or IP address to bind to.
ports	–	Space-separated list (one or more) of ports to bind to.
eventSize	2500	Maximum size of a single event line, in bytes.
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
portHeader	–	If specified, the port number will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the incoming port.
charset.default	UTF-8	Default character set used while parsing syslog events into strings.
charset.port.&lt;port&gt;	–	Character set is configurable on a per-port basis.
batchSize	100	Maximum number of events to attempt to process per request loop. Using the default is usually fine.
readBufferSize	1024	Size of the internal Mina read buffer. Provided for performance tuning. Using the default is usually fine.
numProcessors	(auto-detected)	Number of processors available on the system for use while processing messages. Default is to auto-detect # of CPUs using the Java Runtime API. Mina will spawn 2 request-processing threads per detected CPU, which is often reasonable.
selector.type	replicating	replicating, multiplexing, or custom
selector.*	–	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors.
interceptors.*	 	 
For example, a multiport syslog TCP source for agent named a1:&lt;/port&gt;&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = multiport_syslogtcp
a1.sources.r1.channels = c1
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.ports = 10001 10002 10003
a1.sources.r1.portHeader = port
Syslog UDP Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be syslogudp
host	–	Host name or IP address to bind to
port	–	Port # to bind to
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
For example, a syslog UDP source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = syslogudp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
HTTP Source&lt;/p&gt;

&lt;p&gt;A source which accepts Flume Events by HTTP POST and GET. GET should be used for experimentation only. HTTP requests are converted into flume events by a pluggable “handler” which must implement the HTTPSourceHandler interface. This handler takes a HttpServletRequest and returns a list of flume events. All events handled from one Http request are committed to the channel in one transaction, thus allowing for increased efficiency on channels like the file channel. If the handler throws an exception, this source will return a HTTP status of 400. If the channel is full, or the source is unable to append events to the channel, the source will return a HTTP 503 - Temporarily unavailable status.&lt;/p&gt;

&lt;p&gt;All events sent in one post request are considered to be one batch and inserted into the channel in one transaction.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
type	 	The component type name, needs to be http
port	–	The port the source should bind to.
bind	0.0.0.0	The hostname or IP address to listen on
handler	org.apache.flume.source.http.JSONHandler	The FQCN of the handler class.
handler.*	–	Config parameters for the handler
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
enableSSL	false	Set the property true, to enable SSL
keystore	 	Location of the keystore includng keystore file name
keystorePassword Keystore password
For example, a http source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = http
a1.sources.r1.port = 5140
a1.sources.r1.channels = c1
a1.sources.r1.handler = org.example.rest.RestHandler
a1.sources.r1.handler.nickname = random props
JSONHandler&lt;/p&gt;

&lt;p&gt;A handler is provided out of the box which can handle events represented in JSON format, and supports UTF-8, UTF-16 and UTF-32 character sets. The handler accepts an array of events (even if there is only one event, the event has to be sent in an array) and converts them to a Flume event based on the encoding specified in the request. If no encoding is specified, UTF-8 is assumed. The JSON handler supports UTF-8, UTF-16 and UTF-32. Events are represented as follows.&lt;/p&gt;

&lt;p&gt;[{
  “headers” : {
             “timestamp” : “434324343”,
             “host” : “random_host.example.com”
             },
  “body” : “random_body”
  },
  {
  “headers” : {
             “namenode” : “namenode.example.com”,
             “datanode” : “random_datanode.example.com”
             },
  “body” : “really_random_body”
  }]
To set the charset, the request must have content type specified as application/json; charset=UTF-8 (replace UTF-8 with UTF-16 or UTF-32 as required).&lt;/p&gt;

&lt;p&gt;One way to create an event in the format expected by this handler is to use JSONEvent provided in the Flume SDK and use Google Gson to create the JSON string using the Gson#fromJson(Object, Type) method. The type token to pass as the 2nd argument of this method for list of events can be created by:&lt;/p&gt;

&lt;p&gt;Type type = new TypeToken&amp;lt;List&lt;jsonevent&gt;&amp;gt;() {}.getType();
BlobHandler&lt;/jsonevent&gt;&lt;/p&gt;

&lt;p&gt;By default HTTPSource splits JSON input into Flume events. As an alternative, BlobHandler is a handler for HTTPSource that returns an event that contains the request parameters as well as the Binary Large Object (BLOB) uploaded with this request. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because it buffers up the entire BLOB in RAM.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
handler	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobHandler
handler.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request
Legacy Sources&lt;/p&gt;

&lt;p&gt;The legacy sources allow a Flume 1.x agent to receive events from Flume 0.9.4 agents. It accepts events in the Flume 0.9.4 format, converts them to the Flume 1.0 format, and stores them in the connected channel. The 0.9.4 event properties like timestamp, pri, host, nanos, etc get converted to 1.x event header attributes. The legacy source supports both Avro and Thrift RPC connections. To use this bridge between two Flume versions, you need to start a Flume 1.x agent with the avroLegacy or thriftLegacy source. The 0.9.4 agent should have the agent Sink pointing to the host/port of the 1.x agent.&lt;/p&gt;

&lt;p&gt;Note The reliability semantics of Flume 1.x are different from that of Flume 0.9.x. The E2E or DFO mode of a Flume 0.9.x agent will not be supported by the legacy source. The only supported 0.9.x mode is the best effort, though the reliability setting of the 1.x flow will be applicable to the events once they are saved into the Flume 1.x channel by the legacy source.
Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Avro Legacy Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.avroLegacy.AvroLegacySource
host	–	The hostname or IP address to bind to
port	–	The port # to listen on
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.avroLegacy.AvroLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Thrift Legacy Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.thriftLegacy.ThriftLegacySource
host	–	The hostname or IP address to bind to
port	–	The port # to listen on
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.thriftLegacy.ThriftLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Custom Source&lt;/p&gt;

&lt;p&gt;A custom source is your own implementation of the Source interface. A custom source’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom source is its FQCN.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be your FQCN
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.example.MySource
a1.sources.r1.channels = c1
Scribe Source&lt;/p&gt;

&lt;p&gt;Scribe is another type of ingest system. To adopt existing Scribe ingest system, Flume should use ScribeSource based on Thrift with compatible transfering protocol. For deployment of Scribe please follow the guide from Facebook. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
type	–	The component type name, needs to be org.apache.flume.source.scribe.ScribeSource
port	1499	Port that Scribe should be connected
workerThreads	5	Handing threads number in Thrift
selector.type	 	 
selector.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.scribe.ScribeSource
a1.sources.r1.port = 1463
a1.sources.r1.workerThreads = 5
a1.sources.r1.channels = c1&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide 1.5.0.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume 1.5.0.1 User Guide：introduction</title>
     <link href="http://ningg.github.com/flume-user-guide"/>
     <updated>2014-10-17T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-user-guide</id>
     <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store.&lt;/p&gt;

&lt;p&gt;The use of Apache Flume is not only restricted to log data aggregation. Since data sources are customizable, Flume can be used to transport massive quantities of event data including but not limited to network traffic data, social-media-generated data, email messages and pretty much any data source possible.（数据源：网络流量数据、社交媒体产生的数据、email数据、其他数据，Flume都能收集）&lt;/p&gt;

&lt;p&gt;Apache Flume is a top level project at the Apache Software Foundation.&lt;/p&gt;

&lt;p&gt;There are currently two release code lines available, versions 0.9.x and 1.x.&lt;/p&gt;

&lt;p&gt;Documentation for the 0.9.x track is available at the &lt;a href=&quot;http://archive.cloudera.com/cdh/3/flume/UserGuide/&quot;&gt;Flume 0.9.x User Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This documentation applies to the 1.4.x track.&lt;/p&gt;

&lt;p&gt;New and existing users are encouraged to use the 1.x releases so as to leverage the performance improvements and configuration flexibilities available in the latest architecture.（推荐使用Flume 1.x版本：性能有改善、配置方便，使用了最新架构）&lt;/p&gt;

&lt;h3 id=&quot;system-requirements&quot;&gt;System Requirements&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Java Runtime Environment - Java 1.6 or later (Java 1.7 Recommended) （运行环境：JRE 1.6+，推荐JRE1.7）&lt;/li&gt;
  &lt;li&gt;Memory - Sufficient memory for configurations used by sources, channels or sinks&lt;/li&gt;
  &lt;li&gt;Disk Space - Sufficient disk space for configurations used by channels or sinks&lt;/li&gt;
  &lt;li&gt;Directory Permissions - Read/Write permissions for directories used by agent （agent需要R/W权限）&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;h4 id=&quot;data-flow-model&quot;&gt;Data flow model&lt;/h4&gt;

&lt;p&gt;A Flume event is defined as a unit of data flow having a byte payload and an optional set of string attributes. A Flume agent is a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop).（Flume Event是a unit of data flow having a byte payload 和几个属性集合；Flume Agent是JVM进程，将events flow从一端送到另一端）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image00.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Flume source consumes events delivered to it by an external source like a web server. The external source sends events to Flume in a format that is recognized by the target Flume source. For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink. A similar flow can be defined using a Thrift Flume Source to receive events from a Thrift Sink or a Flume Thrift Rpc Client or Thrift clients written in any language generated from the Flume thrift protocol.When a Flume source receives an event, it stores it into one or more channels. The channel is a passive store that keeps the event until it’s consumed by a Flume sink. The file channel is one example – it is backed by the local filesystem. The sink removes the event from the channel and puts it into an external repository like HDFS (via Flume HDFS sink) or forwards it to the Flume source of the next Flume agent (next hop) in the flow. The source and sink within the given agent run asynchronously with the events staged in the channel.（Source，接收外部data source的数据；Channel，被动接收Source的数据；Sink主动从Channel读取数据，并将其传递出去；利用Channel机制，Source、Sink实现异步处理）&lt;/p&gt;

&lt;h4 id=&quot;complex-flows&quot;&gt;Complex flows&lt;/h4&gt;

&lt;p&gt;Flume allows a user to build multi-hop flows where events travel through multiple agents before reaching the final destination. It also allows fan-in and fan-out flows, contextual routing and backup routes (fail-over) for failed hops.（Flume内flows支持fan-in、fan-out——多入多出，contextual touting和backup routes(fail-over)）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：contextual routing 和 backup routes的含义？&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;reliability&quot;&gt;Reliability&lt;/h4&gt;

&lt;p&gt;The events are staged in a channel on each agent. The events are then delivered to the next agent or terminal repository (like HDFS) in the flow. The events are removed from a channel only after they are stored in the channel of next agent or in the terminal repository. This is a how the single-hop message delivery semantics in Flume provide end-to-end reliability of the flow.（&lt;strong&gt;single-hop message delivery semantics&lt;/strong&gt;：Channel中的event仅在被成功处理之后，才从Channel中删掉。）&lt;/p&gt;

&lt;p&gt;Flume uses a transactional approach to guarantee the reliable delivery of the events. The sources and sinks encapsulate in a transaction the storage/retrieval, respectively, of the events placed in or provided by a transaction provided by the channel. This ensures that the set of events are reliably passed from point to point in the flow. In the case of a multi-hop flow, the sink from the previous hop and the source from the next hop both have their transactions running to ensure that the data is safely stored in the channel of the next hop.（&lt;strong&gt;multi-hop&lt;/strong&gt;：）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Flume如何保证事物操作？没看懂&lt;/p&gt;

&lt;h4 id=&quot;recoverability&quot;&gt;Recoverability&lt;/h4&gt;

&lt;p&gt;The events are staged in the channel, which manages recovery from failure. Flume supports a durable file channel which is backed by the local file system. There’s also a memory channel which simply stores the events in an in-memory queue, which is faster but any events still left in the memory channel when an agent process dies can’t be recovered.（Channel需保证崩溃后，能恢复events，具体：本地FS上保存durable file channel，另，占用一个in-memory queue，Channel进程崩溃后，能加快恢复速度；但，如果agent进程崩溃，将导致内存泄漏：无法回收这一内存）&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;h3 id=&quot;setting-up-an-agent&quot;&gt;Setting up an agent&lt;/h3&gt;

&lt;p&gt;Flume agent configuration is stored in a local configuration file. This is a text file that follows the Java properties file format. Configurations for one or more agents can be specified in the same configuration file. The configuration file includes properties of each source, sink and channel in an agent and how they are wired together to form data flows.（Agent利用config file设置：source、channel、sink的属性，以及不同Agent之间前后联系）&lt;/p&gt;

&lt;h4 id=&quot;configuring-individual-components&quot;&gt;Configuring individual components&lt;/h4&gt;

&lt;p&gt;Each component (source, sink or channel) in the flow has a name, type, and set of properties that are specific to the type and instantiation. For example, an Avro source needs a hostname (or IP address) and a port number to receive data from. A memory channel can have max queue size (“capacity”), and an HDFS sink needs to know the file system URI, path to create files, frequency of file rotation (“hdfs.rollInterval”) etc. All such attributes of a component needs to be set in the properties file of the hosting Flume agent.（设置Component的属性）&lt;/p&gt;

&lt;h4 id=&quot;wiring-the-pieces-together&quot;&gt;Wiring the pieces together&lt;/h4&gt;

&lt;p&gt;The agent needs to know what individual components to load and how they are connected in order to constitute the flow. This is done by listing the names of each of the sources, sinks and channels in the agent, and then specifying the connecting channel for each sink and source. For example, an agent flows events from an Avro source called avroWeb to HDFS sink hdfs-cluster1 via a file channel called file-channel. The configuration file will contain names of these components and file-channel as a shared channel for both avroWeb source and hdfs-cluster1 sink.（设置不同agent构成的topologies）&lt;/p&gt;

&lt;h4 id=&quot;starting-an-agent&quot;&gt;Starting an agent&lt;/h4&gt;

&lt;p&gt;An agent is started using a shell script called flume-ng which is located in the bin directory of the Flume distribution. You need to specify the agent name, the config directory, and the config file on the command line:（启动agent，需要指定参数：agent name、config dir、config file。）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the agent will start running source and sinks configured in the given properties file.&lt;/p&gt;

&lt;h4 id=&quot;a-simple-example&quot;&gt;A simple example&lt;/h4&gt;

&lt;p&gt;Here, we give an example configuration file, describing a single-node Flume deployment. This configuration lets a user generate events and subsequently logs them to the console.（场景：single-node模式，user产生events并且将其输出到控制台）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration defines a single agent named a1. a1 has a source that listens for data on port 44444, a channel that buffers event data in memory, and a sink that logs event data to the console. The configuration file names the various components, then describes their types and configuration parameters. A given configuration file might define several named agents; when a given Flume process is launched a flag is passed telling it which named agent to manifest.（一个配置文件中，可设定多个agents，Flume进程启动时，会指定agent运行）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：配置文件中，具体参数配置：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/flume-config-tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given this configuration file, we can start Flume as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that in a full deployment we would typically include one more option: –conf=&lt;conf-dir&gt;. The &lt;conf-dir&gt; directory would include a shell script flume-env.sh and potentially a log4j properties file. In this example, we pass a Java option to force Flume to log to the console and we go without a custom environment script.（实际开发场景下，通过`--conf=&lt;conf-dir&gt;`传入`&lt;conf-dir&gt;`，通常这一目录下应包含flume-env.sh文件和log4j的配置文件）&lt;/conf-dir&gt;&lt;/conf-dir&gt;&lt;/conf-dir&gt;&lt;/conf-dir&gt;&lt;/p&gt;

&lt;p&gt;From a separate terminal, we can then telnet port 44444 and send Flume an event:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ telnet localhost 44444
Trying 127.0.0.1...
Connected to localhost.localdomain (127.0.0.1).
Escape character is &#39;^]&#39;.
Hello world! &amp;lt;ENTER&amp;gt;
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The original Flume terminal will output the event in a log message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;12/06/19 15:32:19 INFO source.NetcatSource: Source starting
12/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]
12/06/19 15:32:34 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D          Hello world!. }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations - you’ve successfully configured and deployed a Flume agent! Subsequent sections cover agent configuration in much more detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：telnet通过命令行方式，能够发送字符？那么能与服务器交互吗？telnet命令方式下，典型应用场景有哪些？&lt;/p&gt;

&lt;h4 id=&quot;installing-third-party-plugins&quot;&gt;Installing third-party plugins&lt;/h4&gt;

&lt;p&gt;Flume has a fully plugin-based architecture. While Flume ships with many out-of-the-box sources, channels, sinks, serializers, and the like, many implementations exist which ship separately from Flume.&lt;/p&gt;

&lt;p&gt;While it has always been possible to include custom Flume components by adding their jars to the FLUME_CLASSPATH variable in the flume-env.sh file, Flume now supports a special directory called plugins.d which automatically picks up plugins that are packaged in a specific format. This allows for easier management of plugin packaging issues as well as simpler debugging and troubleshooting of several classes of issues, especially library dependency conflicts.（在flume-env.sh中向FLUME_CLASSPATH中添加plugin的位置；另一种方式，向&lt;code&gt;plugins.d&lt;/code&gt;目录下添加plugin，即可自动安装。）&lt;/p&gt;

&lt;h5 id=&quot;the-pluginsd-directory&quot;&gt;The plugins.d directory&lt;/h5&gt;

&lt;p&gt;The &lt;code&gt;plugins.d&lt;/code&gt; directory is located at &lt;code&gt;$FLUME_HOME/plugins.d&lt;/code&gt;. At startup time, the flume-ng start script looks in the &lt;code&gt;plugins.d&lt;/code&gt; directory for plugins that conform to the below format and includes them in proper paths when starting up &lt;code&gt;java&lt;/code&gt;.（系统其中前，自动预处理plugins.d下的plugin）&lt;/p&gt;

&lt;h5 id=&quot;directory-layout-for-plugins&quot;&gt;Directory layout for plugins&lt;/h5&gt;

&lt;p&gt;Each plugin (subdirectory) within plugins.d can have up to three sub-directories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;lib - the plugin’s jar(s)&lt;/li&gt;
  &lt;li&gt;libext - the plugin’s dependency jar(s)&lt;/li&gt;
  &lt;li&gt;native - any required native libraries, such as .so files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Example of two plugins within the plugins.d directory:（&lt;code&gt;plugins.d&lt;/code&gt;目录下，plugin的目录结构如下）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugins.d/
plugins.d/custom-source-1/
plugins.d/custom-source-1/lib/my-source.jar
plugins.d/custom-source-1/libext/spring-core-2.5.6.jar
plugins.d/custom-source-2/
plugins.d/custom-source-2/lib/custom.jar
plugins.d/custom-source-2/native/gettext.so
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;data-ingestion&quot;&gt;Data ingestion&lt;/h3&gt;

&lt;p&gt;Flume supports a number of mechanisms to ingest data from external sources.（从外部 sources 获取数据，Flume有多种方式）&lt;/p&gt;

&lt;h4 id=&quot;rpc&quot;&gt;RPC&lt;/h4&gt;

&lt;p&gt;An Avro client included in the Flume distribution can send a given file to Flume Avro source using avro RPC mechanism:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/flume-ng avro-client -H localhost -p 41414 -F /usr/logs/log.10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command will send the contents of &lt;code&gt;/usr/logs/log.10&lt;/code&gt; to to the Flume source listening on that ports.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：什么含义？以Avro方式，向localhost:41414发送文件？仅仅是Data Source？有一个单独的Flume Source在localhost:41414监听？&lt;/p&gt;

&lt;h4 id=&quot;executing-commands&quot;&gt;Executing commands&lt;/h4&gt;

&lt;p&gt;There’s an exec source that executes a given command and consumes the output. A single ‘line’ of output ie. text followed by carriage return (‘\r’) or line feed (‘\n’) or both together.（exec source，执行command并将output按行发送至Channel）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Flume does not support &lt;code&gt;tail&lt;/code&gt; as a source. One can wrap the &lt;code&gt;tail&lt;/code&gt; command in an exec source to stream the file.（无法直接使用tail，需要包装在exec source中。）&lt;/p&gt;

&lt;h4 id=&quot;network-streams&quot;&gt;Network streams&lt;/h4&gt;

&lt;p&gt;Flume supports the following mechanisms to read data from popular log stream types, such as:（下述方式支持从log system中读取stream）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Avro&lt;/li&gt;
  &lt;li&gt;Thrift&lt;/li&gt;
  &lt;li&gt;Syslog&lt;/li&gt;
  &lt;li&gt;Netcat&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：上面都是什么呐？&lt;/p&gt;

&lt;h3 id=&quot;setting-multi-agent-flow&quot;&gt;Setting multi-agent flow&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to flow the data across multiple agents or hops, the sink of the previous agent and source of the current hop need to be avro type with the sink pointing to the hostname (or IP address) and port of the source.（multiple agents时，previous agent中sink、current agent中source都应为avro类型，对应到相同的&lt;code&gt;IP:port&lt;/code&gt;）&lt;/p&gt;

&lt;h3 id=&quot;consolidation&quot;&gt;Consolidation&lt;/h3&gt;

&lt;p&gt;A very common scenario in log collection is a large number of log producing clients sending data to a few consumer agents that are attached to the storage subsystem. For example, logs collected from hundreds of web servers sent to a dozen of agents that write to HDFS cluster.（一个常见场景：大量client收集数据，写入一个集中系统）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be achieved in Flume by configuring a number of first tier agents with an avro sink, all pointing to an avro source of single agent (Again you could use the thrift sources/sinks/clients in such a scenario). This source on the second tier agent consolidates the received events into a single channel which is consumed by a sink to its final destination.（解决办法：为每个client分配一个agent，再第二层，使用一个agent进行合并，然后写入最后的集中存储系统；不适用single-agent中的multi-sources，因为multi-sources，要求sources必须与channel在同一物理机器上，即，一个agent必须在一个物理机器上）&lt;/p&gt;

&lt;h3 id=&quot;multiplexing-the-flow&quot;&gt;Multiplexing the flow&lt;/h3&gt;

&lt;p&gt;Flume supports multiplexing the event flow to one or more destinations. This is achieved by defining a flow multiplexer that can replicate or selectively route an event to one or more channels.（对agent扩展，定义多个channel，实现fan-out）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image01.png&quot; alt=&quot;A fan-out flow using a (multiplexing) channel selector&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above example shows a source from agent “foo” fanning out the flow to three different channels. This fan out can be replicating or multiplexing. In case of replicating flow, each event is sent to all three channels. For the multiplexing case, an event is delivered to a subset of available channels when an event’s attribute matches a preconfigured value. For example, if an event attribute called “txnType” is set to “customer”, then it should go to channel1 and channel3, if it’s “vendor” then it should go to channel2, otherwise channel3. The mapping can be set in the agent’s configuration file.（两种fan-out方式，replicating\multiplexing，即，复制和多路复用；replicating，输入复制到每个channel中一份；multiplexing，输入仅复制到匹配的channel中，相当于channel前加了个filter）&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;As mentioned in the earlier section, Flume agent configuration is read from a file that resembles a Java property file format with hierarchical property settings.（hierarchical property settings？）&lt;/p&gt;

&lt;h3 id=&quot;defining-the-flow&quot;&gt;Defining the flow&lt;/h3&gt;

&lt;p&gt;To define the flow within a single agent, you need to link the sources and sinks via a channel. （定义single-agent内的flow时，几点：）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;列出agent对应的sources、sinks、channels；&lt;/li&gt;
  &lt;li&gt;指定与source对应的channels，指定与sink对应的channel；&lt;/li&gt;
  &lt;li&gt;一个source可对应多个channel，一个sink只能对应一个channel； &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The format is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set channel for source
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt; ...

# set channel for sink
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink&amp;gt;.channel = &amp;lt;Channel1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：source绑定&lt;span style=&quot;color:red&quot;&gt;channels&lt;/span&gt;、sink绑定&lt;span style=&quot;color:red&quot;&gt;channel&lt;/span&gt;。疑问：单个agent中source只能有一个吗？如果single-agent中有多个source，那么是否也可以实现fan-in？&lt;/p&gt;

&lt;p&gt;For example, an agent named agent_foo is reading data from an external avro client and sending it to HDFS via a memory channel. The config file weblog.config could look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels for the agent
agent_foo.sources = avro-appserver-src-1
agent_foo.sinks = hdfs-sink-1
agent_foo.channels = mem-channel-1

# set channel for source
agent_foo.sources.avro-appserver-src-1.channels = mem-channel-1

# set channel for sink
agent_foo.sinks.hdfs-sink-1.channel = mem-channel-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will make the events flow from avro-AppSrv-source to hdfs-Cluster1-sink through the memory channel mem-channel-1. When the agent is started with the weblog.config as its config file, it will instantiate that flow.&lt;/p&gt;

&lt;h3 id=&quot;configuring-individual-components-1&quot;&gt;Configuring individual components&lt;/h3&gt;

&lt;p&gt;After defining the flow, you need to set properties of each source, sink and channel. This is done in the same hierarchical namespace fashion where you set the component type and other values for the properties specific to each component:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# properties for sources
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source&amp;gt;.&amp;lt;someProperty&amp;gt; = &amp;lt;someValue&amp;gt;

# properties for channels
&amp;lt;Agent&amp;gt;.channel.&amp;lt;Channel&amp;gt;.&amp;lt;someProperty&amp;gt; = &amp;lt;someValue&amp;gt;

# properties for sinks
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Sink&amp;gt;.&amp;lt;someProperty&amp;gt; = &amp;lt;someValue&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The property &lt;code&gt;type&lt;/code&gt; needs to be set for each component for Flume to understand what kind of object it needs to be. Each source, sink and channel type has its own set of properties required for it to function as intended. All those need to be set as needed. In the previous example, we have a flow from avro-AppSrv-source to hdfs-Cluster1-sink through the memory channel mem-channel-1. Here’s an example that shows configuration of each of those components:（不同的组件有不同的property，但都有&lt;code&gt;type&lt;/code&gt;属性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;agent_foo.sources = avro-AppSrv-source
agent_foo.sinks = hdfs-Cluster1-sink
agent_foo.channels = mem-channel-1

# set channel for sources, sinks

# properties of avro-AppSrv-source
agent_foo.sources.avro-AppSrv-source.type = avro
agent_foo.sources.avro-AppSrv-source.bind = localhost
agent_foo.sources.avro-AppSrv-source.port = 10000

# properties of mem-channel-1
agent_foo.channels.mem-channel-1.type = memory
agent_foo.channels.mem-channel-1.capacity = 1000
agent_foo.channels.mem-channel-1.transactionCapacity = 100

# properties of hdfs-Cluster1-sink
agent_foo.sinks.hdfs-Cluster1-sink.type = hdfs
agent_foo.sinks.hdfs-Cluster1-sink.hdfs.path = hdfs://namenode/flume/webdata

#...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;adding-multiple-flows-in-an-agent&quot;&gt;Adding multiple flows in an agent&lt;/h3&gt;

&lt;p&gt;A single Flume agent can contain several independent flows. You can list multiple sources, sinks and channels in a config. These components can be linked to form multiple flows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source1&amp;gt; &amp;lt;Source2&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink1&amp;gt; &amp;lt;Sink2&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于single-agent说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;可包含多个sources、sinks、channels；&lt;/li&gt;
  &lt;li&gt;定义多个sources时，&lt;code&gt;source1&lt;/code&gt;和&lt;code&gt;source2&lt;/code&gt;间，空格间隔；&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:red&quot;&gt;sink与channel一一对应吗&lt;/span&gt;？&lt;/li&gt;
  &lt;li&gt;可包含多个相互独立的flow；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;疑问：source、channel、sink之间对应关系？&lt;code&gt;1：1&lt;/code&gt;？&lt;code&gt;1：n&lt;/code&gt;？&lt;code&gt;n：1&lt;/code&gt;？&lt;code&gt;n：n&lt;/code&gt;？&lt;/p&gt;

&lt;p&gt;Then you can link the sources and sinks to their corresponding channels (for sources) of channel (for sinks) to setup two different flows. For example, if you need to setup two flows in an agent, one going from an external avro client to external HDFS and another from output of a tail to avro sink, then here’s a config to do that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels in the agent
agent_foo.sources = avro-AppSrv-source1 exec-tail-source2
agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2
agent_foo.channels = mem-channel-1 file-channel-2

# flow #1 configuration
agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1
agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1

# flow #2 configuration
agent_foo.sources.exec-tail-source2.channels = file-channel-2
agent_foo.sinks.avro-forward-sink2.channel = file-channel-2	
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;configuring-a-multi-agent-flow&quot;&gt;Configuring a multi agent flow&lt;/h3&gt;

&lt;p&gt;To setup a multi-tier flow, you need to have an avro/thrift sink of first hop pointing to avro/thrift source of the next hop. This will result in the first Flume agent forwarding events to the next Flume agent. For example, if you are periodically sending files (1 file per event) using avro client to a local Flume agent, then this local agent can forward it to another agent that has the mounted for storage.（multi-agent之间通过avro、thrift方式进行连接，通过&lt;code&gt;IP:port&lt;/code&gt;来交互）&lt;/p&gt;

&lt;p&gt;Weblog agent config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list sources, sinks and channels in the agent
agent_foo.sources = avro-AppSrv-source
agent_foo.sinks = avro-forward-sink
agent_foo.channels = file-channel

# define the flow
agent_foo.sources.avro-AppSrv-source.channels = file-channel
agent_foo.sinks.avro-forward-sink.channel = file-channel

# avro sink properties
agent_foo.sources.avro-forward-sink.type = avro
agent_foo.sources.avro-forward-sink.hostname = 10.1.1.100
agent_foo.sources.avro-forward-sink.port = 10000

# configure other pieces
#...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HDFS agent config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list sources, sinks and channels in the agent
agent_foo.sources = avro-collection-source
agent_foo.sinks = hdfs-sink
agent_foo.channels = mem-channel

# define the flow
agent_foo.sources.avro-collection-source.channels = mem-channel
agent_foo.sinks.hdfs-sink.channel = mem-channel

# avro sink properties
agent_foo.sources.avro-collection-source.type = avro
agent_foo.sources.avro-collection-source.bind = 10.1.1.100
agent_foo.sources.avro-collection-source.port = 10000

# configure other pieces
#...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we link the avro-forward-sink from the weblog agent to the avro-collection-source of the hdfs agent. This will result in the events coming from the external appserver source eventually getting stored in HDFS.&lt;/p&gt;

&lt;h3 id=&quot;fan-out-flow&quot;&gt;Fan out flow&lt;/h3&gt;

&lt;p&gt;As discussed in previous section, Flume supports fanning out the flow from one source to multiple channels. There are two modes of fan out, replicating and multiplexing. In the replicating flow, the event is sent to all the configured channels. In case of multiplexing, the event is sent to only a subset of qualifying channels. To fan out the flow, one needs to specify a list of channels for a source and the policy for the fanning it out. This is done by adding a channel “selector” that can be replicating or multiplexing. Then further specify the selection rules if it’s a multiplexer. If you don’t specify a selector, then by default it’s replicating:（fan-out，两种实现方式：replicating、multiplexing；replicating，发送给所有channel；multiplexing，发送给满足条件的channel。具体，设置&lt;code&gt;selector&lt;/code&gt;，并指定规则；默认是replicating）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# List the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source1&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink1&amp;gt; &amp;lt;Sink2&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set list of channels for source (separated by space)
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set channel for sinks
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink1&amp;gt;.channel = &amp;lt;Channel1&amp;gt;
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink2&amp;gt;.channel = &amp;lt;Channel2&amp;gt;

# set selector.type = replicating
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.type = replicating
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The multiplexing select has a further set of properties to bifurcate the flow. This requires specifying a mapping of an event attribute to a set for channel. The selector checks for each configured attribute in the event header. If it matches the specified value, then that event is sent to all the channels mapped to that value. If there’s no match, then the event is sent to set of channels configured as default:（multiplexing方式时，设置header属性，根据header取值不同，分发到相应的channel；都不匹配的，分发到default）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：header的值，是谁设置的？在哪设置的？难道是event中自带的？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Mapping for multiplexing selector
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.type = multiplexing
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.header = &amp;lt;someHeader&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value1&amp;gt; = &amp;lt;Channel1&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value2&amp;gt; = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value3&amp;gt; = &amp;lt;Channel2&amp;gt;
#...

&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.default = &amp;lt;Channel2&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The mapping allows overlapping the channels for each value.（不同header取值对应的channel，可以重复）&lt;/p&gt;

&lt;p&gt;The following example has a single flow that multiplexed to two paths. The agent named agent_foo has a single avro source and two channels linked to two sinks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels in the agent
agent_foo.sources = avro-AppSrv-source1
agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2
agent_foo.channels = mem-channel-1 file-channel-2

# set channels for source
agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 file-channel-2

# set channel for sinks
agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1
agent_foo.sinks.avro-forward-sink2.channel = file-channel-2

# channel selector configuration
agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing
agent_foo.sources.avro-AppSrv-source1.selector.header = State
agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1
agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The selector checks for a header called “State”. If the value is “CA” then its sent to mem-channel-1, if its “AZ” then it goes to file-channel-2 or if its “NY” then both. If the “State” header is not set or doesn’t match any of the three, then it goes to mem-channel-1 which is designated as ‘default’.&lt;/p&gt;

&lt;p&gt;The selector also supports optional channels. To specify optional channels for a header, the config parameter ‘optional’ is used in the following way:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：optional channels，要解决什么问题？RE：仅当required channel中event运行失败，才有可能涉及optional channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# channel selector configuration
agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing
agent_foo.sources.avro-AppSrv-source1.selector.header = State
agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1
agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.optional.CA = mem-channel-1 file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The selector will attempt to write to the required channels first and will fail the transaction if even one of these channels fails to consume the events. The transaction is reattempted on all of the channels. Once all required channels have consumed the events, then the selector will attempt to write to the optional channels. A failure by any of the optional channels to consume the event is simply ignored and not retried.（运行异常的事务，会尝试在所有required channels中重新运行，如果重新运行成功，则将event写入optional channels内。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：transaction、event是怎么划分的？到底什么是transaction？&lt;/p&gt;

&lt;p&gt;If there is an overlap between the optional channels and required channels for a specific header, the channel is considered to be required, and a failure in the channel will cause the entire set of required channels to be retried. For instance, in the above example, for the header “CA” mem-channel-1 is considered to be a required channel even though it is marked both as required and optional, and a failure to write to this channel will cause that event to be retried on all channels configured for the selector.（如果一个channel既是required channel，又是optional channel，则强制认定channel为required channel）&lt;/p&gt;

&lt;p&gt;Note that if a header does not have any required channels, then the event will be written to the default channels and will be attempted to be written to the optional channels for that header. Specifying optional channels will still cause the event to be written to the default channels, if no required channels are specified. If no channels are designated as default and there are no required, the selector will attempt to write the events to the optional channels. Any failures are simply ignored in that case.（如果event，没有任何对应的required channel，则尝试写入default channel，并且尝试写入对应的optional channel；如果没有default channel，则，也会写入optional channel中。）&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide 1.5.0.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>MySQL字符编码和大小写敏感问题</title>
     <link href="http://ningg.github.com/mysql-charset-and-table-name-lower-case"/>
     <updated>2014-10-16T00:00:00+08:00</updated>
     <id>http://ningg.github.com/mysql-charset-and-table-name-lower-case</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;关于MySQL，遇到几个问题，列一下：&lt;/p&gt;

&lt;p&gt;乱码问题：字符集不统一；
无法启动：查看错误日志；
无法删除database：先在数据存储目录清理；
table找不到：table区分大小写；&lt;/p&gt;

&lt;p&gt;MySQL版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[devp@localhost ~]$ mysql -V
mysql  Ver 14.14 Distrib 5.6.20, for Linux (x86_64) using  EditLine wrapper
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;乱码问题&lt;/h2&gt;

&lt;p&gt;看官网，mysql的refman（reference manual，参考手册）中，globalization –&amp;gt; character set configuration，其中提到：
system、server、client的charset不一致时，会产生乱码。&lt;/p&gt;

&lt;p&gt;通过如下命令查看一下，当前mysql各个组件的字符集详情：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &quot;character%&quot;;
+--------------------------+----------------------------+
| Variable_name            | Value                      |
+--------------------------+----------------------------+
| character_set_client     | utf8                       |
| character_set_connection | utf8                       |
| character_set_database   | latin1                     |
| character_set_filesystem | binary                     |
| character_set_results    | utf8                       |
| character_set_server     | latin1                     |
| character_set_system     | utf8                       |
| character_sets_dir       | /usr/share/mysql/charsets/ |
+--------------------------+----------------------------+
8 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过命令：show collation，查看当前MySQL支持的字符集。从上面查询结果可知，server的字符集与system、client不同，则，在my.cnf文件中设定server的字符集即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /usr/my.cnf
character_set_server=utf8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新启动MySQL，OK（根据&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_character_set_server&quot;&gt;官网解释&lt;/a&gt;，不需要向数据库重新插入数据）。database的编码方式，不要手动调整，其始终与default database保持一致，若没有default database，则由server的编码方式决定。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;无法启动&lt;/h2&gt;

&lt;p&gt;通过service mysql start，无法启动MySQL，提示出错，略焦躁，不要着急，有错误日志，查看即可。错误日志位置：/var/lib/mysql/*.err，出错信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ERROR] /usr/sbin/mysqld: unknown variable &#39;default-character-set=utf-8&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原来是在my.cnf文件中添加了一个变量，MySQL无法识别，从my.cnf删除即可。官方文档中&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/server-administration.html&quot;&gt;MySQL Server Administration&lt;/a&gt;，有查看错误日志的详细信息，另外，错误日志位置参考&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/installing.html&quot;&gt;Installing and Upgrading MySQL&lt;/a&gt;中提到的安装目录结构。&lt;/p&gt;

&lt;h2 id=&quot;database&quot;&gt;无法删除Database&lt;/h2&gt;

&lt;p&gt;删除database时，出错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; drop database test;
ERROR 1010 (HY000): Error dropping database (can&#39;t rmdir &#39;./test&#39;, errno: 39)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：到MySQL存放数据的路径下（/var/lib/mysql/），将test数据库对应目录（./test）下内容清空，再删除test数据库即可。&lt;/p&gt;

&lt;h2 id=&quot;table&quot;&gt;找不到table&lt;/h2&gt;

&lt;p&gt;MySQL无法连接，提示表格不存在，设置table名称不区分大小写：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;用ROOT登录，修改&lt;code&gt;my.cnf&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;在&lt;code&gt;[mysqld]&lt;/code&gt;下加入一行：&lt;code&gt;lower_case_table_names=1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;重新启动数据库即可;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;思考&lt;/strong&gt;：字段是否区分大小写？数据库还有其他大小写敏感的地方吗？&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/&quot;&gt;MySQL官方文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/installing.html&quot;&gt;Installing and Upgrading MySQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/server-administration.html&quot;&gt;MySQL Server Administration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>如何参与开源项目</title>
     <link href="http://ningg.github.com/how-to-contribute-open-source-project"/>
     <updated>2014-10-15T00:00:00+08:00</updated>
     <id>http://ningg.github.com/how-to-contribute-open-source-project</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;本来今天晚上想浏览一下flume官网的，不过突然看到How to Get Involved，再看看那些贡献了代码的名单，很是羡慕，我这个人爱吹牛，如果我也在名单中，那岂不又能吹牛一把？哈哈~想想都能笑出声。另一方面，用过的开源工具不少，但是如何参与到开源项目中，我还真不知道，碰巧在看flume官网，那就看看如何参与到flume这个开源项目中去吧。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;官方原文地址：&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Contribute&quot;&gt;How to Contribute&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;Welcome contributors! We strive to include everyone’s contributions. This page provides necessary guidelines on how to contribute effectively towards furthering the development and evolution of Flume. You should also read the guide on setting up &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/Development+Environment&quot;&gt;Development Environment&lt;/a&gt; where you will find details on how to checkout, build and test Flume.
（如何下载源码、编译源码、测试源码，需要先阅读&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/Development+Environment&quot;&gt;Development Environment&lt;/a&gt;。）&lt;/p&gt;

&lt;p&gt;Note: This guide applies to general contributors. If you are a committer, please read the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Commit&quot;&gt;How to Commit&lt;/a&gt; as well.
（committer还需阅读&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Commit&quot;&gt;How to Commit&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;what-can-be-contributed&quot;&gt;What can be contributed?&lt;/h2&gt;

&lt;p&gt;There are many ways you can contribute towards the project. A few of these are:（参与方式，有如下几种）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Jump in on discussions&lt;/strong&gt;: It is possible that someone initiates a thread on the mailing list describing a problem that you have dealt with in the past. You can help the project by chiming in on that thread and guiding that user to overcome or workaround that problem or limitation.（查看邮件列表，参与讨论，帮助他人解决问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;File Bugs&lt;/strong&gt;: If you notice a problem and are sure it is a bug, then go ahead and file a JIRA. If however, you are not very sure that it is a bug, you should first confirm it by discussing it on the Mailing Lists.（通过JIRA，提交bug；如果不确定，通过邮件列表提问，确认是否为bug）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Review Code&lt;/strong&gt;: If you see that a JIRA ticket has a “Patch Available” status, go ahead and review it. It cannot be stressed enough that you must be kind in your review and explain the rationale for your feedback and suggestions. Also note that not all review feedback is accepted - often times it is a compromise between the contributor and reviewer. If you are happy with the change and do not spot any major issues, then +1 it. More information on this is available in the following sections.（通过JIRA，检查代码-Patch补丁，提出反馈意见）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Provide Patches&lt;/strong&gt;: We encourage you to assign the relevant JIRA issue to yourself and supply a patch for it. The patch you provide can be code, documentation, build changes, or any combination of these. More information on this is available in the following sections.（通过JIRA，提交代码-patch补丁，可以是代码、文档、编译细节等）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：邮件列表、JIRA，我都没有关注过，也不知道具体怎么用，打算学一下；patch文件了解一点。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;providing-patches&quot;&gt;Providing Patches&lt;/h2&gt;

&lt;p&gt;In order to provide patches, follow these guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Make sure there is a JIRA&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;If you are working on fixing a problem that already has an associated JIRA, then go ahead and assign it to yourself. （JIRA上找到问题，并指派给自己）&lt;/li&gt;
      &lt;li&gt;If it is already assigned to someone else, check with the current assignee before moving it over to your queue.（跟正在解决这个问题的人商量下）&lt;/li&gt;
      &lt;li&gt;If the current assignee has already worked out some part of the fix, suggest that you can take that change over from them and complete the remaining parts.（如果有人已经修复bug的一部分，你可以接手，把余下的做完）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attach the patches as you go through development&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;While small fixes are easily done in a single patch, it is preferable that you attach patches to the JIRA as you go along. This serves as an early feedback mechanism where interested folks can look it over and suggest changes where necessary. It also ensures that if for some reason you are not able to find the time to complete the change, someone else can take up your initial patches and drive them to completion.（本地开发环境测试通过，就提交patch，即使是small fixes）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Before you submit your patch&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;Your change should be well-formatted and readable. Please use two spaces for indentation (no tabs).（保证patch的well-formatted和readable，使用2个space，避免tab）&lt;/li&gt;
      &lt;li&gt;Carefully consider whether you have handled all boundary conditions and have provided sufficiently defensive code where necessary.（代码的边界条件、异常捕获）&lt;/li&gt;
      &lt;li&gt;Add one or more unit tests, if your change is not covered by existing automated tests.（添加单元测试）&lt;/li&gt;
      &lt;li&gt;Insert javadocs and code comments where appropriate.（添加javadocs和comments）&lt;/li&gt;
      &lt;li&gt;Update the &lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide&lt;/a&gt; (&lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=flume.git;a=blob;f=flume-ng-doc/sphinx/FlumeUserGuide.rst;hb=trunk&quot;&gt;source&lt;/a&gt;) if your change affects the Flume config file or any user interface. Include those changes in your patch.（修改文档）&lt;/li&gt;
      &lt;li&gt;Make sure you update the relevant developer documentation, wiki pages, etc. if your change affects the development environment.（修改开发手册）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Test your changes before submitting a review&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;Before you make the JIRA status as “Patch Available”, please test your changes thoroughly. Try any new feature or fix out for yourself, and make sure that it works.（测试充分）&lt;/li&gt;
      &lt;li&gt;Make sure that all unit/integration tests are passing, and that the functionality you have worked on is tested through existing or new tests.（unit/integration测试）&lt;/li&gt;
      &lt;li&gt;You can run all the tests by going to the root level of the source tree and typing &lt;code&gt;mvn clean install&lt;/code&gt;.（mvn clean install，执行测试）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to create a patch file&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;The preferred naming convention for Flume patches is &lt;code&gt;FLUME-12345.patch&lt;/code&gt;, or &lt;code&gt;FLUME-12345-0.patch&lt;/code&gt; where 12345 is the JIRA number. You might want to name successive versions of the patch something like &lt;code&gt;FLUME-12345-1.patch&lt;/code&gt;, &lt;code&gt;FLUME-12345-2.patch&lt;/code&gt;, etc. as you iterate on your changes based on review feedback and re-submit them.（patch命名方式）&lt;/li&gt;
      &lt;li&gt;The command to generate the patch is &lt;code&gt;git diff&lt;/code&gt;. Example:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;$ git diff &amp;gt; /path/to/FLUME-1234-0.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;How to apply someone else’s patch file&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;You can apply someone else’s patch with the GNU patch tool. Example:&lt;/li&gt;
      &lt;li&gt;Contributors may variously submit patches in a couple of different formats. If you get some dialog from the patch tool asking which file you want to patch, try variously the “-p1” or “-p0” flags to patch. Without any additional arguments, git diff generates patches that are applied using patch &lt;code&gt;-p1&lt;/code&gt;. If you use git diff &lt;code&gt;--no-prefix&lt;/code&gt; to generate your patch, you have to apply it using patch &lt;code&gt;-p0&lt;/code&gt;. The ReviewBoard tool understands both formats and is able to apply both types automatically.（&lt;code&gt;patch&lt;/code&gt;命令的选项）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/src/flume # or wherever you keep the root of your Flume source tree
$ patch -p1 &amp;lt; FLUME-1234.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Submitting your patch for review&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;To submit a patch, attach the patch file to the JIRA and change the status of the JIRA to “Patch Available”.（JIRA上，提交patch，修改状态）&lt;/li&gt;
      &lt;li&gt;If the change is non-trivial, please also post it for review on the Review Board. Use the Repository “flume-git” on Review Board.（关键的bug，需要在Review Board上标记一下）&lt;/li&gt;
      &lt;li&gt;Link the JIRA to the Review Board review. JIRA has a feature you can use for this by going to More Actions &amp;gt; Link &amp;gt; Web Link when logged into JIRA.（Review Board上添加JIRA的链接）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identify a reviewer&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;When posting on review board (repository: “flume-git”), always add the Group “Flume” to the list of reviewers.（添加&lt;code&gt;Flume&lt;/code&gt;到reviewers列表）&lt;/li&gt;
      &lt;li&gt;Optionally, you may also add a specific reviewer to the review. You can pick any of the project committers for review. Note that identifying a reviewer does not stop others from reviewing your change. Be prepared for having your change reviewed by others at any time.（可以指定committer作为reviewer，但其他人仍可以review）&lt;/li&gt;
      &lt;li&gt;If you have posted your change for review and no one has had a chance to review it yet, you can gently remind everyone by dropping a note on the developer mailing list with a link to the review.（可在mailing list中添加一个JIRA链接，告知别人来review代码）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Work with reviewers to get your change fleshed out&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;When your change is reviewed, please engage with the reviewer via JIRA or review board to get necessary clarifications and work out other details.（及时给reviewer反馈，多交流）&lt;/li&gt;
      &lt;li&gt;The goal is to ensure that the final state of your change is acceptable to the reviewer so that they can +1 it.（经过数次交流，reviewer确认代码可用了，会点击+1的）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：代码中添加javadocs，集成测试是什么，我还不清楚。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;reviewing-code&quot;&gt;Reviewing Code&lt;/h2&gt;

&lt;p&gt;Flume uses the &lt;a href=&quot;https://reviews.apache.org/groups/Flume&quot;&gt;Apache Review Board&lt;/a&gt; for doing code reviews. In order for a change to be reviewed, it should be either posted on the review board or attached to the JIRA. If the change is a minor change affecting only few lines and does not seem to impact main logic of the affected sources, it need not be posted on the review board. However, if the code change is large or otherwise impacting the core logic of the affected sources, it should be posted on the review board. Feel free to comment on the JIRA requesting the assignee to post the patch for review on review board.（小改动的patch，贴在JIRA上就好了；涉及核心代码的patch，应同时在JIRA和review board上贴出来。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Not all patches attached to a JIRA are ready for review. Sometimes the patches are attached just to solicit early feedback regarding the implementation direction. Feel free to look it over and give your feedback in the JIRA as necessary. Patches are considered ready for review either when the patch has been posted on review board, or the JIRA status has been changed to ‘Patch Available’. Find here a &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20flume%20AND%20status%20%3D%20%22Patch%20Available%22&quot;&gt;list of Flume JIRAs marked Patch Available&lt;/a&gt;. （patch有时候are not ready for review，只是为了征求意见，看看实现的方向对不对）&lt;/p&gt;

&lt;h3 id=&quot;goals-for-code-reviews&quot;&gt;Goals for Code Reviews&lt;/h3&gt;

&lt;p&gt;The net outcome from the review should be the same - which is to ensure the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bugs/Omissions/Regressions are caught before the change is committed to the source control.（问题已解决）&lt;/li&gt;
  &lt;li&gt;The change is subjected to keeping the quality of code high so as to make the overall system sustainable. The implementation of the change should be easily readable, documented where necessary, and must favor simplicity of implementation.（高质量的代码与可正常运行的系统，同等重要。代码质量包括：可读性、文档、实现简洁）&lt;/li&gt;
  &lt;li&gt;Changes are evaluated from the perspective of a consumer (the reviewer) as opposed to the developer, which often brings out subtleties in the implementation that otherwise go unnoticed.（reviewer通常能为代码实现，提供细微改动的建议）&lt;/li&gt;
  &lt;li&gt;The change should be backward compatible and not require extensive work on existing installations in order for it to be consumed. There are exceptions to this in some cases like when work is done on a major release, but otherwise backward compatibility should be upheld at all times. If you are not clear, raise it is as a concern to be clarified during the review.（change保证，后向兼容，即，原来已有的应用代码部分，不需要大改动；如果不确定是否后向兼容，则，说明一下）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;code-review-guidelines&quot;&gt;Code review guidelines&lt;/h3&gt;

&lt;p&gt;Following are some guidelines on how to do a code review. You may use any other approach instead as long as the above stated goals are met. That said, here is an approach that works fine generally:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Understand the problem being solved&lt;/strong&gt;: This often requires going through the JIRA comments and/or mailing list threads where the discussion around the problem has happened in the past. Look for key aspects of the problem such as how it has impacted the users and what, if any, is the suggested way to solve it. You may not find enough information regarding the problem in some cases, in which case - feel free to ask for clarification from the developer contributing the change.（广泛查询，弄清问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Think about how you would solve the problem&lt;/strong&gt;: There are many ways to solve any code problem, with different ways having different merits. Before proceeding to review the change, think through how you would solve the problem if you were the one implementing the solution. Note the various aspects of the problem that your solution might have. Some such aspects to think about are - impact on backward compatibility, overall usability of the system, any impact on performance etc.（制定详细的解决方案，考虑几点：后向兼容、系统全局可用、性能影响）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluate the proposed change in contrast to your solution&lt;/strong&gt;: Unless the change is obvious, it is likely that the implementation of the change you are reviewing is very different from the solution you would go for. Evaluate this change on the various aspects that you evaluated your solution on in the previous step. See how it measures up and give feedback where you think it could be improved.（review代码时，与自己的方案多方面对比）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Look for typical pitfalls&lt;/strong&gt;: Read through the implementation to see if: it needs to be documented at places where the intention is not clear; if all the boundary conditions are being addressed; if the code is defensive enough; if any bad idioms have leaked in such as double check locking etc. In short, check for things that a developer is likely to miss in their own code which are otherwise obvious to someone trying to read and understand the code.（全面检查配套方面：文档中描述是否清晰、边界条件是否考虑、代码安全性怎么样、是否包含bad idioms；总之，站在非developer的角度，看看哪些潜在问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;See if the change is complete&lt;/strong&gt;: Check if the change is such that it affects the user interface. If it does, then the documentation should likely be updated. What about testing - does it have enough test coverage or not? What about other aspects like license headers, copyright statements etc. How about checkstyle and findbugs - did they generate new warnings? How about compiler warnings?（代码层面上，修改全面了吗？是否测试了、warning信息）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Test the change&lt;/strong&gt;: It is very easy to test the change if you have the development environment setup. Run as many tests as you want with the patch. Manually test the change for functionality that you think is not fully covered via the associated tests. If you find a problem, report it.（全面测试、必要的地方手动测）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-give-feedback&quot;&gt;How to give feedback&lt;/h3&gt;

&lt;p&gt;Once you have collected your comments/concerns/feedback you need to send it to back to the contributor. In doing so, please be as courteous as possible and ensure the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your feedback should be clear and actionable. Giving subjective/vague feedback does not add any value or facilitate a constructive dialog.（feedback要清晰、可操作）&lt;/li&gt;
  &lt;li&gt;Where possible, suggest how your concern can be addressed. For example if your testing revealed that a certain use-case is not satisfied, it is acceptable to state that as is, but it would be even better if you could suggest how the developer can address it. Present your suggestion as a possible solution rather than the solution.（对如何解决问题，提出自己的建议）&lt;/li&gt;
  &lt;li&gt;If you do not understand part of the change, or for some reason were not able to review part of the change, state it explicitly so as to encourage other reviewers to jump in and help.（如果读不懂代码，请明确说出来，以方便其他reviewer给予帮助）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you have provided your feedback, wait for the developer to respond. It is possible that the developer may need further clarification on your feedback, in which case you should promptly provide it where necessary. In general, the dialog between the reviewer and developer should lead to finding a reasonable middle ground where key concerns are satisfied and the goals of the review have been met.&lt;/p&gt;

&lt;p&gt;If a change has met all your criteria for review, please +1 the change to indicate that you are happy with it.（如果代码让你满意，请点击+1）&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Contribute&quot;&gt;How to contribute&lt;/a&gt;中提到了各种规范、细节，这些就是参与开源项目的基本准则，大家都按照这个准则来操作，才能保证开源项目的顺利进行。想到了&lt;a href=&quot;http://robbinfan.com/&quot;&gt;Robbin&lt;/a&gt;的一句话：Small is beautiful, constraint is liberty.&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>In-Stream Big Data Processing</title>
     <link href="http://ningg.github.com/in-stream-big-data-processing"/>
     <updated>2014-10-14T00:00:00+08:00</updated>
     <id>http://ningg.github.com/in-stream-big-data-processing</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;英文原文：&lt;a href=&quot;http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/&quot;&gt;In-Stream-big-data-processing&lt;/a&gt;，有人翻译了&lt;a href=&quot;http://blog.csdn.net/idontwantobe/article/details/25938511&quot;&gt;中文版&lt;/a&gt;，也有直接&lt;a href=&quot;http://dirlt.com/in-stream-big-data-processing.html&quot;&gt;中文注释英文版&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The shortcomings and drawbacks of batch-oriented data processing were widely recognized by the Big Data community quite a long time ago. It became clear that real-time query processing and in-stream processing is the immediate need in many practical applications. In recent years, this idea got a lot of traction and a whole bunch of solutions like Twitter’s Storm, Yahoo’s S4, Cloudera’s Impala, Apache Spark, and Apache Tez appeared and joined the army of Big Data and NoSQL systems. This article is an effort to explore techniques used by developers of in-stream data processing systems, trace the connections of these techniques to massive batch processing and OLTP/OLAP databases, and discuss how one unified query engine can support in-stream, batch, and OLAP processing at the same time.&lt;/p&gt;

&lt;p&gt;At Grid Dynamics, we recently faced a necessity to build an in-stream data processing system that aimed to crunch about 8 billion events daily providing fault-tolerance and strict transactioanlity i.e. none of these events can be lost or duplicated. This system has been designed to supplement and succeed the existing Hadoop-based system that had too high latency of data processing and too high maintenance costs. The requirements and the system itself were so generic and typical that we describe it below as a canonical model, just like an abstract problem statement.&lt;/p&gt;

&lt;p&gt;A high-level overview of the environment we worked with is shown in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/cover-2.png&quot; alt=&quot;cover-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One can see that this environment is a typical Big Data installation: there is a set of applications that produce the raw data in multiple datacenters, the data is shipped by means of Data Collection subsystem to HDFS located in the central facility, then the raw data is aggregated and analyzed using the standard Hadoop stack (MapReduce, Pig, Hive) and the aggregated results are stored in HDFS and NoSQL, imported to the OLAP database and accessed by custom user applications. Our goal was to equip all facilities with a new in-stream engine (shown in the bottom of the figure) that processes most intensive data flows and ships the pre-aggregated data to the central facility, thus decreasing the amount of raw data and heavy batch jobs in Hadoop. The design of the in-stream processing engine itself was driven by the following requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;SQL-like functionality&lt;/strong&gt;. The engine has to evaluate SQL-like queries continuously, including joins over time windows and different aggregation functions that implement quite complex custom business logic. The engine can also involve relatively static data (admixtures) loaded from the stores of Aggregated Data. Complex multi-pass data mining algorithms are beyond the immediate goals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Modularity and flexibility&lt;/strong&gt;. It is not to say that one can simply issue a SQL-like query and the corresponding pipeline will be created and deployed automatically, but it should be relatively easy to assemble quite complex data processing chains by linking one block to another.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault-tolerance&lt;/strong&gt;. Strict fault-tolerance is a principal requirement for the engine. As it sketched in the bottom part of the figure, one possible design of the engine is to use distributed data processing pipelines that implement operations like joins and aggregations or chains of such operations, and connect these pipelines by means of fault-tolerant persistent buffers. These buffers also improve modularity of the system by enabling publish/subscribe communication style and easy addition/removal of the pipelines. The pipelines can be stateful and the engine’s middleware should provide a persistent storage to enable state checkpointing. All these topics will be discussed in the later sections of the article.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability with Hadoop&lt;/strong&gt;. The engine should be able to ingest both streaming data and data from Hadoop i.e. serve as a custom query engine atop of HDFS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;High performance and mobility&lt;/strong&gt;. The system should deliver performance of tens of thousands messages per second even on clusters of minimal size. The engine should be compact and efficient, so one can deploy it in multiple datacenters on small clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To find out how such a system can be implemented, we discuss the following topics in the rest of the article:&lt;/p&gt;

&lt;p&gt;First, we explore relations between in-stream data processing systems, massive batch processing systems, and relational query engines to understand how in-stream processing can leverage a huge number of techniques that were devised for other classes of systems.&lt;/p&gt;

&lt;p&gt;Second, we describe a number of patterns and techniques that are frequently used in building of in-stream processing frameworks and systems. In addition, we survey the current and emerging technologies and provide a few implementation tips.&lt;/p&gt;

&lt;p&gt;The article is based on a research project developed at Grid Dynamics Labs. Much of the credit goes to Alexey Kharlamov and Rafael Bagmanov who led the project and other contributors: Dmitry Suslov, Konstantine Golikov, Evelina Stepanova, Anatoly Vinogradov, Roman Belous, and Varvara Strizhkova.&lt;/p&gt;

&lt;h2 id=&quot;basics-of-distributed-query-processing&quot;&gt;Basics of Distributed Query Processing&lt;/h2&gt;

&lt;p&gt;It is clear that distributed in-stream data processing has something to do with query processing in distributed relational databases. Many standard query processing techniques can be employed by in-stream processing engine, so it is extremely useful to understand classical algorithms of distributed query processing and see how it all relates to in-stream processing and other popular paradigms like MapReduce.&lt;/p&gt;

&lt;p&gt;Distributed query processing is a very large area of knowledge that was under development for decades, so we start with a brief overview of the main techniques just to provide a context for further discussion.&lt;/p&gt;

&lt;h3 id=&quot;partitioning-and-shuffling&quot;&gt;Partitioning and Shuffling&lt;/h3&gt;

&lt;p&gt;Distributed and parallel query processing heavily relies on data partitioning to break down a large data set into multiple pieces that can be processed by independent processors. Query processing could consist of multiple steps and each step could require its own partitioning strategy, so data shuffling is an operation frequently performed by distributed databases.&lt;/p&gt;

&lt;p&gt;Although optimal partitioning for selection and projection operations can be tricky (e.g. for range queries), we can assume that for in-stream data filtering it is practically enough to distribute data among the processors using a hash-based partitioning.&lt;/p&gt;

&lt;p&gt;Processing of distributed joins is not so easy and requires a more thorough examination. In distributed environments, parallelism of join processing is achieved through data partitioning, i.e. the data is distributed among processors and each processor employs a serial join algorithm (e.g. nested-loop join or sort-merge join or hash-based join) to process its part of the data. The final results are consolidated from the results obtained from different processors.&lt;/p&gt;

&lt;p&gt;There are two main data partitioning techniques that can be employed by distributed join processing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disjoint data partitioning&lt;/li&gt;
  &lt;li&gt;Divide and broadcast join&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Disjoint data partitioning technique shuffles the data into several partitions in such a way that join keys in different partitions do not overlap. Each processor performs the join operation on each of these partitions and the final result is obtained as a simple concatenation of the results obtained from different processors.  Consider an example where relation R is joined with relation S on a numerical key k and a simple modulo-based hash function is used to produce the partitions (it is assumes that the data initially distributed among the processors based on some other policy):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/disjoint-partitioning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The divide and broadcast join algorithm is illustrated in the figure below. This method divides the first data set into multiple disjoint partitions (R1, R2, and R3 in the figure) and replicates the second data set to all processors. In a distributed database, division typically is not a part of the query processing itself because data sets are initially distributed among multiple nodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/broadcast-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This strategy is applicable for joining of a large relation with a small relation or two small relations. In-stream data processing systems can employ this technique for stream enrichment i.e. joining a static data (admixture) to a data stream.&lt;/p&gt;

&lt;p&gt;Processing of GroupBy queries also relies on shuffling and fundamentally similar to the MapReduce paradigm in its pure form.  Consider an example where the data is grouped by a string key and sum of the numerical values is computed in each group:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/group-by-query.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, computation consists of two steps: local aggregation and global aggregation. These steps basically correspond to Map and Reduce operations. Local aggregation is optional and raw records can be emitted, shuffled, and aggregated on a global aggregation phase.&lt;/p&gt;

&lt;p&gt;The whole point of this section is that all the algorithms above can be naturally implemented using a message passing architectural style i.e. the query execution engine can be considered as a distributed network of nodes connected by the messaging queues. It is conceptually similar to the in-stream processing pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pipelining&quot;&gt;Pipelining&lt;/h3&gt;

&lt;p&gt;In the previous section, we noted that many distributed query processing algorithms resemble message passing networks. However, it is not enough to organize efficient in-stream processing: all operators in a query should be chained in such a way that the data flows smoothly through the entire pipeline i.e. neither operation should block processing by waiting for a large piece of input data without producing any output or by writing intermediate results on disk. Some operations like sorting are inherently incompatible with this concept (obviously, a sorting block cannot produce any output until the entire input is ingested), but in many cases pipelining algorithms are applicable.  A typical example of pipelining is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/join-pipeline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the hash join algorithm is employed to join four relations: R1, S1, S2, and S3 using 3 processors. The idea is to build hash tables for S1, S2 and S3 in parallel and then stream R1 tuples one by one though the pipeline that joins them with S1, S2 and S3 by looking up matches in the hash tables. In-stream processing naturally employs this technique to join a data stream with the static data (admixtures).&lt;/p&gt;

&lt;p&gt;In relational databases, join operation can take advantage of pipelining by using the symmetric hash join algorithm or some of its advanced variants [1,2]. Symmetric hash join is a generalization of hash join. Whereas a normal hash join requires at least one of its inputs to be completely available to produce first results (the input is needed to build a hash table), symmetric hash join is able to produce first results immediately. In contrast to the normal hash join, it maintains hash tables for both inputs and populates these tables as tuples arrive:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/symmetric-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a tuple comes in, the joiner first looks it up in the hash table of the other stream. If match is found, an output tuple is produced. Then the tuple is inserted in its own hash table.&lt;/p&gt;

&lt;p&gt;However, it does not make a lot of sense to perform a complete join of infinite streams. In many cases join is performed on a finite time window or other type of buffer e.g. LFU cache that contains most frequent tuples in the stream. Symmetric hash join can be employed if the buffer is large comparing to the stream rate or buffer is flushed frequently according to some application logic or buffer eviction strategy is not predictable. In other cases, simple hash join is often sufficient since the buffer is constantly full and does not block the processing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/stream-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is worth noting that in-stream processing often deals with sophisticated stream correlation algorithms where records are matched based on scoring metrics, not on field equality condition. A more complex system of buffers can be required for both streams in such cases.&lt;/p&gt;

&lt;h2 id=&quot;in-stream-processing-patterns&quot;&gt;In-Stream Processing Patterns&lt;/h2&gt;

&lt;p&gt;In the previous section, we discussed a number of standard query processing techniques that can be used in massively parallel stream processing. Thus, on a conceptual level, an efficient query engine in a distributed database can act as a stream processing system and vice versa, a stream processing system can act as a distributed database query engine. Shuffling and pipelining are the key techniques of distributed query processing and message passing networks can naturally implement them. However, things are not so simple. In a contrast to database query engines where reliability is not critical because a read-only query can always be restarted, streaming systems should pay a lot of attention to reliable events processing. In this section, we discuss a number of techniques that are used by streaming systems to provide message delivery guarantees and some other patterns that are not typical for standard query processing.&lt;/p&gt;

&lt;h3 id=&quot;stream-replay&quot;&gt;Stream Replay&lt;/h3&gt;

&lt;p&gt;Ability to rewind data stream back in time and replay the data is very important for in-stream processing systems because of the following reasons:
This is the only way to guarantee correct data processing. Even if data processing pipeline is fault-tolerant, it is very problematic to guarantee that the deployed processing logic is defect-free. One can always face a necessity to fix and redeploy the system and replay the data on a new version of the pipeline.&lt;/p&gt;

&lt;p&gt;Issue investigation could require ad hoc queries. If something goes wrong, one could need to rerun the system on the problematic data with better logging or with code alternations.&lt;/p&gt;

&lt;p&gt;Although it is not always the case, the in-stream processing system can be designed in such a way that it re-reads individual messages from the source in case of processing errors and local failures, even if the system in general is fault-tolerant.&lt;/p&gt;

&lt;p&gt;As a result, the input data typically goes from the data source to the in-stream pipeline via a persistent buffer that allows clients to move their reading pointers back and forth.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/replay-buffer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka messaging queue is well known implementation of such a buffer that also supports scalable distributed deployments, fault-tolerance, and provides high performance.
As a bottom line, Stream Replay technique imposes the following requirements of the system design:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The system is able to store the raw input data for a preconfigured period time.&lt;/li&gt;
  &lt;li&gt;The system is able to revoke a part of the produced results, replay the corresponding input data and produce a new version of the results.&lt;/li&gt;
  &lt;li&gt;The system should work fast enough to rewind the data back in time, replay them, and then catch up with the constantly arriving data stream.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lineage-tracking&quot;&gt;Lineage Tracking&lt;/h3&gt;

&lt;p&gt;In a streaming system, events flow through a chain of processors until the result reaches the final destination (like an external database). Each input event produces a directed graph of descendant events (lineage) that ends by the final results. To guarantee reliable data processing, it is necessary to ensure that the entire graph was processed successfully and to restart processing in case of failures.&lt;/p&gt;

&lt;p&gt;Efficient lineage tracking is not a trivial problem. Let us first consider how Twitter’s Storm tracks the messages to guarantee at-least-once delivery semantics (see the diagram below):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All events that emitted by the sources (first nodes in the data processing graph) are marked by a random ID. For each source, the framework maintains a set of pairs [event ID -&amp;gt; signature] for each initial event. The signature is initially initialized by the event ID.&lt;/li&gt;
  &lt;li&gt;Downstream nodes can generate zero or more events based on the received initial event. Each event carries its own random ID and the ID of the initial event.&lt;/li&gt;
  &lt;li&gt;If the event is successfully received and processed by the next node in the graph, this node updates the signature of the corresponding initial event by XORing the signature with (a) ID of the incoming event and (b) IDs of all events produced based on the incoming event. In the part 2 of diagram below, event 01111 produces events 01100, 10010, and 00010, so the signature for event 01111 becomes 11100 (= 01111 (initial value) xor 01111 xor 01100 xor 10010 xor 00010).&lt;/li&gt;
  &lt;li&gt;An event can be produced based on more than one incoming event. In this case, it is attached several initial event and carries more than one initial IDs downstream (yellow-black event in the part 3 of the figure below).&lt;/li&gt;
  &lt;li&gt;The event considered to be successfully processed as soon as its signature turns into zero i.e. the final node acknowledged that the last event in the graph was processed successfully and no events were emitted downstream. The framework sends a commit message to the source node (see part 3 in the diagram below).&lt;/li&gt;
  &lt;li&gt;The framework traverses a table of the initial events periodically looking for old uncommitted events (events with non-zero signature). Such events are considered as failed and the framework asks the source nodes to replay them.&lt;/li&gt;
  &lt;li&gt;It is important to note that the order of signature updates is not important due to commutative nature of the XOR operation. In the figure below, acknowledgements depicted in the part 2 can arrive after acknowledgements depicted in the part 3. This enables fully asynchronous processing.&lt;/li&gt;
  &lt;li&gt;One can note that the algorithm above is not strictly reliable – the signature could turn into zero accidentally due to unfortunate combination of IDs. However, 64-bit IDs are sufficient to guarantee a very low probability of error, about 2^(-64), that is acceptable in almost all practical applications. As result, the table of signatures could have a small memory footprint.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/lineage-tracking-storm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The described approach is elegant due to its decentrilized nature: nodes act independently sending acknowledgement messages, there is no cental entity that tracks all lineages explicitly. However, it could be difficult to manage transactional processing in this way for flows that maintain sliding windows or other buffers. For example, processing on a sliding window can involve hundreds of thousands events at each moment of time, so it becomes difficult to manage acknowledgements because many events stay uncommitted or computational state should be persisted frequently.&lt;/p&gt;

&lt;p&gt;An alternative approach is used in Apache Spark [3]. The idea is to consider the final result as a function of the incoming data. To simplify lineage tracking, the framework processes events in batches, so the result is a sequence of batches where each batch is a function of the input batches. Resulting batches can be computed in parallel and if some computation fails, the framework simply reruns it. Consider an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/stream-join-microbatching-tx.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the framework joins two streams on a sliding window and then the result passes through one more processing stage. The framework considers the incoming streams not as streams, but as set of batches. Each batch has an ID and the framework can fetch it by the ID at any moment of time. So, stream processing can be represented as a bunch of transactions where each transaction takes a group of input batches, transforms them using a processing function, and persists a result. In the figure above, one of such transactions is highlighted in red. If the transaction fails, the framework simply reruns it. It is important that transactions can be executed in parallel.&lt;/p&gt;

&lt;p&gt;This simple but powerful paradigm enables centralized transaction management and inherently provides exactly-once message processing semantics. It is worth noting that this technique can be used both for batch processing and for stream processing because it treats the input data as a set of batches regardless to their streaming of static nature.&lt;/p&gt;

&lt;h3 id=&quot;state-checkpointing&quot;&gt;State Checkpointing&lt;/h3&gt;

&lt;p&gt;In the previous section we have considered the lineage tracking algorithm that uses signatures (checksums) to provide at-least-one message delivery semantics. This technique improves reliability of the system, but it leaves at least two major open questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In many cases, exactly-once processing semantics is required. For example, the pipeline that counts events can produce incorrect results if some messages will be delivered twice.&lt;/li&gt;
  &lt;li&gt;Nodes in the pipeline can have a computational state that is updated as the messages processed. This state can be lost in case of node failure, so it is necessary to persist or replicate it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Twitter’s Storm addresses these issues by using the following protocol:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Events are grouped into batches and each batch is associated with a transaction ID. A transaction ID is a monotonically growing numerical value (e.g. the first batch has ID 1, the second ID 2, and so on). If the pipeline fails to process a batch, this batch is re-emitted with the same transaction ID.&lt;/li&gt;
  &lt;li&gt;First, the framework announces to the nodes in the pipeline that a new transaction attempt is started. Second, the framework to sends the batch through the pipeline. Finally, the framework announces that transaction attempt if completed and all nodes can commit their state e.g. update it in the external database.&lt;/li&gt;
  &lt;li&gt;The framework guarantees that commit phases are globally ordered across all transactions i.e. the transaction 2 can never be committed before the transaction 1. This guarantee enables processing nodes to use following logic of persistent state updates:
    &lt;ul&gt;
      &lt;li&gt;The latest transaction ID is persisted along with the state.&lt;/li&gt;
      &lt;li&gt;If the framework requests to commit the current transaction with the ID that differs from the ID value persisted in the database, the state can be updated e.g. a counter in the database can be incremented. Assuming a strong ordering of transactions, such update will happen exactly one for each batch.&lt;/li&gt;
      &lt;li&gt;If the current transaction ID equals to the value persisted in the storage, the node skips the commit because this is a batch replay. The node must have processed the batch earlier and updated the state accordingly, but the transaction failed due to an error somewhere else in the pipeline.&lt;/li&gt;
      &lt;li&gt;Strong order of commits is important to achieve exactly-once processing semantics. However, strictly sequential processing of transactions is not feasible because first nodes in the pipeline will often be idle waiting until processing on the downstream nodes is completed. This issues can be alleviated by allowing parallel processing of transactions but serialization of commit steps only, as it shown in the figure below:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/pipelining-commits-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This technique allows one to achieve exactly-once processing semantics assuming that data sources are fault-tolerant and can be replayed. However, persistent state updates can cause serious performance degradation even if large batches are used. By this reason, the intermediate computational state should be minimized or avoided whenever possible.&lt;/p&gt;

&lt;p&gt;As a footnote, it is worth mentioning that state writing can be implemented in different ways. The most straightforward approach is to dump in-memory state to the persistent store as part of the transaction commit process. This does not work well for large states (sliding windows an so on). An alternative is to write a kind of transaction log i.e. a sequence of operations that transform the old state into the new one (for a sliding window it can be a set of added and evicted events). This approach complicates crash recovery because the state has to be reconstructed from the log, but can provide performance benefits in a variety of cases.&lt;/p&gt;

&lt;h3 id=&quot;additive-state-and-sketches&quot;&gt;Additive State and Sketches&lt;/h3&gt;

&lt;p&gt;Additivity of intermediate and final computational results is an important property that drastically simplifies design, implementation, maintenance, and recovery of in-stream data processing systems. Additivity means that the computational result for a larger time range or a larger data partition can be calculated as a combination of results for smaller time ranges or smaller partitions. For example, a daily number of page views can be calculated as a sum of hourly numbers of page views. Additive state allows one to split processing of a stream into processing of batches that can be computed and re-computed independently and, as we discussed in the previous sections, this helps to simplify lineage tracking and reduce complexity of state maintenance.&lt;/p&gt;

&lt;p&gt;It is not always trivial to achieve additivity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In many cases, additivity is indeed trivial. For example, simple counters are additive.&lt;/li&gt;
  &lt;li&gt;In some cases, it is possible to achieve additivity by storing a small amount of additional information. For example, consider a system that calculates average purchase value in the internet shop for each hour. Daily average cannot be obtained from 24 hourly average values. However, the system can easily store a number of transactions along with each hourly average and it is enough to calculate the daily average value.&lt;/li&gt;
  &lt;li&gt;In many cases, it is very difficult or impossible to achieve additivity. For example, consider a system that counts unique visitors on some internet site. If 100 unique users visited the site yesterday and 100 unique user visited the site today, the total number of unique user for two days can be from 100 to 200 depends on how many users visited the site both yesterday and today. One have to maintain lists of user IDs to achieve additivity through intersection/union of the ID lists. Size and processing complexity for these lists can be comparable to the size and processing complexity of the raw data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sketches is a very efficient way to transform non-additive values into additive. In the previous example, lists of ID can be replaced by compact additive statistical counters. These counters provide approximations instead of precise result, but it is acceptable for many practical applications. Sketches are very popular in certain areas like internet advertising and can be considered as an independent pattern of in-stream processing. A thorough overview of the sketching techniques can be found in [5].&lt;/p&gt;

&lt;h3 id=&quot;logical-time-tracking&quot;&gt;Logical Time Tracking&lt;/h3&gt;

&lt;p&gt;It is very common for in-stream computations to depend on time: aggregations and joins are often performed on sliding time windows; processing logic often depends on a time interval between events and so on. Obviously, the in-stream processing system should have a notion of application’s view of time, instead of CPU wall-clock. However, proper time tracking is not trivial because data streams and particular events can be replayed in case of failures. It is often a good idea to have a notion of global logical time that can be implemented as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All events should be marked with a timestamp generated by the original application.&lt;/li&gt;
  &lt;li&gt;Each processor in a pipeline tracks the maximal timestamp it has seen in a stream and updates a global persistent clock by this timestamp if the global clock is behind. All other processors synchronize their time with the global clock.&lt;/li&gt;
  &lt;li&gt;Global clock can be reset in case of data replay.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aggregation-in-a-persistent-store&quot;&gt;Aggregation in a Persistent Store&lt;/h3&gt;

&lt;p&gt;We already have discussed that persistent store can be used for state checkpointing. However, it not the only way to employ an external store for in-stream processing. Let us consider an example that employs Cassandra to join multiple data streams over a time window. Instead of maintaining in-memory event buffers, one can simply save all incoming events from all data streams to Casandra using a join key as row key, as it shown in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/cassandra-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other side, the second process traverses the records periodically, assembles and emits joined events, and evicts the events that fell out of the time window. Cassandra even can facilitate this activity by sorting events according to their timestamps.
It is important to understand that such techniques can defeat the whole purpose of in-stream data processing if implemented incorrectly – writing individual events to the data store can introduce a serious performance bottleneck even for fast stores like Cassandra or Redis. On the other hand, this approach provides perfect persistence of the computational state and different performance optimizations – say, batch writes – can help to achieve acceptable performance in many use cases.&lt;/p&gt;

&lt;h3 id=&quot;aggregation-on-a-sliding-window&quot;&gt;Aggregation on a Sliding Window&lt;/h3&gt;

&lt;p&gt;In-stream data processing frequently deals with queries like “What is the sum of the values in the stream over last 10 minutes?” i.e. with continuous queries on a sliding time window. A straightforward approach to processing of such queries is to compute the aggregation function like sum for each instance of the time window independently. It is clear that this approach is not optimal because of the high similarity between two sequential instances of the time window. If the window at the time T contains samples {s(0), s(1), s(2), …, s(T-1), s(T)}, then the window at the time T+1 contains samples {s(1), s(2), s(3), …, s(T), s(T+1)}. This observation suggests that incremental processing might be used.&lt;/p&gt;

&lt;p&gt;Incremental computations over sliding windows is a group of techniques that are widely used in digital signal processing, in both software and hardware. A typical example is a computation of the sum function. If the sum over the current time window is known, then the sum over the next time window can be computed by adding a new sample and subtracting the eldest sample in the window:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/inremental-aggregation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar techniques exist not only for simple aggregations like sums or products, but also for more complex transformations. For example, the SDFT (Sliding Discreet Fourier Transform) algorithm [4] is a computationally efficient alternative to per-window calculation of the FFT (Fast Fourier Transform) algorithm.&lt;/p&gt;

&lt;h2 id=&quot;query-processing-pipeline-storm-cassandra-kafka&quot;&gt;Query Processing Pipeline: Storm, Cassandra, Kafka&lt;/h2&gt;

&lt;p&gt;Now let us return to the practical problem that was stated in the beginning of this article. We have designed and implemented our in-stream data processing system on top of Storm, Kafka, and Cassandra adopting the techniques described earlier in this article. Here we provide just a very brief overview of the solution – a detailed description of all implementation pitfalls and tricks is too large and probably requires a separate article.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/storm-kafka-cassandra-system.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The system naturally uses Kafka 0.8 as a partitioned fault-tolerant event buffer to enable stream replay and improve system extensibility by easy addition of new event producers and consumers. Kafka’s ability to rewind read pointers also enables random access to the incoming batches and, consequently, Spark-style lineage tracking. It is also possible to point the system input to HDFS to process the historical data.&lt;/p&gt;

&lt;p&gt;Cassandra is employed for state checkpointing and in-store aggregation, as described earlier. In many use cases, it also stores the final results.&lt;/p&gt;

&lt;p&gt;Twitter’s Storm is a backbone of the system. All active query processing is performed in Storm’s topologies that interact with Kafka and Cassandra. Some data flows are simple and straightforward: the data arrives to Kafka; Storm reads and processes it and persist the results to Cassandra or other destination. Other flows are more sophisticated: one Storm topology can pass the data to another topology via Kafka or Cassandra. Two examples of such flows are shown in the figure above (red and blue curved arrows).&lt;/p&gt;

&lt;h2 id=&quot;towards-unified-big-data-processing&quot;&gt;Towards Unified Big Data Processing&lt;/h2&gt;

&lt;p&gt;It is great that the existing technologies like Hive, Storm, and Impala enable us to crunch Big Data using both batch processing for complex analytics and machine learning, and real-time query processing for online analytics, and in-stream processing for continuous querying. Moreover, techniques like Lambda Architecture [6, 7] were developed and adopted to combine these solutions efficiently. This brings us to the question of how all these technologies and approaches could converge to a solid solution in the future.  In this section, we discuss the striking similarity between distributed relational query processing, batch processing, and in-stream query processing to figure out the technologies that could cover all these use cases and, consequently, have the highest potential in this area.&lt;/p&gt;

&lt;p&gt;The key observation is that relational query processing, MapReduce, and in-stream processing could be implemented using exactly the same concepts and techniques like shuffling and pipelining. At the same time:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In-stream processing could require strict data delivery guarantees and persistence of the intermediate state. These properties are not crucial for batch processing where computations can be easily restarted.&lt;/li&gt;
  &lt;li&gt;In-stream processing is inseparable from pipelining. For batch processing, pipelining is not so crucial and even inapplicable in certain cases. Systems like Apache 
Hive are based on staged MapReduce with materialization of the intermediate state and do not take full advantage of pipelining.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two statement above imply that tunable persistence (in-memory message passing versus on-disk materialization) and reliability are the distinctive features of the imaginary query engine that provides a set of processing primitives and interfaces to the high-level frameworks:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/unified-engine.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Among the emerging technologies, the following two are especially notable in the context of this discussion:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Tez [8], a part of the Stinger Initiative [9]. Apache Tez is designed to succeed the MapReduce framework introducing a set of fine-grained query processing primitives. The goal is to enable frameworks like Apache Pig and Apache Hive to decompose their queries and scripts into efficient query processing pipelines instead of sequences of MapReduce jobs that are generally slow due to materialization of intermediate results.&lt;/li&gt;
  &lt;li&gt;Apache Spark [10]. This project is probably the most advanced and promising technology for unified Big Data processing that already includes a batch processing framework, SQL query engine, and a stream processing framework.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;A. Wilschut and P. Apers, “Dataflow Query Execution in a Parallel Main-Memory Environment “&lt;/li&gt;
  &lt;li&gt;T. Urhan and M. Franklin, “XJoin: A Reactively-Scheduled Pipelined Join Operator“&lt;/li&gt;
  &lt;li&gt;M. Zaharia, T. Das, H. Li, S. Shenker, and I. Stoica, “Discretized Streams: An Efﬁcient and Fault-Tolerant Model for Stream Processing on Large Clusters”&lt;/li&gt;
  &lt;li&gt;E. Jacobsen and R. Lyons, &lt;a href=&quot;http://www.ingelec.uns.edu.ar/pds2803/materiales/articulos/slidingdft_bw.pdf&quot;&gt;“The Sliding DFT“&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;A. Elmagarmid, Data Streams Models and Algorithms&lt;/li&gt;
  &lt;li&gt;N. Marz, &lt;a href=&quot;http://www.databasetube.com/database/big-data-lambda-architecture/&quot;&gt;“Big Data Lambda Architecture”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. Kinley, &lt;a href=&quot;http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting&quot;&gt;“The Lambda architecture: principles for architecting realtime Big Data systems”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/hadoop/tez/&quot;&gt;http://hortonworks.com/hadoop/tez/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/stinger/&quot;&gt;http://hortonworks.com/stinger/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark-project.org/&quot;&gt;http://spark-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;这次看到dirtysalt的文章：&lt;a href=&quot;http://dirlt.com/in-stream-big-data-processing.html&quot;&gt;In-stream-big-data-processing（英文版+中文注释）&lt;/a&gt;，顿时解决了困扰自己的一个问题，英文的好文章，如何做记录？直接翻译中文？直接转载中文？NO，最佳方式当然是在原文基础上，添加自己的注释，OK，作者dirtysalt已经在做了，今后我也这么办。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>MySQL安装</title>
     <link href="http://ningg.github.com/install-mysql"/>
     <updated>2014-10-13T00:00:00+08:00</updated>
     <id>http://ningg.github.com/install-mysql</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;“在服务器上装个MySQL，我要用。”“OK，稍等。”这么一稍等，就等了40mins，而且，自己之前整理的MySQL安装步骤，并不完整，借这次安装的机会，整理一下吧。&lt;/p&gt;

&lt;h2 id=&quot;mysql&quot;&gt;安装MySQL&lt;/h2&gt;

&lt;p&gt;通常安装MySQL分为几个基本步骤：本地安装MySQL、设置MySQL的root密码、开启MySQL允许远程访问。&lt;/p&gt;

&lt;h3 id=&quot;mysql-1&quot;&gt;本地安装MySQL&lt;/h3&gt;

&lt;p&gt;Linux环境下安装MySQL，有两种方式：rpm包方式、yum源方式（暂不考虑编译源代码方式）。&lt;/p&gt;

&lt;h4 id=&quot;rpm&quot;&gt;rpm包方式&lt;/h4&gt;

&lt;p&gt;到MySQL官网，下载MySQL社区开源版本，详细版本号为：MySQL-5.6.21-1.linux_glibc2.5.x86_64.rpm-bundle.tar。这是一个集合，包含了如下组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MySQL-server&lt;/li&gt;
  &lt;li&gt;MySQL-client&lt;/li&gt;
  &lt;li&gt;MySQL-embedded&lt;/li&gt;
  &lt;li&gt;MySQL-shared&lt;/li&gt;
  &lt;li&gt;MySQL-shared-compat&lt;/li&gt;
  &lt;li&gt;MySQL-test&lt;/li&gt;
  &lt;li&gt;MySQL-devel&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;mysql-2&quot;&gt;1.解压MySQL安装包&lt;/h5&gt;

&lt;p&gt;执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]#tar -xf MySQL-5.6.21-1.linux_glibc2.5.x86_64.rpm-bundle.tar
[root@ningg mysql]#ls
	MySQL-server-5.6.20-1.el6.x86_64.rpm
	MySQL-client-5.6.20-1.el6.x86_64.rpm      
	MySQL-shared-5.6.20-1.el6.x86_64.rpm
	MySQL-devel-5.6.20-1.el6.x86_64.rpm       
	MySQL-shared-compat-5.6.20-1.el6.x86_64.rpm
	MySQL-embedded-5.6.20-1.el6.x86_64.rpm    
	MySQL-test-5.6.20-1.el6.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&quot;mysql-3&quot;&gt;2.创建MySQL系统管理员&lt;/h5&gt;

&lt;p&gt;执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]#groupadd mysql
[root@ningg mysql]#useradd -g mysql mysql
[root@ningg mysql]#id mysql
	uid=27(mysql) gid=27(mysql) groups=27(mysql)
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&quot;mysql-rpm&quot;&gt;3.安装MySQL rpm包&lt;/h5&gt;

&lt;p&gt;执行命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]#rpm -ivh &quot;*.rpm&quot;
Preparing...              ########### [100%]
   1:MySQL-devel          ########### [ 14%]
   2:MySQL-client         ########### [ 29%]
   3:MySQL-test           ########### [ 43%]
   4:MySQL-embedded       ########### [ 57%]
   5:MySQL-shared-compat  ########### [ 71%]
   6:MySQL-shared         ########### [ 86%]
   7:MySQL-server         ########### [100%]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;补充一下，如果安装出现意外，希望卸载MySQL组件，则，卸载顺序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg ~]# rpm -e MySQL-server-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-embedded-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-shared-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-devel-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-test-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-client-5.5.24-1.rhel5
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;思考：如何保证是mysql用户启动的MySQL？如果使用root运行MySQL，一旦MySQL进程被Hacker控制，Hacker就拥有了root权限？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;yum&quot;&gt;yum源方式&lt;/h4&gt;

&lt;p&gt;（doing…）&lt;/p&gt;

&lt;h3 id=&quot;mysqlroot&quot;&gt;设置MySQL的root密码&lt;/h3&gt;

&lt;p&gt;更详细内容参考&lt;a href=&quot;http://dev.mysql.com/doc/mysql-security-excerpt/5.6/en/user-account-management.html&quot;&gt;MySQL 5.6 Manual: User account management&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;mysql-4&quot;&gt;1.修改MySQL启动配置&lt;/h4&gt;

&lt;p&gt;查找&lt;code&gt;my.cnf&lt;/code&gt;文件位置，两个命令：&lt;code&gt;locate &quot;my.cnf&quot;&lt;/code&gt;和&lt;code&gt;find / -name &quot;my.cnf&quot;&lt;/code&gt;（备注：两个命令有差异，具体参考&lt;a href=&quot;http://312788172.iteye.com/blog/730280&quot;&gt;文章&lt;/a&gt;）。
通常文件位置&lt;code&gt;/etc/my.cnf&lt;/code&gt;或者&lt;code&gt;/usr/my.cnf&lt;/code&gt;，依具体情况行事，在其中设置不启用授权表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]# vim /usr/my.cnf
# For advice on how to change settings please see
# http://dev.mysql.com/doc/refman/5.6/en/server-configuration-defaults.html

[mysqld]
# 新增加下面一行，含义：设置不启用授权表
skip-grant-tables
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;root&quot;&gt;2.重置root密码&lt;/h4&gt;

&lt;p&gt;重新启动MySQL：&lt;code&gt;service mysql restart&lt;/code&gt;，然后进行如下操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]# mysql

mysql&amp;gt; use mysql
mysql&amp;gt; update user set Password=PASSWORD(&#39;1234&#39;) where User=&#39;root&#39;;
mysql&amp;gt; flush privileges;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后，修改&lt;code&gt;my.cnf&lt;/code&gt;文件，注释掉&lt;code&gt;skip-grant-tables&lt;/code&gt;；然后，重启MySQL：&lt;code&gt;service mysql restart&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;3.补充说明&lt;/h4&gt;

&lt;p&gt;针对msyql数据库下的user表，说明几点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; use mysql
#使用下面命令查看表格当前记录
mysql&amp;gt; select * from user \G;
#查看user表格的字段类型
mysql&amp;gt; describe user;
#查看Host\User\Password字段；
mysql&amp;gt; select host,user,password from user;
+-----------+------+---------------+
| host      | user | password      |
+-----------+------+---------------+
| %         | root | *81B936FD50F6 |
| cib02167  | root | *81B936FD50F6 |
| 127.0.0.1 | root | *81B936FD50F6 |
| ::1       | root | *81B936FD50F6 |
+-----------+------+---------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;user表：用户信息、用户权限、密码、可以登录访问的远端主机等。&lt;/li&gt;
  &lt;li&gt;host字段：表示登录MySQL的主机，可以是IP、主机名，如果为&lt;code&gt;%&lt;/code&gt;则表示任何客户端主机都能登录，建议开发时，设置为&lt;code&gt;%&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;命令SET PASSWORD&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#设置用户在不同主机环境下的登录密码
SET PASSWORD FOR &#39;root&#39;@&#39;%&#39; = PASSWORD(&#39;newpass&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令UPDATE&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 可修改user表格内容（MySQL不区分大小写）
update user set password=password(&#39;new-pw&#39;) where user=&#39;root&#39; and host=&#39;%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新授权表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 修改用户信息等，务必flush
flush privileges
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;mysql-5&quot;&gt;开启MySQL允许远程访问&lt;/h3&gt;

&lt;p&gt;在user表中，添加一条&lt;code&gt;user=root&lt;/code&gt;且&lt;code&gt;host=%&lt;/code&gt;的记录，并且通过SET PASSWORD命令重置密码即可。host字段取值&lt;code&gt;%&lt;/code&gt;，即表示任何客户端机器，涵盖远程访问的机器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 可修改user表格内容（MySQL不区分大小写）
update user set password=password(&#39;new-pw&#39;) where user=&#39;root&#39; and host=&#39;%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;疑问：如果没有&lt;code&gt;user=root&lt;/code&gt;且&lt;code&gt;host=%&lt;/code&gt;的记录怎么办？
RE：新建一条记录，或者将&lt;code&gt;user=root&lt;/code&gt;的记录，利用update命令修改为&lt;code&gt;host=%&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-2&quot;&gt;常见问题&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;问题1&lt;/strong&gt;：You must SET PASSWORD before executing this statement&lt;/p&gt;

&lt;p&gt;解决办法：根据提示直接使用set password命令重置密码即可，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; set password=password(&#39;new-pw&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;问题2&lt;/strong&gt;：这个本质上是不是MySQL的管理问题？有哪些用户，哪些用户可以远程登录？&lt;/p&gt;

&lt;p&gt;回应：是的，你很用心在思考，官方文档有很多细节，很有意思的，可以看一下，具体：&lt;code&gt;MySQL Manual&lt;/code&gt;–&lt;code&gt;Security in MySQL&lt;/code&gt;–&lt;code&gt;User Account Management&lt;/code&gt;，有详尽的说明。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/mysql-security-excerpt/5.6/en/grant-table-structure.html&quot;&gt;MySQL 5.6 Manual: Privilege system grant tables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/mysql-security-excerpt/5.6/en/user-account-management.html&quot;&gt;MySQL 5.6 Manual: User account management&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/downloads/mysql/&quot;&gt;MySQL开源社区版本下载地址&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/&quot;&gt;MySQL官方文档下载地址&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://312788172.iteye.com/blog/730280&quot;&gt;linux下which、whereis、locate、find 命令的区别&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;这篇文章，写的都是小问题，如果读过MySQL的官方文档，就知道&lt;code&gt;User Account Management&lt;/code&gt;的基本知识了，也不用遇到什么问题就蒙圈了，用一个东西，先浏览学习一下官方文档很必要的，看似浪费时间，其实是捷径。当然，一个个小问题折磨自己，才让我意识到读一遍MySQL官方文档的好处。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>消息队列（Message Queue）基本概念</title>
     <link href="http://ningg.github.com/message-queue-intro"/>
     <updated>2014-10-08T00:00:00+08:00</updated>
     <id>http://ningg.github.com/message-queue-intro</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;之前做日志收集模块时，用到flume，另外也有的方案，集成kafaka来提升系统可扩展性，其中涉及到&lt;code&gt;消息队列&lt;/code&gt;，当时自己并不清楚为什么要使用&lt;code&gt;消息队列&lt;/code&gt;，而在我自己提出的原始日志采集方案中不适用&lt;code&gt;消息队列&lt;/code&gt;时，有几个基本问题：1.日志文件上传过程，有个基本的&lt;code&gt;生产者-消费者&lt;/code&gt;问题；2.另外系统崩溃时，数据丢失的处理问题。&lt;/p&gt;

&lt;p&gt;今天，几位同事再次谈到&lt;code&gt;消息队列&lt;/code&gt;这么个东西，很NB的样子，我也想弄清楚，OK，搞起。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;什么是消息队列&lt;/h2&gt;

&lt;p&gt;消息队列（Message Queue，简称MQ），从字面意思上看，本质是个队列，FIFO先入先出，只不过队列中存放的内容是&lt;code&gt;message&lt;/code&gt;而已。其主要用途：不同进程Process/线程Thread之间通信。为什么会产生&lt;code&gt;消息队列&lt;/code&gt;？这个问题问的好，我大概查了一下，没有查到最初产生消息队列的背景，但我猜测可能几个原因：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不同进程（process）之间传递消息时，两个进程之间耦合程度过高，改动一个进程，引发必须修改另一个进程，为了隔离这两个进程，在两进程间抽离出一层（一个模块），所有两进程之间传递的消息，都必须通过&lt;code&gt;消息队列&lt;/code&gt;来传递，单独修改某一个进程，不会影响另一个；&lt;/li&gt;
  &lt;li&gt;不同进程（process）之间传递消息时，为了实现标准化，将消息的格式规范化了，并且，某一个进程接受的消息太多，一下子无法处理完，并且也有先后顺序，必须对收到的消息进行排队，因此诞生了事实上的&lt;code&gt;消息队列&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不管到底是什么原因催生了&lt;code&gt;消息队列&lt;/code&gt;，总之，上面两个猜测是其实际应用的典型场景。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;为什么要用&lt;/h2&gt;

&lt;p&gt;切合前一部分猜测的&lt;code&gt;消息队列&lt;/code&gt;产生背景，其主要解决两个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;系统解耦：项目开始时，无法确定最终需求，不同进程间，添加一层，实现解耦，方便今后的扩展。&lt;/li&gt;
  &lt;li&gt;消息缓存：系统中，不同进程处理消息速度不同，MQ，可以实现不同Process之间的缓冲，即，写入MQ的速度可以尽可能地快，而处理消息的速度可以适当调整（或快、或慢）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面针对&lt;strong&gt;系统解耦&lt;/strong&gt;、&lt;strong&gt;消息缓存&lt;/strong&gt;两点，来分析实际应用&lt;code&gt;消息队列&lt;/code&gt;过程中，可能遇到的问题。虚拟场景：Process_A通过消息队列MQ_1向Process_B传递消息，几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;针对MQ_1中一条消息message_1，如何确保Process_B从MQ_1中只取一次message_1，不会重复多次取出message_1？&lt;/li&gt;
  &lt;li&gt;如果MQ_1中message_1已经被Process_B取出，正在处理的关键时刻，Process_B崩溃了，哭啊，我的问题是，如果重启Process_B，是否会丢失message_1？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不要着急，阅读了下面的简要介绍后，水到渠成，上面几个问题就可以解决了。
消息队列有如下几个好处，这大都是由其&lt;strong&gt;系统解耦&lt;/strong&gt;和&lt;strong&gt;消息缓存&lt;/strong&gt;两点扩展而来的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提升系统可靠性：
    &lt;ul&gt;
      &lt;li&gt;冗余：Process_B崩溃之后，数据并不会丢失，因为MQ多采用&lt;code&gt;put-get-delete&lt;/code&gt;模式，即，仅当确认message被完成处理之后，才从MQ中移除message；&lt;/li&gt;
      &lt;li&gt;可恢复：MQ实现解耦，部分进程崩溃，不会拖累整个系统瘫痪，例，Process_B崩溃之后，Process_A仍可向MQ中添加message，并等待Process_B恢复；&lt;/li&gt;
      &lt;li&gt;可伸缩：有较强的峰值处理能力，通常应用会有突发的访问流量上升情况，使用足够的硬件资源时刻待命，空闲时刻较长，资源浪费，而&lt;code&gt;消息队列&lt;/code&gt;却能够平滑峰值流量，缓解系统组件的峰值压力；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;提升系统可扩展性：
    &lt;ul&gt;
      &lt;li&gt;调整模块：由于实现解耦，可以很容易调整，消息入队速率、消息处理速率、增加新的Process；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;其他：
    &lt;ul&gt;
      &lt;li&gt;单次送达：保证MQ中一个message被处理一次，并且只被处理一次，本质：get获取一个message后，这一message即被预定，同一进程不会再次获取这一message，当且仅当进程处理完这一message后，MQ中会delete这个message，否则，过一段时间后，这一message自动解除被预订状态，进程能够重新预定这个message；&lt;/li&gt;
      &lt;li&gt;排序保证：即，满足队列的FIFO，先入先出策略；&lt;/li&gt;
      &lt;li&gt;异步通信：很多场景下，不会立即处理消息，这是，可以在MQ中存储message，并在某一时刻再进行处理；&lt;/li&gt;
      &lt;li&gt;数据流的阶段性能定位：获取用户某一操作的各个阶段（通过message来标识），捕获不同阶段的耗时，可用于定位系统瓶颈。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;常用的消息队列&lt;/h2&gt;

&lt;p&gt;（doing…）&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;消息队列&lt;/code&gt;实现了进程间通信的升级，如下图所示：&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;top 10 uses for message queue：&lt;a href=&quot;http://blog.iron.io/2012/12/top-10-uses-for-message-queue.html&quot;&gt;英文原文&lt;/a&gt;、&lt;a href=&quot;/download/message-queue-intro/top-10-mq.pdf&quot;&gt;pdf版本&lt;/a&gt;、&lt;a href=&quot;http://www.oschina.net/translate/top-10-uses-for-message-queue&quot;&gt;中文译文&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Message_queue&quot;&gt;Message Queue wiki&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bbs.csdn.net/topics/110160741&quot;&gt;http://bbs.csdn.net/topics/110160741&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/yuanyi_wang/archive/2009/12/30/1636178.html&quot;&gt;http://www.cnblogs.com/yuanyi_wang/archive/2009/12/30/1636178.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.php1.cn/article/9865.html&quot;&gt;http://www.php1.cn/article/9865.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>想和自己谈一谈</title>
     <link href="http://ningg.github.com/personal-think"/>
     <updated>2014-09-30T00:00:00+08:00</updated>
     <id>http://ningg.github.com/personal-think</id>
     <content type="html">&lt;p&gt;今天读了&lt;code&gt;***&lt;/code&gt;的博客，特别是每年的个人总结，基本上2004–2013的都看了，希望能从中窥探一点前辈工作十余年的体会，同时也想看看有没有能够借鉴的，万幸，读的较为认真，我也产生了一点想法，借这个时机，我想和自己谈一谈。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;有个打算，有点追求&lt;/h2&gt;

&lt;p&gt;整体上，我需要有些时间较长的总结、计划，比如一年、6个月、3个月的打算；每次打算，需要要涵盖3个方面：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;前一段，我做了什么？有什么可以积累的地方、有什么改进的地方；&lt;/li&gt;
  &lt;li&gt;接下来我需要做什么？计划怎么行动？&lt;/li&gt;
  &lt;li&gt;做这些事情，有一个基本的理念，有吗？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;说到较长时间的打算，我在学校读书的最后一年，曾有过3个月的打算，其实打算开始之前，已经陆续铺垫有3个多月，事后总结，最初的打算完成了60%，当时获得了很大的信心，那种幸福、自信的感觉为自己今后的一些决定带来了内心的支撑，特别是，当出现争议的时候，自己能够在自己进行深入分析后做决断。&lt;/p&gt;

&lt;p&gt;自己做事情，脑袋里有一个基本的理念吗？理念还是有的，整体上几个点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;知识、认识方面的&lt;strong&gt;分享&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;多人一起做事时的&lt;strong&gt;协作&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;所用基础工具的&lt;strong&gt;开源&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;团队、个人核心在于&lt;strong&gt;成长&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从上述理念出发，更能得到自己的共鸣，不要怕对外开放、交流之后，技术上被超越，即使被超越了，对整个社会的进步也是好的。&lt;em&gt;（如果公司对于某些技术有保密要求，这就绝不能对外分享，但是整体的框架，一些设计理念上的东西，也可以在交流中成长）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;能够随身携带的技能，如果可以预见的时间内会用到，那就尽多获取、提早准备，例如，驾驶、外语、急救措施、生活中安全措施。当然，其中最重要是：身体。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;视野放开，保持敏感&lt;/h2&gt;

&lt;p&gt;要将视野放开，保持与时代相接触，对于自己所处行业，需要有自己的认识，这个认识可以是错误的，分享出去，自然会有人忍不住来指正，关注国内和国外最新的科技动态，包括产品、生活、行业；主要途径：业内顶尖人才、顶尖公司动向，领域内主流媒体的动向。&lt;/p&gt;

&lt;p&gt;前沿媒体：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;www.36kr.com&quot;&gt;36Kr&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.infoq.com&quot;&gt;InfoQ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.valleytalk.org&quot;&gt;弯曲评论&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;个人博客、微信：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;coolshell.cn&quot;&gt;酷壳coolshell&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.williamlong.info&quot;&gt;月光博客&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;陈利人：&lt;a href=&quot;http://weibo.com/lirenchen&quot;&gt;微博&lt;/a&gt;、&lt;code&gt;微信号：daiziguizhongren&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;梁斌：&lt;a href=&quot;http://weibo.com/pennyliang&quot;&gt;微博&lt;/a&gt;、&lt;code&gt;微信号：pennyjob&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Fenng：&lt;a href=&quot;http://hutu.me/&quot;&gt;小道消息&lt;/a&gt;、&lt;a href=&quot;http://news.dbanotes.net/&quot;&gt;Startup News&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;新兴的潮流网站：对一些如同爆炸性出现的新技术、网站、观点，保持敏感度，进行简要学习其基本规律，了解轮廓。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;上面说了两点，其实就是一个意思：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;有个打算，有点追求：做事情，集中精力，一件一件解决掉；&lt;/li&gt;
  &lt;li&gt;视野放开，保持敏感：做事情，看趋势，3~5年、20年；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;我写blog，大都是分要点，列一个1、2、3，调理还算清晰，但，我总感觉我blog读起来味道怪怪的，相反，读Fenng、caoz、haoel的文章，能感觉到有平铺直叙、娓娓道来，很有味道。写blog，不仅要内容有用，而且，让人读起来也要舒服，这一点，我需要学习改进。&lt;/p&gt;

</content>
   </entry>
   
 
</feed>
