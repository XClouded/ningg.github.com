<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>NingG.github.com</title>
   <link href="http://ningg.github.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://ningg.github.com" rel="alternate" type="text/html" />
   <updated>2014-11-30T21:48:01+08:00</updated>
   <id>http://ningg.github.com</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>Ganglia 3.6.1：Ganglia Monitoring Daemon v3.6.1 Configuration</title>
     <link href="http://ningg.github.com/ganglia-gmond-conf"/>
     <updated>2014-11-22T00:00:00+08:00</updated>
     <id>http://ningg.github.com/ganglia-gmond-conf</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://sourceforge.net/projects/ganglia/files/&quot;&gt;ganglia-3.6.1(ganglia monitoring core)&lt;/a&gt;，其源码包中gmond/gmond.conf.html文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;name&quot;&gt;NAME&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;gmond.conf&lt;/strong&gt; - configuration file for ganglia monitoring daemon (&lt;strong&gt;gmond&lt;/strong&gt;)&lt;/p&gt;

&lt;h2 id=&quot;description&quot;&gt;DESCRIPTION&lt;/h2&gt;

&lt;p&gt;The gmond.conf file is used to configure the ganglia monitoring daemon (gmond) which is part of the** Ganglia Distributed Monitoring System**.&lt;/p&gt;

&lt;h2 id=&quot;sections-and-attributes&quot;&gt;SECTIONS AND ATTRIBUTES&lt;/h2&gt;

&lt;p&gt;All sections and attributes are case-insensitive. For example, &lt;strong&gt;name&lt;/strong&gt; or &lt;strong&gt;NAME&lt;/strong&gt; or &lt;strong&gt;Name&lt;/strong&gt; or &lt;strong&gt;NaMe&lt;/strong&gt; are all equivalent.&lt;/p&gt;

&lt;p&gt;Some sections can be included in the configuration file &lt;strong&gt;multiple&lt;/strong&gt; times and some sections are &lt;strong&gt;singular&lt;/strong&gt;. For example, you can have only one &lt;strong&gt;cluster&lt;/strong&gt; section to define the attributes of the cluster being monitored; however, you can have multiple &lt;strong&gt;udp_recv_channel&lt;/strong&gt; sections to allow gmond to receive message on multiple UDP channels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：gmond.conf配置文件，说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;整个配置文件，分为：多个section，每个section下有多个attribute&lt;/li&gt;
  &lt;li&gt;section和attribute是case-insensitive的&lt;/li&gt;
  &lt;li&gt;在整个配置文件中，有的section只能有一个，有的就可以有多个&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cluster&quot;&gt;cluster&lt;/h3&gt;

&lt;p&gt;There should &lt;strong&gt;only be one cluster section&lt;/strong&gt; defined. This section controls how gmond reports the attributes of the cluster that it is part of.&lt;/p&gt;

&lt;p&gt;The cluster section has four attributes: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;name&lt;/li&gt;
  &lt;li&gt;owner&lt;/li&gt;
  &lt;li&gt;latlong&lt;/li&gt;
  &lt;li&gt;url&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cluster {
    name = &quot;Millennium Cluster&quot;
    owner = &quot;UC Berkeley CS Dept.&quot;
    latlong = &quot;N37.37 W122.23&quot;
    url = &quot;http://www.millennium.berkeley.edu/&quot;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;name&lt;/strong&gt; attributes specifies the name of the cluster of machines. &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;owner&lt;/strong&gt; tag specifies the administrators of the cluster. 
    &lt;ul&gt;
      &lt;li&gt;The pair &lt;strong&gt;name/owner&lt;/strong&gt; should be unique to all clusters in the world.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;latlong&lt;/strong&gt; attribute is the latitude and longitude GPS coordinates of this cluster on earth. Specified to 1 mile accuracy with two decimal places per axis in decimal.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;url&lt;/strong&gt; for more information on the cluster. Intended to give purpose, owner, administration, and account details for this cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There directives directly control the XML output of gmond. For example, the cluster configuration example above would translate into the following XML.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;CLUSTER NAME=&quot;Millennium Cluster&quot; OWNER=&quot;UC Berkeley CS Dept.&quot;
	   LATLONG=&quot;N37.37 W122.23&quot; URL=&quot;http://www.millennium.berkeley.edu/&quot;&amp;gt;
...
&amp;lt;/CLUSTER&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;host&quot;&gt;host&lt;/h3&gt;

&lt;p&gt;The host section provides information about the host running this instance of gmond. Currently only the &lt;strong&gt;location&lt;/strong&gt; string attribute is supported. Example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host {
    location = &quot;1,2,3&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The numbers represent &lt;strong&gt;Rack&lt;/strong&gt;, &lt;strong&gt;Rank&lt;/strong&gt; and &lt;strong&gt;Plane&lt;/strong&gt; respectively.&lt;/p&gt;

&lt;h3 id=&quot;globals&quot;&gt;globals&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;globals&lt;/strong&gt; section controls general characteristics of &lt;strong&gt;gmond&lt;/strong&gt; such as whether is should daemonize, what user it should run as, whether is should send/receive date and such. The globals section has the following attributes: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;daemonize&lt;/li&gt;
  &lt;li&gt;setuid&lt;/li&gt;
  &lt;li&gt;user&lt;/li&gt;
  &lt;li&gt;debug_level&lt;/li&gt;
  &lt;li&gt;mute&lt;/li&gt;
  &lt;li&gt;deaf&lt;/li&gt;
  &lt;li&gt;allow_extra_data&lt;/li&gt;
  &lt;li&gt;host_dmax&lt;/li&gt;
  &lt;li&gt;host_tmax&lt;/li&gt;
  &lt;li&gt;cleanup_threshold&lt;/li&gt;
  &lt;li&gt;gexec&lt;/li&gt;
  &lt;li&gt;send_metadata_interval&lt;/li&gt;
  &lt;li&gt;module_dir&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;globals {
    daemonize = true
    setuid = true
    user = nobody
    host_dmax = 3600
    host_tmax = 40
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;daemonize&lt;/strong&gt; attribute is a &lt;strong&gt;boolean&lt;/strong&gt;. 
    &lt;ul&gt;
      &lt;li&gt;When &lt;strong&gt;true&lt;/strong&gt;, gmond will daemonize. &lt;/li&gt;
      &lt;li&gt;When &lt;strong&gt;false&lt;/strong&gt;, gmond will run in the foreground.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;setuid&lt;/strong&gt; attribute is a &lt;strong&gt;boolean&lt;/strong&gt;. 
    &lt;ul&gt;
      &lt;li&gt;When &lt;strong&gt;true&lt;/strong&gt;, gmond will set its effective UID to the uid of the user specified by the &lt;strong&gt;user&lt;/strong&gt; attribute. &lt;/li&gt;
      &lt;li&gt;When &lt;strong&gt;false&lt;/strong&gt;, gmond will not change its effective user.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;debug_level&lt;/strong&gt; is an &lt;strong&gt;integer&lt;/strong&gt; value. 
    &lt;ul&gt;
      &lt;li&gt;When set to &lt;strong&gt;zero&lt;/strong&gt; (0), gmond will run normally. &lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;debug_level&lt;/strong&gt; greater than zero will result in gmond running in the foreground and outputting debugging information. &lt;/li&gt;
      &lt;li&gt;The higher the &lt;strong&gt;debug_level&lt;/strong&gt; the more verbose the output.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;mute&lt;/strong&gt; attribute is a &lt;strong&gt;boolean&lt;/strong&gt;. 
    &lt;ul&gt;
      &lt;li&gt;When &lt;strong&gt;true&lt;/strong&gt;, gmond will &lt;strong&gt;not send&lt;/strong&gt; data regardless of any other configuration directives.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;deaf&lt;/strong&gt; attribute is a &lt;strong&gt;boolean&lt;/strong&gt;. 
    &lt;ul&gt;
      &lt;li&gt;When &lt;strong&gt;true&lt;/strong&gt;, gmond will &lt;strong&gt;not receive&lt;/strong&gt; data regardless of any other configuration directives.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;allow_extra_data&lt;/strong&gt; attribute is a &lt;strong&gt;boolean&lt;/strong&gt;. 
    &lt;ul&gt;
      &lt;li&gt;When &lt;strong&gt;false&lt;/strong&gt;, gmond will not send out the &lt;strong&gt;EXTRA_ELEMENT&lt;/strong&gt; and &lt;strong&gt;EXTRA_DATA&lt;/strong&gt; parts of the XML. &lt;/li&gt;
      &lt;li&gt;This might be useful if you are using your own frontend to the metric data and will like to save some bandwith.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;strong&gt;EXTRA_ELEMENT&lt;/strong&gt; and &lt;strong&gt;EXTRA_DATA&lt;/strong&gt;中存储了哪些信息？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;host_dmax&lt;/strong&gt; value is an &lt;strong&gt;integer&lt;/strong&gt; with units in seconds. 
    &lt;ul&gt;
      &lt;li&gt;When set to &lt;strong&gt;zero&lt;/strong&gt; (0), gmond will never delete a host from its list even when a remote host has stopped reporting. &lt;/li&gt;
      &lt;li&gt;If host_dmax is set to a positive number then gmond will flush a host after it has not heard from it for &lt;strong&gt;host_dmax&lt;/strong&gt; seconds. By the way, dmax means &lt;strong&gt;delete max&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：从list中删除某个host的有什么影响？gmond接收不到某个host的report信息，那留着他有什么用？如果有新增的host，gmond能够自动识别出来吗？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;host_tmax&lt;/strong&gt; value is an &lt;strong&gt;integer&lt;/strong&gt; with units in seconds. 
    &lt;ul&gt;
      &lt;li&gt;This value represents the maximum amount of time that gmond should wait between updates from a host. &lt;/li&gt;
      &lt;li&gt;As messages may get lost in the network, gmond will consider the host as being down if it has not received any messages from it after &lt;strong&gt;4 times&lt;/strong&gt; this value. &lt;/li&gt;
      &lt;li&gt;For example, if host_tmax is set to 20, the host will appear as down after 80 seconds with no messages from it. By the way, tmax means &lt;strong&gt;timeout max&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：此处的&lt;strong&gt;host_tmax&lt;/strong&gt;超过这一时间，认为host down，与上面的 delete host from list有什么区别？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;cleanup_threshold&lt;/strong&gt; is the minimum amount of time before gmond will cleanup any hosts or metrics where tn &amp;gt; dmax a.k.a. expired data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;gexec&lt;/strong&gt; boolean allows you to specify whether gmond will announce the hosts availability to run gexec jobs. &lt;strong&gt;Note&lt;/strong&gt;: this requires that &lt;strong&gt;gexecd&lt;/strong&gt; is running on the host and the proper keys have been installed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;send_metadata_interval&lt;/strong&gt; establishes an interval in which gmond will send or resend the metadata packets that describe each enabled metric. &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;This directive by default is set to &lt;strong&gt;0&lt;/strong&gt; which means that gmond will only send the metadata packets at startup and upon request from other gmond nodes running remotely. &lt;/li&gt;
      &lt;li&gt;If a new machine running gmond is added to a cluster, it needs to announce itself and inform all other nodes of the metrics that it currently supports. &lt;/li&gt;
      &lt;li&gt;In &lt;strong&gt;multicast&lt;/strong&gt; mode, this isn’t a problem because any node can request the metadata of all other nodes in the cluster. &lt;/li&gt;
      &lt;li&gt;However in &lt;strong&gt;unicast&lt;/strong&gt; mode, a resend interval must be established. The interval value is the minimum number of seconds between resends.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：为什么&lt;strong&gt;multicast&lt;/strong&gt;和&lt;strong&gt;unicast&lt;/strong&gt;（组播与单播）方式有差异？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;override_hostname&lt;/strong&gt; and &lt;strong&gt;override_ip&lt;/strong&gt; parameters allow an arbitrary hostname and/or IP (hostname can be optionally specified without IP) to use when identifying metrics coming from this host.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;module_dir&lt;/strong&gt; is an &lt;strong&gt;optional&lt;/strong&gt; parameter indicating the directory where the &lt;strong&gt;DSO&lt;/strong&gt; modules are to be located. &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;If &lt;strong&gt;absent&lt;/strong&gt;, the value to use is set at configure time with the &lt;strong&gt;–with-moduledir&lt;/strong&gt; option which will default if omitted to the a subdirectory named “ganglia” in the directory where libganglia will be installed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, in a 32-bit Intel compatible Linux host that is usually:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/lib/ganglia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;strong&gt;DSO&lt;/strong&gt; modules是什么？&lt;/p&gt;

&lt;h3 id=&quot;udpsendchannel&quot;&gt;udp_send_channel&lt;/h3&gt;

&lt;p&gt;You can define as many &lt;strong&gt;udp_send_channel&lt;/strong&gt; sections as you like within the limitations of memory and file descriptors. If &lt;strong&gt;gmond&lt;/strong&gt; is configured as &lt;strong&gt;mute&lt;/strong&gt; this section will be ignored.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;udp_send_channel&lt;/code&gt; section，可以设置多个；&lt;/li&gt;
  &lt;li&gt;如果&lt;code&gt;mute&lt;/code&gt;属性设置为true，则，&lt;code&gt;udp_send_channel&lt;/code&gt;配置信息不会启用；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;udp_send_channel&lt;/strong&gt; has a total of seven attributes: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mcast_join&lt;/li&gt;
  &lt;li&gt;mcast_if&lt;/li&gt;
  &lt;li&gt;host&lt;/li&gt;
  &lt;li&gt;port&lt;/li&gt;
  &lt;li&gt;ttl&lt;/li&gt;
  &lt;li&gt;bind&lt;/li&gt;
  &lt;li&gt;bind_hostname&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;bind&lt;/strong&gt; and &lt;strong&gt;bind_hostname&lt;/strong&gt; are mutually exclusive.（两个属性&lt;code&gt;bind&lt;/code&gt;和&lt;code&gt;bind_hostname&lt;/code&gt;互斥，只需配置一个）&lt;/p&gt;

&lt;p&gt;For example, the 2.5.x version gmond would send on the following single channel by default…&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;udp_send_channel {
    mcast_join = 239.2.11.71
    port       = 8649
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;mcast_join&lt;/strong&gt; and &lt;strong&gt;mcast_if&lt;/strong&gt; attributes are &lt;strong&gt;optional&lt;/strong&gt;. When specified &lt;strong&gt;gmond&lt;/strong&gt; will create the UDP socket and join the &lt;strong&gt;mcast_join&lt;/strong&gt; multicast group and send data out the interface specified by &lt;strong&gt;mcast_if&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：几个疑问：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mcast_join用来标识一个multicast group，必须是IP吗？这个IP的用途是什么？IP必须是cluster中某个node吗？&lt;/li&gt;
  &lt;li&gt;设置了mcast_join之后，还能设置host吗？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can use the &lt;strong&gt;bind&lt;/strong&gt; attribute to bind to a particular local address to be used as the source for the multicast packets sent or let gmond resolve the default hostname if &lt;code&gt;bind_hostname = yes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If only a &lt;strong&gt;host&lt;/strong&gt; and &lt;strong&gt;port&lt;/strong&gt; are specified then gmond will send &lt;strong&gt;unicast&lt;/strong&gt; UDP messages to the hosts specified.&lt;/p&gt;

&lt;p&gt;You could specify &lt;strong&gt;multiple&lt;/strong&gt; &lt;strong&gt;unicast&lt;/strong&gt; hosts for redundancy as &lt;strong&gt;gmond&lt;/strong&gt; will send UDP messages to &lt;strong&gt;all&lt;/strong&gt; UDP channels.&lt;/p&gt;

&lt;p&gt;Be careful though not to mix &lt;strong&gt;multicast&lt;/strong&gt; and &lt;strong&gt;unicast&lt;/strong&gt; attributes in the same &lt;strong&gt;udp_send_channel&lt;/strong&gt; definition.&lt;/p&gt;

&lt;p&gt;For example…&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;udp_send_channel {
    host = host.foo.com
    port = 2389
}
  
udp_send_channel {
    host = 192.168.3.4
    port = 2344
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;would configure gmond to send messages to two hosts. The &lt;strong&gt;host&lt;/strong&gt; specification can be an IPv4/IPv6 address or a resolvable hostname.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;ttl&lt;/strong&gt; attribute lets you modify the Time-To-Live (TTL) of outgoing messages (unicast or multicast).&lt;/p&gt;

&lt;h3 id=&quot;udprecvchannel&quot;&gt;udp_recv_channel&lt;/h3&gt;

&lt;p&gt;You can specify as many udp_recv_channel sections as you like within the limits of memory and file descriptors. If gmond is configured deaf this attribute will be ignored.&lt;/p&gt;

&lt;p&gt;The udp_recv_channel section has following attributes: mcast_join, bind, port, mcast_if, family, retry_bind and buffer. The udp_recv_channel can also have an acl definition (see ACCESS CONTROL LISTS below).&lt;/p&gt;

&lt;p&gt;For example, the 2.5.x gmond ran with a single udp receive channel…&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
    mcast_join = 239.2.11.71
    bind       = 239.2.11.71
    port       = 8649
  }
The mcast_join and mcast_if should only be used if you want to have this UDP channel receive multicast packets the multicast group mcast_join on interface mcast_if. If you do not specify multicast attributes then gmond will simply create a UDP server on the specified port.&lt;/p&gt;

&lt;p&gt;You can use the bind attribute to bind to a particular local address.&lt;/p&gt;

&lt;p&gt;The family address is set to inet4 by default. If you want to bind the port to an inet6 port, you need to specify that in the family attribute. Ganglia will not allow IPV6=&amp;gt;IPV4 mapping (for portability and security reasons). If you want to listen on both inet4 and inet6 for a particular port, explicitly state it with the following:&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
    port = 8666
    family = inet4
  }
  udp_recv_channel {
    port = 8666
    family = inet6
  }
If you specify a bind address, the family of that address takes precedence. f your IPv6 stack doesn’t support IPV6_V6ONLY, a warning will be issued but gmond will continue working (this should rarely happen).&lt;/p&gt;

&lt;p&gt;Multicast Note: for multicast, specifying a bind address with the same value used for mcast_join will prevent unicast UDP messages to the same port from being processed.&lt;/p&gt;

&lt;p&gt;The sFlow protocol (see http://www.sflow.org) can be used to collect a standard set of performance metrics from servers. For servers that don’t include embedded sFlow agents, an open source sFlow agent is available on SourceForge (see http://host-sflow.sourceforge.net).&lt;/p&gt;

&lt;p&gt;To configure gmond to receive sFlow datagrams, simply add a udp_recv_channel with the port set to 6343 (the IANA registered port for sFlow):&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
    port = 6343
  }
Note: sFlow is unicast protocol, so don’t include mcast_join join. Note: To use some other port for sFlow, set it here and then specify the port in an sflow section (see below).&lt;/p&gt;

&lt;p&gt;gmond will fail to run if it can’t bind to all defined udp_recv_channels. Sometimes, on machines configured by DHCP, for example, the gmond daemon starts before a network address is assigned to the interface. Consequently, the bind fails and the gmond daemon does not run. To assist in this situation, the boolean parameter retry_bind can be set to the value true and then the daemon will not abort on failure, it will enter a loop and repeat the bind attempt every 60 seconds:&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
    port = 6343
    retry_bind = true
  }
If you have a large system with lots of metrics, you might experience UDP drops. This happens when gmond is not able to process the UDP fast enough from the network. In this case you might consider changing your setup into a more distributed setup using aggregator gmond hosts. Alternatively you can choose to create a bigger receive buffer:&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
    port = 6343
    buffer = 10485760
  }
B&lt;buffer&gt; is specified in bytes, i.e.: 10485760 will allow 10MB UDP 
to be buffered in memory.
Note: increasing buffer size will increase memory usage by gmond&lt;/buffer&gt;&lt;/p&gt;

&lt;h3 id=&quot;tcpacceptchannel&quot;&gt;tcp_accept_channel&lt;/h3&gt;

&lt;p&gt;You can specify as many tcp_accept_channel sections as you like within the limitations of memory and file descriptors. If gmond is configured to be mute, then these sections are ignored.&lt;/p&gt;

&lt;p&gt;The tcp_accept_channel has the following attributes: bind, port, interface, family and timeout. A tcp_accept_channel may also have an acl section specified (see ACCESS CONTROL LISTS below).&lt;/p&gt;

&lt;p&gt;For example, 2.5.x gmond would accept connections on a single TCP channel.&lt;/p&gt;

&lt;p&gt;tcp_accept_channel {
    port = 8649
  }
The bind address is optional and allows you to specify which local address gmond will bind to for this channel.&lt;/p&gt;

&lt;p&gt;The port is an integer than specifies which port to answer requests for data.&lt;/p&gt;

&lt;p&gt;The family address is set to inet4 by default. If you want to bind the port to an inet6 port, you need to specify that in the family attribute. Ganglia will not allow IPV6=&amp;gt;IPV4 mapping (for portability and security reasons). If you want to listen on both inet4 and inet6 for a particular port, explicitly state it with the following:&lt;/p&gt;

&lt;p&gt;tcp_accept_channel {
    port = 8666
    family = inet4
  }
  tcp_accept_channel {
    port = 8666
    family = inet6
  }
If you specify a bind address, the family of that address takes precedence. If your IPv6 stack doesn’t support IPV6_V6ONLY, a warning will be issued but gmond will continue working (this should rarely happen).&lt;/p&gt;

&lt;p&gt;The timeout attribute allows you to specify how many microseconds to block before closing a connection to a client. The default is set to -1 (blocking IO) and will never abort a connection regardless of how slow the client is in fetching the report data.&lt;/p&gt;

&lt;p&gt;The interface is not implemented at this time (use bind).&lt;/p&gt;

&lt;h3 id=&quot;collectiongroup&quot;&gt;collection_group&lt;/h3&gt;

&lt;p&gt;You can specify as many collection_group section as you like within the limitations of memory. A collection_group has the following attributes: collect_once, collect_every and time_threshold. A collection_group must also contain one or more metric sections.&lt;/p&gt;

&lt;p&gt;The metric section has the following attributes: (one of name or name_match; name_match is only permitted if pcre support is compiled in), value_threshold and title. For a list of available metric names, run the following command:&lt;/p&gt;

&lt;p&gt;% gmond -m
Here is an example of a collection group for a static metric…&lt;/p&gt;

&lt;p&gt;collection_group {
    collect_once   = yes
    time_threshold = 1800
    metric {
     name = “cpu_num”
     title = “Number of CPUs”
    }
  }
This collection_group entry would cause gmond to collect the cpu_num metric once at startup (since the number of CPUs will not change between reboots). The metric cpu_num would be send every 1/2 hour (1800 seconds). The default value for the time_threshold is 3600 seconds if no time_threshold is specified.&lt;/p&gt;

&lt;p&gt;The time_threshold is the maximum amount of time that can pass before gmond sends all metrics specified in the collection_group to all configured udp_send_channels. A metric may be sent before this time_threshold is met if during collection the value surpasses the value_threshold (explained below).&lt;/p&gt;

&lt;p&gt;Here is an example of a collection group for a volatile metric…&lt;/p&gt;

&lt;p&gt;collection_group {
    collect_every = 60
    time_threshold = 300
    metric {
      name = “cpu_user”
      value_threshold = 5.0
      title = “CPU User”
    }
    metric {
      name = “cpu_idle”
      value_threshold = 10.0
      title = “CPU Idle”
    }
  }
This collection group would collect the cpu_user and cpu_idle metrics every 60 seconds (specified in collect_every). If cpu_user varies by 5.0% or cpu_idle varies by 10.0%, then the entire collection_group is sent. If no value_threshold is triggered within time_threshold seconds (in this case 300), the entire collection_group is sent.&lt;/p&gt;

&lt;p&gt;Each time the metric value is collected the new value is compared with the old value collected. If the difference between the last value and the current value is greater than the value_threshold, the entire collection group is send to the udp_send_channels defined.&lt;/p&gt;

&lt;p&gt;It’s important to note that all metrics in a collection group are sent even when only a single value_threshold is surpassed.&lt;/p&gt;

&lt;p&gt;In addition a user friendly title can be substituted for the metric name by including a title within the metric section.&lt;/p&gt;

&lt;p&gt;By using the name_match parameter instead of name, it is possible to use a single definition to configure multiple metrics that match a regular expression. The perl compatible regular expression (pcre) syntax is used. This approach is particularly useful for a series of metrics that may vary in number between reboots (e.g. metric names that are generated for each individual NIC or CPU core).&lt;/p&gt;

&lt;p&gt;Here is an example of using the name_match directive to enable the multicpu metrics:&lt;/p&gt;

&lt;p&gt;metric {
    name_match = “multicpu_([a-z]+)([0-9]+)”
    value_threshold = 1.0
    title = “CPU-\2 \1”
  }
Note that in the example above, there are two matches: the alphabetical match matches the variations of the metric name (e.g. idle, system) while the numeric match matches the CPU core number. The second thing to note is the use of substitutions within the argument to title.&lt;/p&gt;

&lt;p&gt;If both name and name_match are specified, then name is ignored.&lt;/p&gt;

&lt;h3 id=&quot;modules&quot;&gt;Modules&lt;/h3&gt;

&lt;p&gt;A modules section contains the parameters that are necessary to load a metric module. A metric module is a dynamically loadable module that extends the available metrics that gmond is able to collect. Each modules section contains at least one module section. Within a module section are the directives name, language, enabled, path and params. The module name is the name of the module as determined by the module structure if the module was developed in C/C++. Alternatively, the name can be the name of the source file if the module has been implemented in a interpreted language such as python. A language designation must be specified as a string value for each module. The language directive must correspond to the source code language in which the module was implemented (ex. language = “python”). If a language directive does not exist for the module, the assumed language will be “C/C++”. The enabled directive allows a metric module to be easily enabled or disabled through the configuration file. If the enabled directive is not included in the module configuration, the enabled state will default to “yes”. One thing to note is that if a module has been disabled yet the metric which that module implements is still listed as part of a collection group, gmond will produce a warning message. However gmond will continue to function normally by simply ignoring the metric. The path is the path from which gmond is expected to load the module (C/C++ compiled dynamically loadable module only). The params directive can be used to pass a single string parameter directly to the module initialization function (C/C++ module only). Multiple parameters can be passed to the module’s initialization function by including one or more param sections. Each param section must be named and contain a value directive. Once a module has been loaded, the additional metrics can be discovered by invoking gmond -m.&lt;/p&gt;

&lt;p&gt;modules {
     module {
       name = “example_module”
       language = “C/C++”
       enabled = yes
       path = “modexample.so”
       params = “An extra raw parameter”
       param RandomMax {
         value = 75
       }
       param ConstantValue {
         value = 25
       }
     }
   }&lt;/p&gt;

&lt;h3 id=&quot;sflow&quot;&gt;sFlow&lt;/h3&gt;

&lt;p&gt;The sflow group is optional and has the following optional attributes: udp_port, accept_vm_metrics, accept_http_metrics, accept_memcache_metrics, accept_jvm_metrics, multiple_http_instances,multiple_memcache_instances, multiple_jvm_instances. By default, a udp_recv_channel on port 6343 (the IANA registered port for sFlow) is all that is required to accept and process sFlow datagrams. To receive sFlow on some other port requires both a udp_recv_channel for the other port and a udp_port setting here. For example:&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
     port = 7343
   }
   sflow {
     udp_port = 7343
   }
An sFlow agent running on a hypervisor may also be sending metrics for its local virtual machines. By default these metrics are ignored, but the accept_vm_metrics flag can be used to accept those metrics too, and prefix them with an identifier for each virtual machine.&lt;/p&gt;

&lt;p&gt;sflow {
     accept_vm_metrics = yes
   }
The sFlow feed may also contain metrics sent from HTTP or memcached servers, or from Java VMs. Extra options can be used to ignore or accept these metrics, and to indicate that there may be multiple instances per host. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sflow {
  accept_http_metrics = yes
  multiple_http_instances = yes
} will allow the HTTP metrics, and also mark them with a distinguishing identifier so that each instance can be trended separately. (If multiple instances are reporting and this flag is not set, the results are likely to be garbled.)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;include&quot;&gt;Include&lt;/h3&gt;

&lt;p&gt;This directive allows the user to include additional configuration files rather than having to add all gmond configuration directives to the gmond.conf file. The following example includes any file with the extension of .conf contained in the directory conf.d as if the contents of the included configuration files were part of the original gmond.conf file. This allows the user to modularize their configuration file. One usage example might be to load individual metric modules by including module specific .conf files.&lt;/p&gt;

&lt;p&gt;include (‘/etc/ganglia/conf.d/*.conf’)&lt;/p&gt;

&lt;h2 id=&quot;access-control&quot;&gt;ACCESS CONTROL&lt;/h2&gt;

&lt;p&gt;The udp_recv_channel and tcp_accept_channel directives can contain an Access Control List (ACL). This ACL allows you to specify exactly which hosts gmond process data from.&lt;/p&gt;

&lt;p&gt;An example of an acl entry looks like&lt;/p&gt;

&lt;p&gt;acl {
    default = “deny”
    access {
      ip = 192.168.0.4
      mask = 32
      action = “allow”
    }
  }
This ACL will by default reject all traffic that is not specifically from host 192.168.0.4 (the mask size for an IPv4 address is 32, the mask size for an IPv6 address is 128 to represent a single host).&lt;/p&gt;

&lt;p&gt;Here is another example&lt;/p&gt;

&lt;p&gt;acl {
    default = “allow”
    access {
      ip = 192.168.0.0
      mask = 24
      action = “deny”
    }
    access {
      ip = ::ff:1.2.3.0
      mask = 120
      action = “deny”
    }
  }
This ACL will by default allow all traffic unless it comes from the two subnets specified with action = “deny”.&lt;/p&gt;

&lt;h2 id=&quot;example&quot;&gt;EXAMPLE&lt;/h2&gt;

&lt;p&gt;The default behavior for a 2.5.x gmond would be specified as…&lt;/p&gt;

&lt;p&gt;udp_recv_channel {
    mcast_join = 239.2.11.71
    bind       = 239.2.11.71
    port       = 8649
  }
  udp_send_channel {
    mcast_join = 239.2.11.71
    port       = 8649
  }
  tcp_accept_channel {
    port       = 8649
  }
To see the complete default configuration for gmond simply run:&lt;/p&gt;

&lt;p&gt;% gmond -t
gmond will print out its default behavior in a configuration file and then exit. Capturing this output to a file can serve as a useful starting point for creating your own custom configuration.&lt;/p&gt;

&lt;p&gt;% gmond -t &amp;gt; custom.conf
edit custom.conf to taste and then&lt;/p&gt;

&lt;p&gt;% gmond -c ./custom.conf&lt;/p&gt;

&lt;h2 id=&quot;see-also&quot;&gt;SEE ALSO&lt;/h2&gt;

&lt;p&gt;gmond(1).&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;NOTES&lt;/h2&gt;

&lt;p&gt;The ganglia web site is at http://ganglia.info/.&lt;/p&gt;

&lt;h2 id=&quot;copyright&quot;&gt;COPYRIGHT&lt;/h2&gt;

&lt;p&gt;Copyright (c) 2005 The University of California, Berkeley&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

</content>
   </entry>
   
   <entry>
     <title>Ganglia 3.6.1：User Guide</title>
     <link href="http://ningg.github.com/ganglia-manual"/>
     <updated>2014-11-21T00:00:00+08:00</updated>
     <id>http://ningg.github.com/ganglia-manual</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://sourceforge.net/projects/ganglia/files/&quot;&gt;ganglia-3.6.1(ganglia monitoring core)&lt;/a&gt;，其源码包中ganglia.html文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;name&quot;&gt;Name&lt;/h2&gt;

&lt;p&gt;ganglia - distributed monitoring system&lt;/p&gt;

&lt;h2 id=&quot;version&quot;&gt;Version&lt;/h2&gt;

&lt;p&gt;ganglia 3.6.1&lt;/p&gt;

&lt;p&gt;The latest version of this software and document will always be found at http://ganglia.sourceforge.net/.&lt;/p&gt;

&lt;h2 id=&quot;synopsis&quot;&gt;Synopsis&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;	 ______                  ___
	/ ____/___ _____  ____ _/ (_)___ _
   / / __/ __ `/ __ \/ __ `/ / / __ `/
  / /_/ / /_/ / / / / /_/ / / / /_/ /
  \____/\__,_/_/ /_/\__, /_/_/\__,_/
				   /____/ Distributed Monitoring System 

`__
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. It is based on a hierarchical design targeted at federations of clusters. It relies on a multicast-based listen/announce protocol to monitor state within clusters and uses a tree of point-to-point connections amongst representative cluster nodes to federate clusters and aggregate their state. It leverages widely used technologies such as XML for data representation, XDR for compact, portable data transport, and RRDtool for data storage and visualization. It uses carefully engineered data structures and algorithms to achieve very low per-node overheads and high concurrency. The implementation is robust, has been ported to an extensive set of operating systems and processor architectures, and is currently in use on over 500 clusters around the world. It has been used to link clusters across university campuses and around the world and can scale to handle clusters with 2000 nodes.&lt;/p&gt;

&lt;p&gt;The ganglia system is comprised of two unique daemons, a PHP-based web frontend and a few other small utility programs.&lt;/p&gt;

&lt;h3 id=&quot;ganglia-monitoring-daemon-gmond&quot;&gt;Ganglia Monitoring Daemon (gmond)&lt;/h3&gt;

&lt;p&gt;Gmond is a multi-threaded daemon which runs on each cluster node you want to monitor. Installation is easy. You don’t have to have a common NFS filesystem or a database backend, install special accounts, maintain configuration files or other annoying hassles.&lt;/p&gt;

&lt;p&gt;Gmond has four main responsibilities: monitor changes in host state, announce relevant changes, listen to the state of all other ganglia nodes via a unicast or multicast channel and answer requests for an XML description of the cluster state.&lt;/p&gt;

&lt;p&gt;Each gmond transmits in information in two different ways: unicasting/multicasting host state in external data representation (XDR) format using UDP messages or sending XML over a TCP connection.&lt;/p&gt;

&lt;h3 id=&quot;ganglia-meta-daemon-gmetad&quot;&gt;Ganglia Meta Daemon (gmetad)&lt;/h3&gt;

&lt;p&gt;Federation in Ganglia is achieved using a tree of point-to-point connections amongst representative cluster nodes to aggregate the state of multiple clusters. At each node in the tree, a Ganglia Meta Daemon (gmetad) periodically polls a collection of child data sources, parses the collected XML, saves all numeric, volatile metrics to round-robin databases and exports the aggregated XML over a TCP sockets to clients. Data sources may be either gmond daemons, representing specific clusters, or other gmetad daemons, representing sets of clusters. Data sources use source IP addresses for access control and can be specified using multiple IP addresses for failover. The latter capability is natural for aggregating data from clusters since each gmond daemon contains the entire state of its cluster.&lt;/p&gt;

&lt;h3 id=&quot;ganglia-php-web-frontend&quot;&gt;Ganglia PHP Web Frontend&lt;/h3&gt;

&lt;p&gt;The Ganglia web frontend provides a view of the gathered information via real-time dynamic web pages. Most importantly, it displays Ganglia data in a meaningful way for system administrators and computer users. Although the web frontend to ganglia started as a simple HTML view of the XML tree, it has evolved into a system that keeps a colorful history of all collected data.&lt;/p&gt;

&lt;p&gt;The Ganglia web frontend caters to system administrators and users. For example, one can view the CPU utilization over the past hour, day, week, month, or year. The web frontend shows similar graphs for Memory usage, disk usage, network statistics, number of running processes, and all other Ganglia metrics.&lt;/p&gt;

&lt;p&gt;The web frontend depends on the existence of the gmetad which provides it with data from several Ganglia sources. Specifically, the web frontend will open the local port 8651 (by default) and expects to receive a Ganglia XML tree. The web pages themselves are highly dynamic; any change to the Ganglia data appears immediately on the site. This behavior leads to a very responsive site, but requires that the full XML tree be parsed on every page access. Therefore, the Ganglia web frontend should run on a fairly powerful, dedicated machine if it presents a large amount of data.&lt;/p&gt;

&lt;p&gt;The Ganglia web frontend is written in the PHP scripting language, and uses graphs generated by gmetad to display history information. It has been tested on many flavours of Unix (primarily Linux) with the Apache webserver and the PHP module (5.0.0 or later). The GD graphics library for PHP is used to generate pie charts in the frontend and needs to be installed separately. On RPM-based system, it is usually provided by the php-gd package.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;The latest version of all ganglia software can always be downloaded from http://ganglia.info/&lt;/p&gt;

&lt;p&gt;Ganglia runs on Linux (i386, x86_64, ia64, sparc, alpha, powerpc, m68k, mips, arm, hppa, s390), FreeBSD, NetBSD, OpenBSD, DragonflyBSD, MacOS X, Solaris, AIX, IRIX, Tru64, HPUX and Windows NT/XP/2000/2003/2008 making it as portable as it is scalable.&lt;/p&gt;

&lt;h3 id=&quot;monitoring-core-installation&quot;&gt;Monitoring Core Installation&lt;/h3&gt;

&lt;p&gt;If you use the Linux RPMs provided on the ganglia web site, you can skip to the end of this section.&lt;/p&gt;

&lt;p&gt;Ganglia uses the GNU autoconf so compilation and installation of the monitoring core is basically&lt;/p&gt;

&lt;p&gt;% ./configure
  % make
  % make install
but there are some issues that you need to take a look at first.&lt;/p&gt;

&lt;p&gt;Kernel multicast support
If you use the ganglia multicast support, you must have a kernel that supports multicast. The vast majority of machines have multicast support by default. If you have problems with ganglia this is a core issue.&lt;/p&gt;

&lt;p&gt;Gmetad is not installed by default
Since gmetad relies on the Round-Robin Database Tool ( see http://www.rrdtool.org/ ) it will not be compiled unless you explicit request it by using a –with-gmetad flag.&lt;/p&gt;

&lt;p&gt;% ./configure –with-gmetad
The configure script will fail if it cannot find the rrdtool library and header files. By default, it expects to find them at /usr/include/rrd.h and /usr/lib/librrd.so. If you installed them in different locations then you need to instruct configure where to find them using:&lt;/p&gt;

&lt;p&gt;% ./configure –with-librrd=/rrd/path –with-gmetad
Of course, you need to substitute /rrd/path with the real location of the rrd tool directory where the header file can be located inside an include subdirectory and the library can be located inside a lib subdirectory. As an alternative you could set “-L” in LDFLAGS, and “-I” in CFLAGS and CPPFLAGS for the library path and the header path respectively.&lt;/p&gt;

&lt;p&gt;AIX should not be compiled with shared libraries
You must add the –disable-shared configure flags if you are running on AIX. For more details refer to the README.AIX file&lt;/p&gt;

&lt;p&gt;% ./configure –disable-shared
Solaris dependencies could be problematic
Not really a Solaris specific problem, but since Solaris has several different package repositories, all of them unofficial, it is difficult to be sure that all possible permutations have been confirmed to work reliably.&lt;/p&gt;

&lt;p&gt;Be sure to have all dependencies covered, as explained in the INSTALL file and to use GNU make and a gcc compiler that builds 32bit binaries with all other libraries matching that ISA.&lt;/p&gt;

&lt;p&gt;When in doubt, build the problematic dependency from source and remember to distribute it together with your ganglia build as everything is dynamically linked by default.&lt;/p&gt;

&lt;p&gt;Be particularly careful with libConfuse, especially if using the old 2.5 version. LibConfuse 2.5 is known to be incorrectly packaged and to compile by default as a static library which will fail to link with ganglia.&lt;/p&gt;

&lt;p&gt;Propietary *NIX systems might not work at all
The good news is that the libmetrics code that used to work before 3.1 is still most likely working fine and so there is nothing fundamentally broken about it.&lt;/p&gt;

&lt;p&gt;But the bad news is that in order to add the dynamic metric functionality, the build system and the way gmond used to locate its metrics had to be changed significantly. Therefore getting gmond to build and work again required fixes to be implemented for all platforms.&lt;/p&gt;

&lt;p&gt;Since none of the developers had access to HPUX, IRIX, Tru64 (OSF/1), or Darwin (MacOS X) those platforms might not be able to build or run a 3.1 gmond yet. If you have access to any of these platforms and want to run ganglia 3.1, feel free to drop by the ganglia-developers list with suggestions, or even better patches.&lt;/p&gt;

&lt;p&gt;GEXEC confusion
GEXEC is a scalable cluster remote execution system which provides fast, RSA authenticated remote execution of parallel and distributed jobs. It provides transparent forwarding of stdin, stdout, stderr, and signals to and from remote processes, provides local environment propagation, and is designed to be robust and to scale to systems over 1000 nodes. Internally, GEXEC operates by building an n-ary tree of TCP sockets and threads between gexec daemons and propagating control information up and down the tree. By using hierarchical control, GEXEC distributes both the work and resource usage associated with massive amounts of parallelism across multiple nodes, thereby eliminating problems associated with single node resource limits (e.g., limits on the number of file descriptors on front-end nodes). (from http://www.theether.org/gexec )&lt;/p&gt;

&lt;p&gt;gexec is a great cluster execution tool but integrating it with ganglia is a bit clumsy. GEXEC can run standalone without access to a ganglia gmond. In standalone mode gexec will use the hosts listed in your GEXEC_SVRS variable to run on. For example, say I want to run hostname on three machines in my cluster: host1, host2 and host3. I use the following command line.&lt;/p&gt;

&lt;p&gt;% GEXEC_SVRS=”host1 host2 host3” gexec -n 3 hostname
and gexec would build an n-ary tree (binary tree by default) of TCP sockets to those machines and run the command hostname&lt;/p&gt;

&lt;p&gt;As an added feature, you can have gexec pull a host list from a locally running gmond and use that as the host list instead of GEXEC_SVRS. The list is load balanced and gexec will start the job on the n least-loaded machines.&lt;/p&gt;

&lt;p&gt;For example..&lt;/p&gt;

&lt;p&gt;% gexec -n 5 hostname
will run the command hostname on the five least-loaded machines in a cluster.&lt;/p&gt;

&lt;p&gt;To turn on the gexec feature in ganglia you must configure ganglia with the –enable-gexec flag&lt;/p&gt;

&lt;p&gt;% ./configure –enable-gexec
Enabling gexec means that by default any host running gmond will send a special message announcing that gexec is installed on it and open for requests.&lt;/p&gt;

&lt;p&gt;Now the question is, what if I don’t want gexec to run on every host in my cluster? For example, you may not want to have gexec run jobs on your cluster frontend nodes.&lt;/p&gt;

&lt;p&gt;You simply add the following line to your gmond configuration file (/etc/ganglia/gmond.conf by default)&lt;/p&gt;

&lt;p&gt;no_gexec on
Simple huh? I know the configuration file option, no_gexec, seems crazy (and it is). Why have an option that says “yes to no gexec”? The early versions of gmond didn’t use a configuration file but instead commandline options. One of the commandline options was simply –no-gexec and the default was to announce gexec as on.&lt;/p&gt;

&lt;p&gt;Once you have successfully run&lt;/p&gt;

&lt;p&gt;% ./configure &lt;options&gt;
  % make
  % make install
you should find the following files installed in /usr (by default).&lt;/options&gt;&lt;/p&gt;

&lt;p&gt;/usr/bin/gstat
  /usr/bin/gmetric
  /usr/sbin/gmond
  /usr/sbin/gmetad
If you installed ganglia using RPMs then these files will be installed when you install the RPM. The RPM is installed simply by running&lt;/p&gt;

&lt;p&gt;% rpm -Uvh ganglia-gmond-3.6.1.i386.rpm
  % rpm -Uvh ganglia-gmetad-3.6.1.i386.rpm
Once you have the necessary binaries installed, you can test your installation by running&lt;/p&gt;

&lt;p&gt;% ./gmond
This will start the ganglia monitoring daemon. You should then be able to run&lt;/p&gt;

&lt;p&gt;% telnet localhost 8649
And get an XML description of the state of your machine (and any other hosts running gmond at the time).&lt;/p&gt;

&lt;p&gt;If you are installing by source on Linux, scripts are provided to start gmetad and gmond at system startup. They are easy to install from the source root.&lt;/p&gt;

&lt;p&gt;% cp ./gmond/gmond.init /etc/rc.d/init.d/gmond
   % chkconfig –add gmond
   % chkconfig –list gmond
     gmond              0:off   1:off   2:on    3:on    4:on    5:on    6:off
   % /etc/rc.d/init.d/gmond start
     Starting GANGLIA gmond:                                    [  OK  ]
Repeat this step with gmetad.&lt;/p&gt;

&lt;h3 id=&quot;php-web-frontend-installation&quot;&gt;PHP Web Frontend Installation&lt;/h3&gt;

&lt;p&gt;The ./web directory of the ganglia distribution contains all the necessary PHP files for running your web frontend. Copy those files to /var/www/html, however look for the variable DocumentRoot in your Apache configuration files to be sure. All the PHP script files use relative URLs in their links, so you may place the ganglia/ directory anywhere convenient.&lt;/p&gt;

&lt;p&gt;Ensure your webserver understands how to process PHP script files. Currently, the web frontend contains certain php language that requires PHP version 5 or greater. Processing PHP script files usually requires a webserver module, such as the mod_php for the popular Apache webserver. In RedHat Linux, the RPM package that provides this module is called simply “php”.&lt;/p&gt;

&lt;p&gt;For Apache, mod_php module must be enabled. The following lines should appear somewhere in Apache’s *conf files. This example applies to Red Hat Linux (and clones). The actual filenames may vary on your system. If you installed the php module using an RPM package, this work will have been done automatically.&lt;/p&gt;

&lt;p&gt;LoadModule php5_module modules/libphp5.so
  AddHandler php5-script .php
  AddType text/html .php
The webfrontend requires the existance of the gmetad package on the webserver. Follow the installation instructions on the gmetad page. Specifically, the webfrontend requires the rrdtool and the rrds/ directory from gmetad. If you are a power user, you may use NFS to simulate the local existance of the rrds.&lt;/p&gt;

&lt;p&gt;Test your installation. Visit the URL:&lt;/p&gt;

&lt;p&gt;http://localhost/ganglia/
With a web-browser, where localhost is the address of your webserver.&lt;/p&gt;

&lt;p&gt;Installation of the web frontend is simplified on Linux by using rpm.&lt;/p&gt;

&lt;p&gt;% rpm -Uvh ganglia-web-3.6.1-1.noarch.rpm
  Preparing…                ########################################### [100%]
     1:ganglia-web            ########################################### [100%]&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;h3 id=&quot;gmond-configuration&quot;&gt;Gmond Configuration&lt;/h3&gt;

&lt;p&gt;The configuration file format has changed between gmond version 2.5.x and version 3.x. The change was necessary in order to allow more complex configuration options.&lt;/p&gt;

&lt;p&gt;Gmond has a default configuration it will use if it does not find the default configuration file /etc/ganglia/gmond.conf. To see the default configuration simply run the command:&lt;/p&gt;

&lt;p&gt;% gmond –default_config
and gmond will output its default configuration to stdout. This default configuration can serve as a good starting place for building a more custom configuration.&lt;/p&gt;

&lt;p&gt;% gmond –default_config &amp;gt; gmond.conf
would create a file gmond.conf which you can then edit to taste and copy to /etc/ganglia/gmond.conf or elsewhere.&lt;/p&gt;

&lt;p&gt;To start gmond with a configuration file other then /etc/ganglia/gmond.conf, simply specify the configuration file location by running&lt;/p&gt;

&lt;p&gt;% gmond –config /my/ganglia/configs/custom.conf
If you want to convert a 2.5.x configuration file to 3.x file format, run the following command&lt;/p&gt;

&lt;p&gt;% gmond –convert ./old_25_config.conf
and gmond with output the equivalent 3.x configuration file to stdout. You can then redirect that output to a new configuration file which can serve as a starting point for your configuration.&lt;/p&gt;

&lt;p&gt;% gmond –convert ./old_25_config.conf &amp;gt; ./new_26_config.conf
For details about gmond configuration options, simply run&lt;/p&gt;

&lt;p&gt;% man gmond.conf
for a complete listing of options with detailed explanations.&lt;/p&gt;

&lt;h3 id=&quot;gmetad-configuration&quot;&gt;Gmetad Configuration&lt;/h3&gt;

&lt;p&gt;The behavior of the Ganglia Meta Daemon is completely controlled by a single configuration file which is by default /etc/ganglia/gmetad.conf. For gmetad to do anything useful you much specify at least one data_source in the configuration. The format of the data_source line is as follows&lt;/p&gt;

&lt;p&gt;data_source “Cluster A” 127.0.0.1  1.2.3.4:8655  1.2.3.5:8625
  data_source “Cluster B” 1.2.4.4:8655
In this example, there are two unique data sources: “Cluster A” and “Cluster B”. The Cluster A data source has three redundant sources. If gmetad cannot pull the data from the first source, it will continue trying the other sources in order.&lt;/p&gt;

&lt;p&gt;If you do not specify a port number, gmetad will assume the default ganglia port which is 8649 (U&lt;em&gt;N&lt;/em&gt;I*X on a phone key pad)&lt;/p&gt;

&lt;p&gt;For a sample gmetad configuration file with comments, look at the gmetad.conf file provided as part of the distribution package in the gmetad directory&lt;/p&gt;

&lt;p&gt;gmetad has a –conf option to allow you to specify alternate configuration files&lt;/p&gt;

&lt;p&gt;% ./gmetad -conf=/tmp/my_custom_config.conf&lt;/p&gt;

&lt;h3 id=&quot;php-web-frontend-configuration&quot;&gt;PHP Web Frontend Configuration&lt;/h3&gt;

&lt;p&gt;Most configuration parameters reside in the ganglia/conf.php file. Here you may alter the template, gmetad location, RRDtool location, and set the default time range and metrics for graphs.&lt;/p&gt;

&lt;p&gt;The static portions of the Ganglia website are themable. This means you can alter elements such as section lables, some links, and images to suit your individual tastes and environment. The template_name variable names a directory containing the current theme. Ganglia uses TemplatePower to implement themes. A user-defined skin must conform to the template interface as defined by the default theme. Essentially, the variable names and START/END blocks in a custom theme must remain the same as the default, but all other HTML elements may be changed.&lt;/p&gt;

&lt;p&gt;Other configuration variables in conf.php specify the location of gmetad’s files, and where to find the rrdtool program. These locations need only be changed if you do not run gmetad on the webserver. Otherwise the default locations should work fine. The default_range variable specifies what range of time to show on the graphs by default, with possible values of hour, day, week, month, year. The default_metric parameter specifies which metric to show on the cluster view page by default.&lt;/p&gt;

&lt;h2 id=&quot;commandline-tools&quot;&gt;Commandline Tools&lt;/h2&gt;

&lt;p&gt;There are two commandline tools that work with gmond to add custom metrics and query the current state of a cluster: gmetric and gstat respectively.&lt;/p&gt;

&lt;h3 id=&quot;gmetric&quot;&gt;Gmetric&lt;/h3&gt;

&lt;p&gt;The Ganglia Metric Tool (gmetric) allows you to easily monitor any arbitrary host metrics that you like expanding on the core metrics that gmond measures by default.&lt;/p&gt;

&lt;p&gt;If you want help with the gmetric sytax, simply use the “help” commandline option&lt;/p&gt;

&lt;p&gt;% gmetric –help
  gmetric 3.6.1
  Purpose:
    The Ganglia Metric Client (gmetric) announces a metric
    on the list of defined send channels defined in a configuration file
  Usage: gmetric [OPTIONS]…
    -h, –help          Print help and exit
    -V, –version       Print version and exit
    -c, –conf=STRING   The configuration file to use for finding send channels
                        (default=&lt;code&gt;/etc/ganglia/gmond.conf&#39;)
    -n, --name=STRING   Name of the metric
    -v, --value=STRING  Value of the metric
    -t, --type=STRING   Either
                        string|int8|uint8|int16|uint16|int32|uint32|float|double
    -u, --units=STRING  Unit of measure for the value e.g. Kilobytes, Celcius
                        (default=&lt;/code&gt;’)
    -s, –slope=STRING  Either zero|positive|negative|both  (default=&lt;code&gt;both&#39;)
    -x, --tmax=INT      The maximum time in seconds between gmetric calls
                        (default=&lt;/code&gt;60’)
    -d, –dmax=INT      The lifetime in seconds of this metric  (default=`0’)
    -S, –spoof=STRING  IP address and name of host/device (colon separated) we
                          are spoofing  (default=’’)
    -H, –heartbeat     spoof a heartbeat message (use with spoof option)&lt;/p&gt;

&lt;p&gt;Gmetric sends the metric specified on the commandline to all udp_send_channels specified in the configuration file /etc/ganglia/gmond.conf by default. If you want to send metric to alternate udp_send_channels, you can specify a different configuration file as such:&lt;/p&gt;

&lt;p&gt;% gmetric –conf=./custom.conf -n “wow” -v “it works” -t “string”
All metrics in ganglia have a name, value, type and optionally units. For example, say I wanted to measure the temperature of my CPU (something gmond doesn’t do by default) then I could send this metric with name=”temperature”, value=”63”, type=”int16” and units=”Celcius”.&lt;/p&gt;

&lt;p&gt;Assume I have a program called cputemp which outputs in text the temperature of the CPU&lt;/p&gt;

&lt;p&gt;% cputemp
  63
I could easily send this data to all listening gmonds by running&lt;/p&gt;

&lt;p&gt;% gmetric –name temperature –value &lt;code&gt;cputemp&lt;/code&gt; –type int16 –units Celcius
Check the exit value of gmetric to see if it successfully sent the data: 0 on success and -1 on failure.&lt;/p&gt;

&lt;p&gt;To constantly sample this temperature metric, you just need too add this command to your cron table.&lt;/p&gt;

&lt;h3 id=&quot;gstat&quot;&gt;Gstat&lt;/h3&gt;

&lt;p&gt;The Ganglia Cluster Status Tool (gstat) is a commandline utility that allows you to get status report for your cluster.&lt;/p&gt;

&lt;p&gt;To get help with the commandline options, simply pass gstat the –help option&lt;/p&gt;

&lt;p&gt;% gstat –help
  gstat 3.6.1
  Purpose:
    The Ganglia Status Client (gstat) connects with a
    Ganglia Monitoring Daemon (gmond) and output a load-balanced list
    of cluster hosts
  Usage: gstat [OPTIONS]…
     -h         –help             Print help and exit
     -V         –version          Print version and exit
     -a         –all              List all hosts.  Not just hosts running gexec (default=off)
     -d         –dead             Print only the hosts which are dead (default=off)
     -m         –mpifile          Print a load-balanced mpifile (default=off)
     -1         –single_line      Print host and information all on one line (default=off)
     -l         –list             Print ONLY the host list (default=off)
     -n         –numeric          Print numeric addresses instead of hostnames (default=off)
     -iSTRING   –gmond_ip=STRING  Specify the ip address of the gmond to query (default=’127.0.0.1’)
     -pINT      –gmond_port=INT   Specify the gmond port to query (default=8649)
Note: gstat with no option will only show gexec-enabled hosts. To see all hosts that are UP (regardless of their gexec state) you need to add the –all flag.&lt;/p&gt;

&lt;p&gt;% gstat –all&lt;/p&gt;

&lt;h2 id=&quot;extending-ganglia-through-metric-modules&quot;&gt;Extending Ganglia through metric modules&lt;/h2&gt;

&lt;p&gt;There are currently two ways in which metric modules can be written and plugged into Gmond in order to extend the types of metrics that Ganglia is able to monitor. As of Ganglia 3.1, a pluggable interface has been added to allow the Gmond metric gathering agent to collect any type of metric that can be acquired through programatic means. The primary metric module interface is C with a secondary python interface. This means that pluggable modules can either be written and compiled into dynamically loadable C based language modules or written and deployed as python pluggable modules.&lt;/p&gt;

&lt;p&gt;The basic steps when writting a pluggable module either in C or in python, is as follows:&lt;/p&gt;

&lt;p&gt;Create a module definition structure that contains callback data and metric information
Implement 3 callback functions that will serve as the links between the Gmond metric gathering agent and the metric module. These callback functions include module initialization, metric handler and module cleanup.
There are simple metric module examples for both a C based and a python based module under the gmond/modules and gmond/python_modules source code sub-trees. Please see these module examples for more details.&lt;/p&gt;

&lt;h2 id=&quot;frequently-asked-questions-faq&quot;&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt;

&lt;p&gt;What metrics does ganglia collect on platform x?
To see a complete list of the metrics that a particular gmond supports, run the command:&lt;/p&gt;

&lt;p&gt;% gmond -m
and gmond will output all the metrics that it is capable of collecting and sending.&lt;/p&gt;

&lt;p&gt;This table describes all the metrics that ganglia collects and shows what platforms the metric are supported on. (The following table is only partially complete).&lt;/p&gt;

&lt;p&gt;Metric Name    Description                             Platforms
  ———————————————————————–
  boottime      System boot timestamp                    l,f
  bread_sec
  bwrite_sec
  bytes_in      Number of bytes in per second            l,f
  bytes_out     Number of bytes out per second           l,f
  cpu_aidle     Percent of time since boot idle CPU      l
  cpu_arm
  cpu_avm
  cpu_idle      Percent CPU idle                         l,f
  cpu_intr
  cpu_nice      Percent CPU nice                         l,f
  cpu_num       Number of CPUs                           l,f
  cpu_rm
  cpu_speed     Speed in MHz of CPU                      l,f
  cpu_ssys
  cpu_system    Percent CPU system                       l,f
  cpu_user      Percent CPU user                         l,f
  cpu_vm
  cpu_wait
  cpu_wio
  disk_free     Total free disk space                    l,f
  disk_total    Total available disk space               l,f
  load_fifteen  Fifteen minute load average              l,f
  load_five     Five minute load average                 l,f
  load_one      One minute load average                  l,f
  location      GPS coordinates for host                 e
  lread_sec
  lwrite_sec
  machine_type
  mem_buffers   Amount of buffered memory                l,f
  mem_cached    Amount of cached memory                  l,f
  mem_free      Amount of available memory               l,f
  mem_shared    Amount of shared memory                  l,f
  mem_sreclaimable    Amount of slab reclaimable memory  l (kernel &amp;gt;= 2.6.19)
  mem_total     Amount of available memory               l,f
  mtu           Network maximum transmission unit        l,f
  os_name       Operating system name                    l,f
  os_release    Operating system release (version)       l,f
  part_max_used Maximum percent used for all partitions  l,f
  phread_sec
  phwrite_sec
  pkts_in       Packets in per second                    l,f
  pkts_out      Packets out per second                   l,f
  proc_run      Total number of running processes        l,f
  proc_total    Total number of processes                l,f
  rcache
  swap_free     Amount of available swap memory          l,f
  swap_total    Total amount of swap memory              l,f
  sys_clock     Current time on host                     l,f
  wcache
  Platform key:
  l = Linux, f = FreeBSD, a = AIX, c = Cygwin
  m = MacOS, i = IRIX, h = HPUX,  t = Tru64
  e = Every Platform
If you are interested in how the metrics are collected, just take a look in directory ./libmetrics in the source distribution. There is a directory for each platform that is supported.&lt;/p&gt;

&lt;p&gt;What does the error “Process XML (x): XML_ParseBuffer() error at line x: not well-formed”
This is an error that occurs when a ganglia components reads data from another ganglia component and finds that the XML is not well-formed. The most common time this is a problem is when the PHP web frontend tries to read the XML stream from gmetad.&lt;/p&gt;

&lt;p&gt;To troubleshoot this problem, capture an XML from the ganglia component in question (gmetad/gmond). This is easy to do if you have telnet installed. Simply login to the machine running the component and run.&lt;/p&gt;

&lt;p&gt;% telnet localhost 8651
By default, gmetad exports its XML on port 8651 and gmond exports its XML on port 8649. Modify the port number above to suite your configuration.&lt;/p&gt;

&lt;p&gt;When you connect to the port you should get an XML stream. If not, look in the process table on the machine to ensure that the component is actually running.&lt;/p&gt;

&lt;p&gt;Once you are getting an XML stream, capture it to a file by running.&lt;/p&gt;

&lt;p&gt;% telnet localhost 8651 &amp;gt; XML.txt
  Connection closed by foreign host.
If you open the file XML.txt, you will see the captured XML stream. You will need to remove the first three lines of the XML.txt which will read…&lt;/p&gt;

&lt;p&gt;Trying 127.0.0.1…
  Connected to localhost.
  Escape character is ‘^]’.
Those lines are output from telnet and not the ganglia component (I wish telnet would send those messages to stderr but they are send to stdout).&lt;/p&gt;

&lt;p&gt;There are many ways that XML can be misformed. The great tool for validating XML is xmllint. xmllint will read the file and find the line containing the error.&lt;/p&gt;

&lt;p&gt;% xmllint –valid –noout XML.txt
will read your captured XML stream, validate it against the ganglia DTD and check that it is well-formed XML. xmllint will quiet exit if there are no errors. If there are errors they will be reported with line numbers. For example…&lt;/p&gt;

&lt;p&gt;/tmp/XML.txt:3393: error: Opening and ending tag mismatch: HOST and CLUSTER
  &amp;lt;/CLUSTER&amp;gt;
         ^
  /tmp/XML.txt:3394: error: Opening and ending tag mismatch: CLUSTER and GANGLIA_XML
  &amp;lt;/GANGLIA_XML&amp;gt;
             ^
  /tmp/XML.txt:3395: error: Premature end of data in tag GANGLIA_XML
If you get errors, open XML.txt and go to the line numbers in question. See if you can understand based on your configuration how these errors could occur. If you cannot fix the problem yourself, please email your XML.txt and output from xmllint to ganglia-developers@lists.sourceforge.net. Please include information about the version of each component in question along with the operating system they are running on. The more details we have about your configuration the more likely it is we will be able to help you. Also, all mailing to ganglia-developers is archiving and available to read on the web. You may want to modify XML.txt to remove any sensitive information.&lt;/p&gt;

&lt;p&gt;How do I remove a host from the list?
A common problem that people have is not being able to remove a host from the ganglia web frontend.&lt;/p&gt;

&lt;p&gt;Here is a common scenario&lt;/p&gt;

&lt;p&gt;All hosts in a cluster are send on the ganglia udp_send_channels.
One of the hosts fails or is moved for whatever reason.
All the hosts in the cluster report that the host is “dead” or “expired”.
The sysadmin wants to removed this host from the “dead” list.
Unfortunately there is currently no nice way to remove a single dead host from the list. All data in gmond is soft state so you will need to restart all gmond and gmetad processes. It is important to note that ALL dead hosts will be flushed from the record by restarting the processes (since they have to hear the host at least once to know it is expired).&lt;/p&gt;

&lt;p&gt;If you add the line&lt;/p&gt;

&lt;p&gt;globals {
    host_dmax = 3600
  }
then hosts will be removed from host tables when they haven’t been heard from in 3600 seconds. See man gmond.conf for details.&lt;/p&gt;

&lt;p&gt;How good is Solaris, IRIX, Tru64 support?
Here is an email from Steve Wagner about the state of the ganglia on Solaris, IRIX and Tru64. Steve is to thank for porting ganglia to Solaris and Tru64. He also helped with the IRIX port.&lt;/p&gt;

&lt;p&gt;State of the IRIX port:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CPU percentage stuff hasn’t improved despite my efforts.  I fear there
may be a flaw in the way I’m summing counters for all the CPUs.&lt;/li&gt;
  &lt;li&gt;Auto-detection of network interfaces apparently segfaults.&lt;/li&gt;
  &lt;li&gt;Memory and load reporting appear to be running properly.&lt;/li&gt;
  &lt;li&gt;CPU speed is not being reported properly on multi-proc machines.&lt;/li&gt;
  &lt;li&gt;Total/running processes are not reported.&lt;/li&gt;
  &lt;li&gt;gmetad untested.&lt;/li&gt;
  &lt;li&gt;Monitoring core apparently stable in foreground, background being tested
   (had a segfault earlier).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;State of the Tru64 port:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CPU percentage stuff here works perfectly.&lt;/li&gt;
  &lt;li&gt;Memory and swap usage stats are suspected to be inaccurate.&lt;/li&gt;
  &lt;li&gt;Total/running processes are not reported.&lt;/li&gt;
  &lt;li&gt;gmetad untested.&lt;/li&gt;
  &lt;li&gt;Monitoring core apparently stable in foreground and background.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;State of the Solaris port:
   *  CPU percentages are slightly off, but correct enough for trending
      purposes.
   *  Load, ncpus, CPU speed, breads/writes, lreads/writes, phreads/writes,
      and rcache/wcache are all accurate.
   *  Memory/swap statistics are suspiciously flat, but local stats bear
      this out (and they &lt;em&gt;are&lt;/em&gt; being updated) so I haven’t investigated
      further.
   *  Total processes are counted, but not running ones.
   *  gmetad appears stable&lt;/p&gt;

&lt;p&gt;Anyway, all three ports I’ve been messing with are usable and fairly
   stable.  Although there are areas for improvement I think we really can’t
   keep hogging all this good stuff - what I’m looking at is ready for
   release.
Where are the debian packages?
Debian packages for 2.5 are available from the main Debian archive for all releases.&lt;/p&gt;

&lt;p&gt;There was never an oficial Debian package for 3.0 and so if you need to use that branch you will need to build your own binaries.&lt;/p&gt;

&lt;p&gt;Packages for 3.1 are available from Debian (and therefore derivative distributions like Ubuntu) standard repositories.&lt;/p&gt;

&lt;p&gt;How should I configure multihomed machines?
Various issues arise when a multihomed machine is running the gmond agent.&lt;/p&gt;

&lt;p&gt;Sending multicast packets out on the right interface: the mcast_if option can be declared in the udp_send_channel to force outgoing multicast packets to use a particular interface. The system administrator may also be able to make other platform-specific configuration settings through the OS to achieve the desired behaviour.&lt;/p&gt;

&lt;p&gt;Ensuring that outgoing metric packets are always sent with the same source address: the bind and bind_hostname parameters are the solution. Either (but not both) of these can be specified in the udp_send_channel if required. See the gmond.conf man page for details.&lt;/p&gt;

&lt;p&gt;Previous advice given in this document suggested adding a route like so:&lt;/p&gt;

&lt;p&gt;route add -host 239.2.11.71 dev eth1&lt;/p&gt;

&lt;p&gt;and this method is still valid, but it will be over-ridden by the bind or bind_hostname setting.&lt;/p&gt;

&lt;p&gt;How should I configure my Cisco Catalyst Switches?
Perhaps information regarding gmond on networks set up through cisco catalyst switches should be mentioned in the ganglia documentation. I think by default multicast traffic on the catalyst will flood all devices unless configured properly. Here is a relavent snipet from a message forum, with a link to cisco document.&lt;/p&gt;

&lt;p&gt;If what you are trying to do, is minimizing the impact on your network due to a multicast application, this link may describe what you want to do: http://www.cisco.com/warp/public/473/38.html&lt;/p&gt;

&lt;p&gt;We set up our switches according to this after a consultant came in and installed an application multicasting several hundred packets per second. This made the network functional again.&lt;/p&gt;

&lt;h2 id=&quot;getting-support&quot;&gt;Getting Support&lt;/h2&gt;

&lt;p&gt;The tired and thirsty prospector threw himself down at the edge of the 
  watering hole and started to drink. But then he looked around and saw 
  skulls and bones everywhere. “Uh-oh,” he thought. “This watering hole 
  is reserved for skeletons.” –Jack Handey
There are three mailing lists available to you: ganglia-general, ganglia-developers and ganglia-announce. You can join these lists or read their archives by visiting https://sourceforge.net/mail/?group_id=43021&lt;/p&gt;

&lt;p&gt;All of the ganglia mailing lists are closed. That means that in order to post to the lists, you must be subscribed to the list. We’re sorry for the inconvenience however it is very easy to subscribe and unsubscribe from the lists. We had to close the mailing lists because of SPAM problems.&lt;/p&gt;

&lt;p&gt;When you need help please follow these steps until your problem is resolved.&lt;/p&gt;

&lt;p&gt;completely read the documentation&lt;/p&gt;

&lt;p&gt;check the ganglia-general archive to see if other people have had the same problem&lt;/p&gt;

&lt;p&gt;post your support request to the ganglia-general mailing list&lt;/p&gt;

&lt;p&gt;check the ganglia-developers archive&lt;/p&gt;

&lt;p&gt;post your question to the ganglia-developers list&lt;/p&gt;

&lt;p&gt;please send all bugs, patches, and feature requests to the ganglia-developers list after you have checked the ganglia-developers archive to see if the question has already been asked and answered.&lt;/p&gt;

&lt;h2 id=&quot;copyright&quot;&gt;Copyright&lt;/h2&gt;

&lt;p&gt;Copyright (C) 2002,2003 University of California, Berkeley&lt;/p&gt;

&lt;h2 id=&quot;authors&quot;&gt;Authors&lt;/h2&gt;

&lt;p&gt;The Ganglia Development Team…&lt;/p&gt;

&lt;p&gt;Bas van der Vlies      basv               Developer    basv at users.sourceforge.net 
 Neil T. Spring         bluehal            Developer    bluehal at users.sourceforge.net
 Brooks Davis           brooks_en_davis    Developer    brooks_en_davis at users.sourceforge.net
 Eric Fraser            fraze              Developer    fraze at users.sourceforge.net 
 greg bruno             gregbruno          Developer    gregbruno at users.sourceforge.net
 Jeff Layton            laytonjb        Developer       laytonjb at users.sourceforge.net     &lt;br /&gt;
 Doc Schneider          maddocbuddha    Developer       maddocbuddha at users.sourceforge.net 
 Mason Katz             masonkatz       Developer       masonkatz at users.sourceforge.net    &lt;br /&gt;
 Mike Howard            mhoward         Developer       mhoward at users.sourceforge.net      &lt;br /&gt;
 Matt Massie            massie          Project Admin   massie at users.sourceforge.net
 Oliver Mössinger      olivpass        Developer       olivpass at users.sourceforge.net     &lt;br /&gt;
 Preston Smith          pmsmith         Developer       pmsmith at users.sourceforge.net      &lt;br /&gt;
 Federico David Sacerdoti sacerdoti     Developer       sacerdoti at users.sourceforge.net    &lt;br /&gt;
 Tim Cera               timcera         Developer       timcera at users.sourceforge.net      &lt;br /&gt;
 Mathew Benson          wintermute11    Developer       wintermute11 at users.sourceforge.net &lt;br /&gt;
 Brad Nicholes          bnicholes       Developer       bnicholes at users.sourceforge.net
 Carlo Arenas           carenas         Developer       carenas at users.sourceforge.net&lt;/p&gt;

&lt;h2 id=&quot;contributors&quot;&gt;Contributors&lt;/h2&gt;

&lt;p&gt;There have been dozens of contributors who have provided patches and helpful bug reports. We need to list them here later.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

</content>
   </entry>
   
   <entry>
     <title>Ganglia简介与安装</title>
     <link href="http://ningg.github.com/install-ganglia"/>
     <updated>2014-11-20T00:00:00+08:00</updated>
     <id>http://ningg.github.com/install-ganglia</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;准备监控整个Flume、Kafka、Storm框架运行状态，不想重复造轮子，初步查询官网发现这个几个东西都可以跟Ganglia结合。初步查了一下Ganglia的应用很广泛，上Ganglia，走起。&lt;/p&gt;

&lt;h2 id=&quot;ganglia&quot;&gt;Ganglia基本知识&lt;/h2&gt;

&lt;p&gt;[Ganglia官网]提供了较为简介的介绍，整理一下有几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ganglia是一个可扩展性不错的分布式监控系统；&lt;/li&gt;
  &lt;li&gt;监控对象：集群，这个集群上可以有分布式系统，也可以只是单独的集群；&lt;/li&gt;
  &lt;li&gt;Ganglia本身就是分布式的集群，那就有集群结构，Ganglia集群采用分层结构；&lt;/li&gt;
  &lt;li&gt;Ganglia利用了一些现有的技术，列几个：
    &lt;ul&gt;
      &lt;li&gt;XML for data representation;&lt;/li&gt;
      &lt;li&gt;XDR for compact, portable data transport;&lt;/li&gt;
      &lt;li&gt;RDDtool for data storage and visualization;&lt;/li&gt;
      &lt;li&gt;data structures and algorithms to achieve very low per-node overheads and high concurrency;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;软件版本信息&lt;/h3&gt;

&lt;p&gt;此次采用最新的Ganglia版本，具体：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/projects/ganglia/files/&quot;&gt;ganglia-3.6.1(ganglia monitoring core)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[ganglia-3.6.1(ganglia-web)][ganglia-3.6.1(ganglia-web)]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当前（2014-11-20），Ganglia由以下几个组件构成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2个 unique daemons：&lt;code&gt;gmond&lt;/code&gt;、&lt;code&gt;gmetad&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;1个 PHP-based web frontend;&lt;/li&gt;
  &lt;li&gt;几个 small utility programs;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下面针对Ganglia组件的介绍，完全参考自ganglia-3.6.1:ganglia monitoring core源码压缩包中的ganglia.html文件。&lt;/p&gt;

&lt;h3 id=&quot;ganglia-monitoring-daemon-gmond&quot;&gt;Ganglia Monitoring Daemon (gmond)&lt;/h3&gt;

&lt;p&gt;Gmond is a multi-threaded daemon which runs on each cluster node you want to monitor. Installation is easy. You don’t have to have a common NFS filesystem or a database backend, install special accounts, maintain configuration files or other annoying hassles.&lt;/p&gt;

&lt;p&gt;Gmond has four main responsibilities: monitor changes in host state, announce relevant changes, listen to the state of all other ganglia nodes via a unicast or multicast channel and answer requests for an XML description of the cluster state.&lt;/p&gt;

&lt;p&gt;Each gmond transmits in information in two different ways: unicasting/multicasting host state in external data representation (XDR) format using UDP messages or sending XML over a TCP connection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于&lt;code&gt;gmond&lt;/code&gt;daemon说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gmond部署位置：每一个需要监控的node；&lt;/li&gt;
  &lt;li&gt;gmond需要的配置：安装简便，不依赖数据库；&lt;/li&gt;
  &lt;li&gt;gmond进程的作用：
    &lt;ul&gt;
      &lt;li&gt;monitor changes in host state&lt;/li&gt;
      &lt;li&gt;announce relevant changes&lt;/li&gt;
      &lt;li&gt;listen to the state of all other ganglia nodes via a unicast or multicast channel&lt;/li&gt;
      &lt;li&gt;answer requests for an XML description of the cluster state&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;gmond进程向其他节点发送信息（unicasting or multicasting）有两种方式：
    &lt;ul&gt;
      &lt;li&gt;UDP：external data representation（XDR）；&lt;/li&gt;
      &lt;li&gt;TCP：XML；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ganglia-meta-daemon-gmetad&quot;&gt;Ganglia Meta Daemon (gmetad)&lt;/h3&gt;

&lt;p&gt;Federation in Ganglia is achieved using a tree of point-to-point connections amongst representative cluster nodes to aggregate the state of multiple clusters. At each node in the tree, a Ganglia Meta Daemon (gmetad) periodically polls a collection of child data sources, parses the collected XML, saves all numeric, volatile metrics to round-robin databases and exports the aggregated XML over a TCP sockets to clients. Data sources may be either gmond daemons, representing specific clusters, or other gmetad daemons, representing sets of clusters. Data sources use source IP addresses for access control and can be specified using multiple IP addresses for failover. The latter capability is natural for aggregating data from clusters since each gmond daemon contains the entire state of its cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于gmetad说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ganglia也是集群，那就有集群的结构，Ganglia：树状结构；&lt;/li&gt;
  &lt;li&gt;gmetad部署位置：树的每个node上；&lt;/li&gt;
  &lt;li&gt;gmetad进程的作用：
    &lt;ul&gt;
      &lt;li&gt;periodically polls a collection of child data sources&lt;/li&gt;
      &lt;li&gt;parses the collected XML&lt;/li&gt;
      &lt;li&gt;saves all numeric, volatile metrics to round-robin databases&lt;/li&gt;
      &lt;li&gt;exports the aggregated XML over a TCP sockets to clients&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;gmetad进程从child data source收集数据，data source是指：
    &lt;ul&gt;
      &lt;li&gt;gmond daemons, representing specific clusters&lt;/li&gt;
      &lt;li&gt;other gmetad daemons, representing sets of clusters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;data source有几个区分点：
    &lt;ul&gt;
      &lt;li&gt;use source IP addresses for access control&lt;/li&gt;
      &lt;li&gt;using multiple IP addresses for failover（失效备援）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ganglia-php-web-frontend&quot;&gt;Ganglia PHP Web Frontend&lt;/h3&gt;

&lt;p&gt;The Ganglia web frontend provides a view of the gathered information via real-time dynamic web pages. Most importantly, it displays Ganglia data in a meaningful way for system administrators and computer users. Although the web frontend to ganglia started as a simple HTML view of the XML tree, it has evolved into a system that keeps a colorful history of all collected data.&lt;/p&gt;

&lt;p&gt;The Ganglia web frontend caters to system administrators and users. For example, one can view the CPU utilization over the past hour, day, week, month, or year. The web frontend shows similar graphs for Memory usage, disk usage, network statistics, number of running processes, and all other Ganglia metrics.&lt;/p&gt;

&lt;p&gt;The web frontend depends on the existence of the &lt;code&gt;gmetad&lt;/code&gt; which provides it with data from several Ganglia sources. Specifically, the web frontend will open the local port &lt;code&gt;8651&lt;/code&gt; (by default) and expects to receive a Ganglia XML tree. The web pages themselves are highly dynamic; any change to the Ganglia data appears immediately on the site. This behavior leads to a very responsive site, but requires that the full XML tree be parsed on every page access. Therefore, the Ganglia web frontend should run on a fairly powerful, dedicated machine if it presents a large amount of data.&lt;/p&gt;

&lt;p&gt;The Ganglia web frontend is written in the PHP scripting language, and uses graphs generated by gmetad to display history information. It has been tested on many flavours of Unix (primarily Linux) with the Apache webserver and the PHP module (5.0.0 or later). The GD graphics library for PHP is used to generate pie charts in the frontend and needs to be installed separately. On RPM-based system, it is usually provided by the php-gd package.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于PHP Web Frontend说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提供real-time dynamic web pages；这个实时的动态页面信息是如何获得的呢？
    &lt;ul&gt;
      &lt;li&gt;每个web page都需要解析整个XML tree；&lt;/li&gt;
      &lt;li&gt;如果data的量比较大，则建议web frontend运行在一个专用的主机上；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;能够提供的监控数据：
    &lt;ul&gt;
      &lt;li&gt;CPU利用率&lt;/li&gt;
      &lt;li&gt;Memory usage&lt;/li&gt;
      &lt;li&gt;disk usage&lt;/li&gt;
      &lt;li&gt;network statistics&lt;/li&gt;
      &lt;li&gt;number of running processes等等；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;时间维度上：hour、day、week、month、year；&lt;/li&gt;
  &lt;li&gt;Web Frontend依赖于&lt;code&gt;gmetad&lt;/code&gt;进程，gmetad进程负责从Ganglia sources中获取data并提供给web frontend；&lt;/li&gt;
  &lt;li&gt;Web Frontend默认开启&lt;code&gt;8651&lt;/code&gt;端口，来接收Ganglia XML tree数据；&lt;/li&gt;
  &lt;li&gt;Ganglia Web Frontend是PHP语言实现的，并且通过安装GD graphics library（php）可展现饼状图；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：疑问，Web Frontend 只能读取其运行的服务器上&lt;code&gt;gmetad&lt;/code&gt;提供的数据？这就是说，要求web frontend必须运行在整个Ganglia集群的树状结构的根节点？&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;配置基础环境&lt;/h2&gt;

&lt;p&gt;当前服务器基本环境（CentOS 6.4 x86_64）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost html]# lsb_release -a
LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID: CentOS
Description:    CentOS release 6.4 (Final)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-3&quot;&gt;新增用户和组&lt;/h3&gt;

&lt;p&gt;为方便所有操作，以及进行权限管理，新建用户ganglia：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;useradd ganglia
# 默认，创建user：ganglia时，也创建了group：ganglia
# groupadd ganglia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如何卸载gweb？（make uninstall）&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;几个基本组件&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;gmetad&lt;/code&gt;进程需要去&lt;a href=&quot;http://www.rrdtool.org/&quot;&gt;rrdtool&lt;/a&gt;，同时如果要同时在node上安装&lt;code&gt;gmetad&lt;/code&gt;和&lt;code&gt;gmond&lt;/code&gt;，则需要提前安装：apr&lt;em&gt;、pcre&lt;/em&gt;、zlib*，具体：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#rrdtool
yum install rrdtool
yum install rrdtool-devel

#apr
yum install apr
yum install apr-devel

#libpcre
yum install pcre
yum install pcre-devel

#zlib-devel
yum install zlib
yum install zlib-devel

#python-devel
yum install python
yum install python-devel
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;libconfuse&quot;&gt;libconfuse&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# 出错信息
libconfuse not found

... can not be used when making a shared object; recompile with -fPIC
/usr/local/confuse/lib/libconfuse.a: could not read symbols: Bad value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体版本信息：confuse-2.7.tar.gz，下载来源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.nongnu.org/confuse/&quot;&gt;libconfuse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/martinh/libconfuse&quot;&gt;libconfuse(GitHub)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下载之后，直接安装即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf confuse-2.7.tar.gz
cd confuse-2.7

./configure CFLAGS=-fPIC --disable-nls
make
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;libexpat&quot;&gt;libexpat&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# 出错信息
libexpat not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体版本信息：expat-2.1.0.tar.gz，下载来源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.libexpat.org/&quot;&gt;libexpat&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/LuaDist/libexpat&quot;&gt;libexpat(GitHub)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下载之后，直接安装即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf expat-2.1.0.tar.gz
cd expat-2.1.0

./configure
make
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ganglia-1&quot;&gt;安装Ganglia&lt;/h2&gt;

&lt;p&gt;安装Ganglia有几种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rpm包：rpm -Uvh ganglia-*.rpm&lt;/li&gt;
  &lt;li&gt;yum源：yum install ganglia*&lt;/li&gt;
  &lt;li&gt;本地编译源代码：make &amp;amp;&amp;amp; make install&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于在&lt;a href=&quot;http://sourceforge.net/projects/ganglia/files/&quot;&gt;Ganglia官网&lt;/a&gt;上没有找到最新的rpm包，并且本地配置的yum源没有提供ganglia组件，因此本次采用编译源代码方式。&lt;/p&gt;

&lt;p&gt;在前面配置好基础环境之后，这一部分简要说一下，如何安装、启动Ganglia，具体包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gmond&lt;/li&gt;
  &lt;li&gt;gmetad&lt;/li&gt;
  &lt;li&gt;web frontend&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gmondgmetad&quot;&gt;gmond和gmetad&lt;/h3&gt;

&lt;p&gt;此次安装版本为：&lt;a href=&quot;http://sourceforge.net/projects/ganglia/files/&quot;&gt;ganglia-3.6.1(ganglia monitoring core)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;安装命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf ganglia-3.6.1.tar.gz
cd ganglia-3.6.1

# ./configure默认安装 gmond
# 利用 --with-gmetad选项，同时安装 gmetad
./configure --with-gmetad --enable-gexec
make
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;gmetad&quot;&gt;配置gmetad&lt;/h4&gt;

&lt;p&gt;由于gmetad依赖rrdtool，需要设置两个东西：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;设置datasource和UID，具体：&lt;/p&gt;

    &lt;p&gt;vim /usr/local/etc/gmetad.conf
  data_source “RT-SYS” localhost
  setuid_username “apache”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;设置rrdtool的数据目录，具体命令：&lt;/p&gt;

    &lt;p&gt;mkdir -p /var/lib/ganglia/rrds
  chown -R apache:apache /var/lib/ganglia/rrds&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上述&lt;code&gt;chown&lt;/code&gt;时，利用的用户UID与gmetad.conf中&lt;code&gt;setuid_username&lt;/code&gt;的配置保持一致；另外，安装配置好之后，可通过命令&lt;code&gt;gmetad -d 2&lt;/code&gt;来在前台运行gmetad进程，以方便查看其运行状态。&lt;/p&gt;

&lt;h4 id=&quot;gmond&quot;&gt;配置gmond&lt;/h4&gt;

&lt;p&gt;利用gmond的默认配置生成配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gmond -t &amp;gt; /usr/local/etc/gmond.conf

vim /usr/local/etc/gmond.conf
cluster {  
	name=&quot;RT-SYS&quot;   //和gmetad.conf配置文件对应  
	owner=&quot;apache&quot;   //和gmetad.conf配置文件对应  
	latlong=&quot;unspecified&quot;  
	url=&quot;unspecified&quot;  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;备注&lt;/strong&gt;：&lt;code&gt;whereis&lt;/code&gt;命令的用途？例如下面怎么解释&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost html]# whereis gmond
gmond: /usr/local/sbin/gmond /usr/local/etc/gmond.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;gmond的详细信息，可以通过命令&lt;code&gt;man gmond&lt;/code&gt;和&lt;code&gt;man gmond.conf&lt;/code&gt;来查看。&lt;/p&gt;

&lt;h3 id=&quot;gmondgmetad-1&quot;&gt;添加服务：gmond、gmetad&lt;/h3&gt;

&lt;p&gt;通过上述安装步骤，服务器上应该已经安装了以下文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;/usr/local/bin/gstat&lt;/li&gt;
  &lt;li&gt;/usr/local/bin/gmetric&lt;/li&gt;
  &lt;li&gt;/usr/local/sbin/gmond&lt;/li&gt;
  &lt;li&gt;/usr/local/sbin/gmetad&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;备注&lt;/strong&gt;：本地实测是上面的位置，与官方源码自带文档ganglia-3.6.1/ganglia.html的说法有差异。&lt;/p&gt;

&lt;p&gt;在Linux上按照上述步骤，通过编译源码方式安装的的Ganglia，那可以将&lt;code&gt;gmond&lt;/code&gt;和&lt;code&gt;gmetad&lt;/code&gt;添加到sys service中，并且配置是否开机启动。
配置gmetad服务步骤如下（gmond同理）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@cib02166 ganglia-3.6.1]# cd gmetad
[root@localhost gmetad]# cp gmetad.init /etc/rc.d/init.d/gmetad

# 修改/etc/init.d/gmetad中 GMETAD=/usr/local/sbin/gmetad
[root@localhost gmetad]# vim /etc/init.d/gmetad

[root@localhost gmetad]# chkconfig --add gmetad
[root@localhost gmetad]# chkconfig --list gmetad
gmetad          0:off   1:off   2:on    3:on    4:on    5:on    6:off

[root@localhost gmetad]# service gmetad start
Starting GANGLIA gmetad:                                   [  OK  ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;思考&lt;/strong&gt;：如何验证gmond、gmetad已经正常安装并成功启动？
&lt;strong&gt;RE&lt;/strong&gt;：两种方法，方法1：通过命令&lt;code&gt;netstat -tpnl | grep &quot;gmond&quot;&lt;/code&gt;即可查看是否启动，当然也可以通过&lt;code&gt;service gmond status&lt;/code&gt;查看；方法2：通过&lt;code&gt;gmond -d 5&lt;/code&gt;在前台启动进程，并查看输出信息。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;思考&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不添加service，怎么启动gmond、gmeta？&lt;/li&gt;
  &lt;li&gt;为什么要有gmetad.init？&lt;/li&gt;
  &lt;li&gt;/etc/rc.d/init.d/目录又是干什么的？&lt;/li&gt;
  &lt;li&gt;chkconfig命令的含义？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;web-frontend&quot;&gt;web frontend&lt;/h3&gt;

&lt;h4 id=&quot;section-5&quot;&gt;安装前准备&lt;/h4&gt;

&lt;p&gt;此次安装版本为：&lt;a href=&quot;http://sourceforge.net/projects/ganglia/files/&quot;&gt;ganglia-3.6.2(ganglia-web)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：提前说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;运行web frontend的节点，需要提前安装gmetad进程；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ganglia-web-3.6.2/conf.php&lt;/code&gt;文件包含了大部分的配置信息：
    &lt;ul&gt;
      &lt;li&gt;template&lt;/li&gt;
      &lt;li&gt;gmetad location&lt;/li&gt;
      &lt;li&gt;RRDtool location&lt;/li&gt;
      &lt;li&gt;set the default time range and metrics for graphs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;提前安装配置Apache服务器，具体安装命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 安装apache服务器，以及PHP支持的组件
yum install php-common php-cli php php-gd httpd
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;web-frontend-1&quot;&gt;安装web frontend（推荐）&lt;/h4&gt;

&lt;p&gt;在ganglia web的解压文件中能够看到一个文件&lt;code&gt;Makefile&lt;/code&gt;，通过对其进行设置就可以实现web frontend的快捷安装，具体要配置的参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Location where gweb should be installed to (excluding conf, dwoo dirs).
GDESTDIR = /usr/share/ganglia-webfrontend

# Location where default apache configuration should be installed to.
GCONFDIR = /etc/ganglia-web

# Gweb statedir (where conf dir and Dwoo templates dir are stored)
GWEB_STATEDIR = /var/lib/ganglia-web

# Gmetad rootdir (parent location of rrd folder)
GMETAD_ROOTDIR = /var/lib/ganglia

APACHE_USER = www-data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将对上面可配置的参数进行简要介绍：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GDESTDIR：Location where gweb should be installed to (excluding conf, dwoo dirs)，要与Apache服务器的配置文件&lt;code&gt;http.conf&lt;/code&gt;中&lt;code&gt;$DocumentRoot&lt;/code&gt;保持一致，通常命名为&lt;code&gt;$DocumentRoot/ganglia&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;GCONFDIR：Location where default apache configuration should be installed to.&lt;em&gt;（什么含义？）&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;GWEB_STATEDIR：Gweb statedir (where conf dir and Dwoo templates dir are stored)&lt;/li&gt;
  &lt;li&gt;GMETAD_ROOTDIR：gmetad rootdir，parrent localtion of rrd folder&lt;/li&gt;
  &lt;li&gt;APACHE_USER：设置Apache服务器的UID，具体，与Apache服务器配置文件&lt;code&gt;http.conf&lt;/code&gt;中&lt;code&gt;$User&lt;/code&gt;保持一致&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;配置完&lt;code&gt;Makefile&lt;/code&gt;文件后，直接运行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ganglia-web-3.6.2
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开浏览器，查看[http://locahost/ganglia]&lt;/p&gt;

&lt;p&gt;（疑问：makefile的作用？单纯的命令集合吗？）&lt;/p&gt;

&lt;h4 id=&quot;web-frontend-2&quot;&gt;安装web frontend（弃用）&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf ganglia-web-3.6.2.tar.gz
cd ganglia-web-3.6.2

# 将整个文件夹ganglia-web-3.6.2复制到apache服务器的DocumentRoot所指定的目录下
cp -a -f ganglia-web-3.6.2 /var/www/html/
ln -s /var/www/html/ganglia-web-3.6.2 /var/www/html/ganglia

service httpd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过：[http://locahost/ganglia]即可访问。如果[http://locahost/ganglia]显示如下页面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/install-ganglia/web-frontend-error.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大意是说无法创建目录以及文件，OK，估计是权限问题，在后台，手动创建一个根目录，并将owner更改为apache（是启动httpd的用户）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/ganglia-web/conf
mkdir -p /var/lib/ganglia-web/dwoo/cache
mkdir -p /var/lib/ganglia-web/dwoo/compiled
chown -R apache:apache /var/lib/ganglia-web
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;遇到的错误以及解决办法&lt;/h2&gt;

&lt;h3 id=&quot;section-7&quot;&gt;错误1&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# service gmond start时，出现错误：
error while loading shared libraries: libconfuse.so.0: cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /usr/local/lib
ln -s libconfuse.so.0 libconfuse.so.0.0.0
cp libconfuse.so.0 ../lib64/
service gmond start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;错误2&lt;/h3&gt;

&lt;p&gt;启动gmetad之后，通过命令&lt;code&gt;service gmetad status&lt;/code&gt;查询出现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gmetad dead but subsys locked
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析：上述错误没有见过呀，&lt;code&gt;subsys locked&lt;/code&gt;很眼熟，应该是进程已被锁定，但有这一点信息还不够，遇到问题时，基本思路：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;收集详细的错误信息，来分析错误；&lt;/li&gt;
  &lt;li&gt;根据上述分析，采取处理对策；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gmetad&lt;/code&gt;进程的启动日志在哪？文件&lt;code&gt;/var/log/messages&lt;/code&gt;；&lt;em&gt;（启动信息输出到messages中，这是在哪设置的？）&lt;/em&gt;通过如下命令来查看具体信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost log]# tail -f messages
/usr/local/sbin/gmetad[9510]: Please make sure that /var/lib/ganglia/rrds exists: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;奥，原来gmetad进程存储数据的目录&lt;code&gt;/var/lib/ganglia/rrds&lt;/code&gt;没有创建，抓紧创建一下（命令：&lt;code&gt;mkdir -p /var/lib/ganglia/rrds&lt;/code&gt;），再次启动gmetad，还是不行，查看错误信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost log]# tail -f messages
/usr/local/sbin/gmetad[11701]: Please make sure that /var/lib/ganglia/rrds is owned by nobody
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;gmetad在启动之后，会自动归指定UID接管，具体在&lt;code&gt;/usr/local/etc/gmetad.conf&lt;/code&gt;中配置&lt;code&gt;setuid_username&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#-------------------------------------------------------------------------------
# If you don&#39;t want gmetad to setuid then set this to off
# default: on
# setuid off
#
#-------------------------------------------------------------------------------
# User gmetad will setuid to (defaults to &quot;nobody&quot;)
# default: &quot;nobody&quot;
setuid_username &quot;storm&quot;
#
#-------------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，修改目录&lt;code&gt;/var/lib/ganglia/rrds&lt;/code&gt;的所属用户和组（与上述设置保持一致）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chown -R storm:storm /var/lib/ganglia/rrds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK，再次启动gmetad，成功启动。&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;错误3&lt;/h3&gt;

&lt;p&gt;通过rpm包或者yum源方式安装web frontend时，默认web frontend会被安装在&lt;code&gt;/usr/share/ganglia-webfrontend&lt;/code&gt;目录下，这样通过[http://locahost/ganglia]就无法进行访问。&lt;/p&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 利用符号链接
ln -s /usr/share/ganglia-webfrontend /var/www/html/ganglia
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-10&quot;&gt;补充思考&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：本文中实际是直接编译源码来安装组件的，即：&lt;code&gt;./configure&lt;/code&gt;以及&lt;code&gt;make install&lt;/code&gt;方式，那有个问题：在安装成功后，删掉安装时使用的源码文件，会影响安装的组件吗？&lt;em&gt;（个人感觉应该不影响才是基本需求）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RE&lt;/strong&gt;：要弄清上面的问题，需要弄清楚&lt;code&gt;./configure&lt;/code&gt;以及&lt;code&gt;make install&lt;/code&gt;执行过程中，导致进行了哪些操作？即：如何将软件安装到哪了？配置文件在哪？&lt;/p&gt;

&lt;h2 id=&quot;ganglia-2&quot;&gt;配置Ganglia集群的拓扑&lt;/h2&gt;

&lt;p&gt;上面主要说的是一个目标：在一台服务器上安装Ganglia的组件：gmond、gmetad、web frontend；这些都是针对单个服务器（single ganglia node）来说的。那如何将多个Ganglia node构成一个Ganglia cluster呢？之前我们简单提到：Ganglia是按照树状结构来组织的，下面将说一下细节。&lt;/p&gt;

&lt;p&gt;构成拓扑的有几个概念：node、cluster、grid，什么含义？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：同一台服务器上，能够部署多个gmond吗？有个问题是：我在3台服务器上，同时部署了3个集群：Flume Cluster、Kafka Cluster、Storm Cluster，希望能够在3个页面上分别监控每个集群的情况。这就涉及一个问题：Ganglia监控的基本单元是物理服务器？还是逻辑上的一个节点？在应用层，Ganglia监控服务情况如何？&lt;/p&gt;

&lt;p&gt;（doing…）&lt;/p&gt;

&lt;h2 id=&quot;ganglia-3&quot;&gt;与Ganglia集成&lt;/h2&gt;

&lt;p&gt;（通过Ganglia来监控Flume、Kafka、Storm的运行状态，不仅仅是OS层面的，更重要的是具体应用及其组件的运行状态）&lt;/p&gt;

&lt;p&gt;（doing…）&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;回顾与总结&lt;/h2&gt;

&lt;p&gt;Ganglia用于监测分布式系统的运行状态，如何把Ganglia集群用起来？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;安装Ganglia：
    &lt;ul&gt;
      &lt;li&gt;Ganglia本身也是集群；&lt;/li&gt;
      &lt;li&gt;Ganglia集群拓扑是树状结构；&lt;/li&gt;
      &lt;li&gt;在Ganglia集群的所有node上安装Ganglia组件；&lt;/li&gt;
      &lt;li&gt;配置Ganglia集群的拓扑结构；&lt;/li&gt;
      &lt;li&gt;在某个服务器上，汇总并实时刷新监控数据；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;定制Ganglia来监控应用系统：
    &lt;ul&gt;
      &lt;li&gt;（如何定制？下面是随便说的）&lt;/li&gt;
      &lt;li&gt;在Flume、Kafka、Storm运行的服务器上都安装Ganglia的client；&lt;/li&gt;
      &lt;li&gt;Ganglia client收集应用的运行状态数据，并汇总到Ganglia的某个node；&lt;/li&gt;
      &lt;li&gt;疑问：在Ganglia集群外的某个服务器上，可以安装、使用web frontend吗？&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-12&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/yuki-lau/p/3201110.html&quot;&gt;分布式监控工具Ganglia介绍与集群部署&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/shenlan211314/article/details/7421758&quot;&gt;GangLia简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ganglia&quot;&gt;Ganglia(GitHub)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/LuaDist/libexpat&quot;&gt;libexpat(GitHub)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/martinh/libconfuse&quot;&gt;libconfuse(GitHub)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/xqj198404/article/details/9447211&quot;&gt;编译出错 recompile with -fPIC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/p/ganglia/mailman/message/19414944/&quot;&gt;Problems compiling ganglia 3.1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/p/ganglia/mailman/ganglia-general/thread/A526454B-CDD5-4B0F-9805-68A92B9459E9@crackpot.org/&quot;&gt;gmetad dead but subsys locked&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sachinsharm.wordpress.com/2013/08/17/setup-and-configure-ganglia-3-6-on-centosrhel-6-3/&quot;&gt;Setup and configure Ganglia-3.6 on CentOS/RHEL 6.3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://yaoweibin2008.blog.163.com/blog/static/11031392008763256465/&quot;&gt;Ganglia 体系结构及功能介绍&lt;/a&gt;（力荐）&lt;/li&gt;
  &lt;li&gt;Massie M L, Chun B N, Culler D E. &lt;a href=&quot;/download/ganglia/The-Ganglia-Distributed-Monitoring-System.pdf&quot;&gt;The ganglia distributed monitoring system: design, implementation, and experience&lt;/a&gt; Journal. Parallel Computing, 2004, 30(7): 817-840.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于gmond的配置信息，官方参考来源有几个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ganglia-3.6.1:ganglia monitoring core源码中ganglia.html文件有Configuration介绍；&lt;/li&gt;
  &lt;li&gt;ganglia-3.6.1:ganglia monitoring core源码中gmond/gmond.conf.html文件有详尽的说明；&lt;/li&gt;
  &lt;li&gt;安装完gmond之后，&lt;code&gt;man gmond&lt;/code&gt;可查看gmond命令的基本信息，&lt;code&gt;man gmond.conf&lt;/code&gt;可以参看gmond详细的配置信息；&lt;/li&gt;
  &lt;li&gt;sourceforge上的&lt;a href=&quot;http://sourceforge.net/p/ganglia/mailman/&quot;&gt;Ganglia Mailing Lists&lt;/a&gt;，可以直接搜索；&lt;em&gt;（Ganglia跟之前接触的Apache开源项目不同，其在sourceforge上进行讨论，因此通过Mailing Lists搜索问题，是获取信息的关键途径）&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/download/ganglia/monitoring_with_ganglia.pdf&quot;&gt;Monitoring with Ganglia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;特别推荐参考来源&lt;/strong&gt;：又有新发现，GitHub上有wiki：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ganglia/monitor-core/wiki&quot;&gt;Ganglia core(GitHub) WIKI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ganglia/ganglia-web/wiki&quot;&gt;Ganglia web(GitHub) WIKI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-13&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;今天无意间看到&lt;code&gt;YUKI小糖&lt;/code&gt;的&lt;a href=&quot;http://www.cnblogs.com/yuki-lau/p/3201110.html&quot;&gt;Ganglia集群部署文章&lt;/a&gt;，看到其也会在博文中唠叨几句；突然相当，我x，难道这是工程师的共同习性吗？莫不是因为一天到晚跟机器接触太久了，没有人说话，就通过博客来唠叨了吧~~啊哈哈~~&lt;em&gt;（平静一下心情，细想想，还是挺凄凉的…）&lt;/em&gt;&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume、Kafka、Storm小结</title>
     <link href="http://ningg.github.com/flume-kafka-storm-summary"/>
     <updated>2014-11-10T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-kafka-storm-summary</id>
     <content type="html">&lt;h2 id=&quot;flume&quot;&gt;Flume&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;可靠性和可恢复性&lt;/h3&gt;

&lt;h4 id=&quot;reliability&quot;&gt;Reliability&lt;/h4&gt;

&lt;p&gt;The events are staged in a channel on each agent. The events are then delivered to the next agent or terminal repository (like HDFS) in the flow. The events are removed from a channel only after they are stored in the channel of next agent or in the terminal repository. This is a how the single-hop message delivery semantics in Flume provide end-to-end reliability of the flow.（&lt;strong&gt;single-hop message delivery semantics&lt;/strong&gt;：Channel中的event仅在被成功处理之后，才从Channel中删掉。）&lt;/p&gt;

&lt;p&gt;Flume uses a transactional approach to guarantee the reliable delivery of the events. The sources and sinks encapsulate in a transaction the storage/retrieval, respectively, of the events placed in or provided by a transaction provided by the channel. This ensures that the set of events are reliably passed from point to point in the flow. In the case of a multi-hop flow, the sink from the previous hop and the source from the next hop both have their transactions running to ensure that the data is safely stored in the channel of the next hop.（&lt;strong&gt;multi-hop&lt;/strong&gt;：）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Flume如何保证事物操作？没看懂&lt;/p&gt;

&lt;h4 id=&quot;recoverability&quot;&gt;Recoverability&lt;/h4&gt;

&lt;p&gt;The events are staged in the channel, which manages recovery from failure. Flume supports a durable file channel which is backed by the local file system. There’s also a memory channel which simply stores the events in an in-memory queue, which is faster but any events still left in the memory channel when an agent process dies can’t be recovered.（Channel需保证崩溃后，能恢复events，具体：本地FS上保存durable file channel，另，占用一个in-memory queue，Channel进程崩溃后，能加快恢复速度；但，如果agent进程崩溃，将导致内存泄漏：无法回收这一内存）&lt;/p&gt;

&lt;h2 id=&quot;kafka&quot;&gt;Kafka&lt;/h2&gt;

&lt;p&gt;（TODO List）&lt;/p&gt;

&lt;p&gt;（Kafka集群涉及到的可扩展性和可靠性）&lt;/p&gt;

&lt;h2 id=&quot;storm&quot;&gt;Storm&lt;/h2&gt;

&lt;p&gt;（TODO List）&lt;/p&gt;

&lt;p&gt;（Storm集群相关的可扩展性和可靠性）&lt;/p&gt;

&lt;h2 id=&quot;flumekafkastorm&quot;&gt;Flume/Kafka/Storm框架性能测试&lt;/h2&gt;

&lt;p&gt;简要说一下，性能测试的目标：弄清楚整个框架的承载能力，到底能处理多达流量的数据。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;前期问题&lt;/h3&gt;

&lt;p&gt;几个搭建测试环境相关的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Flume集群与Kafka集群之间&lt;/strong&gt;，&lt;strong&gt;是否要走网络&lt;/strong&gt;？简要来说，Flume集群的最后一个聚合的Agent是否要放置到Kafka即群里？
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;RE&lt;/strong&gt;：Flume的最后一个负责聚合的Agent即使放置在某个Kafka broker上，仍然是要走网络的，因为Kafka本身就是一个集群，Flume的Kafka Sink与其他Kafka broker连接时，走的也是网络；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flume收集的数据源&lt;/strong&gt;，&lt;strong&gt;是否同时包含IP:port和日志&lt;/strong&gt;？Flume提供了从这两种source收集数据的能力，测试的时候要覆盖到。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;搭建测试环境步骤&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;实现Flume集群；
    &lt;ul&gt;
      &lt;li&gt;165、166收集数据，并以avro方式汇聚到167；&lt;/li&gt;
      &lt;li&gt;167上以logger方式将收到的message输出到stdout；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;实现Kafka集群；
    &lt;ul&gt;
      &lt;li&gt;配置每一个 Kafka broker 的 id 以及 zookeeper 集群；&lt;/li&gt;
      &lt;li&gt;将167上Flume agent的sink修改为Flume Kafka Sink；&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;测试方案列表&lt;/h3&gt;

&lt;h4 id=&quot;zookeeper&quot;&gt;zookeeper集群&lt;/h4&gt;

&lt;p&gt;当前使用的是CDH中自带的zookeeper：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;节点位置：
    &lt;ul&gt;
      &lt;li&gt;168.7.1.68:2181&lt;/li&gt;
      &lt;li&gt;168.7.1.69:2181&lt;/li&gt;
      &lt;li&gt;168.7.1.70:2181&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：zookeeper集群的基本原理，如何监控其性能？&lt;/p&gt;

&lt;h4 id=&quot;flume-1&quot;&gt;Flume集群&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;节点位置：
    &lt;ul&gt;
      &lt;li&gt;168.7.2.165: 21811&lt;/li&gt;
      &lt;li&gt;168.7.2.166: 21811&lt;/li&gt;
      &lt;li&gt;168.7.2.167: 21811（作为聚合的Agent）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;下载路径和版本信息：
    &lt;ul&gt;
      &lt;li&gt;下载路径：http://flume.apache.org/download.html&lt;/li&gt;
      &lt;li&gt;版本信息：apache-flume-1.5.0.1-bin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flume的配置文件需要考虑几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;source包含两个：Exec Source、NetCat Source、avro Source；&lt;/li&gt;
  &lt;li&gt;Sink包含：logger Sink、avro Sink、Flume Kafka Sink；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：一个问题，使用Exec Source来进行收集数据时，有一种情况，如果&lt;code&gt;tail -F&lt;/code&gt;命令意外终止了，Flume无法自动重启这一命令，原因：Flume无法确定是文件没有新增信息，还是tail命令意外终止；为解决这一问题，官网有两个建议：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用Spooling Directory Source，不过这个Source也有一个问题，他要求将文件添加到一个固定目录，这就会造成信息传递的实时性降低；&lt;/li&gt;
  &lt;li&gt;通过JDK直接与Flume集成；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;个人想法&lt;/strong&gt;：官网给出的信息很权威，不过可以到官网的JIRA上看看，其他人也遇到这个问题，应该会有其他思路。&lt;/p&gt;

&lt;h4 id=&quot;kafka-1&quot;&gt;Kafka集群&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;节点位置：
    &lt;ul&gt;
      &lt;li&gt;168.7.2.165:9091&lt;/li&gt;
      &lt;li&gt;168.7.2.166:9091&lt;/li&gt;
      &lt;li&gt;168.7.2.167:9091&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;下载路径和版本信息：
    &lt;ul&gt;
      &lt;li&gt;下载路径：http://kafka.apache.org/downloads.html&lt;/li&gt;
      &lt;li&gt;版本信息：kafka_2.9.2-0.8.1.1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：有几个疑问：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何确定当前Kafka集群中broker存活状态？&lt;/li&gt;
  &lt;li&gt;Kafka运行过程中，可定制的输出日志有哪些？输出日志位置？&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;storm-1&quot;&gt;Storm集群&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;节点位置：
    &lt;ul&gt;
      &lt;li&gt;168.7.1.68:2181&lt;/li&gt;
      &lt;li&gt;168.7.1.69:2181&lt;/li&gt;
      &lt;li&gt;168.7.1.70:2181&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：如何构建Storm集群？&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;问题汇总&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Flume、Kafka、Storm构成框架，如何监控每个模块的存活状态和性能？如何确定系统处理的瓶颈位置？&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;参考来源&lt;/h2&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1：broker config（doing）</title>
     <link href="http://ningg.github.com/kafka-broker-config"/>
     <updated>2014-11-09T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-broker-config</id>
     <content type="html">&lt;h2 id=&quot;kafka&quot;&gt;Kafka整体架构的回顾&lt;/h2&gt;

&lt;p&gt;（todo）&lt;/p&gt;

&lt;p&gt;要点：broker、producer、consumer的定位。&lt;/p&gt;

&lt;h2 id=&quot;broker-configs&quot;&gt;Broker Configs&lt;/h2&gt;

&lt;p&gt;The essential configurations are the following:（必要的配置如下）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;broker.id&lt;/li&gt;
  &lt;li&gt;log.dirs&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Topic-level configurations and defaults are discussed in &lt;a href=&quot;http://kafka.apache.org/documentation.html#topic-config&quot;&gt;more detail below&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：什么叫作&lt;strong&gt;Topic-level configuration&lt;/strong&gt;？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Property
    &lt;ul&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;broker.id
    &lt;ul&gt;
      &lt;li&gt;null(non-negative integer id)&lt;/li&gt;
      &lt;li&gt;Each broker is uniquely identified by a non-negative integer id. This id serves as the broker’s “name” and allows the broker to be moved to a different host/port without confusing consumers. You can choose any number you like so long as it is unique.（唯一标识broker，目的：当broker移动到另一个&lt;code&gt;host:port&lt;/code&gt;后，不会对consumer造成影响）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;log.dirs
    &lt;ul&gt;
      &lt;li&gt;/tmp/kafka-logs&lt;/li&gt;
      &lt;li&gt;A comma-separated list of one or more directories in which Kafka data is stored. Each new partition that is created will be placed in the directory which currently has the fewest partitions.（以逗号&lt;code&gt;,&lt;/code&gt;分割，Kafka data的存储位置；新建的partition将会被放置在当前partition数最小的目录下）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;port
    &lt;ul&gt;
      &lt;li&gt;6667&lt;/li&gt;
      &lt;li&gt;The port on which the server accepts client connections.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zookeeper.connect
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;Specifies the ZooKeeper connection string in the form &lt;code&gt;hostname:port&lt;/code&gt;, where hostname and port are the host and port for a node in your ZooKeeper cluster. To allow connecting through other ZooKeeper nodes when that host is down you can also specify multiple hosts in the form &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;ZooKeeper also allows you to add a “chroot” path which will make all kafka data for this cluster appear under a particular path. This is a way to setup multiple Kafka clusters or other applications on the same ZooKeeper cluster. To do this give a connection string in the form &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3/chroot/path&lt;/code&gt; which would put all this cluster’s data under the path &lt;code&gt;/chroot/path&lt;/code&gt;. Note that you must create this path yourself prior to starting the broker and consumers must use the same connection string.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于&lt;code&gt;zookeeper&lt;/code&gt;参数，几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka集群需要借助Zookeeper来进行管理，因此，需要设定Zookeeper集群的位置，可以只设置一个Zookeeper，也可以设置一个列表，疑问：设置一个zookeeper与一个zookeeper列表有差异吗？当只设置一个zookeeper服务器时，是否会自动获取zookeeper列表？&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;可以设置&lt;code&gt;chroot&lt;/code&gt;目录，用于存储kafka集群相关数据，这么做的原因：方便同一个zookeeper集群，管理多个应用（例如，kafka集群）；但需要在启动broker之前，提前创建&lt;code&gt;chroot&lt;/code&gt;目录，并且consumer需要使用相同的&lt;code&gt;zookeeper.connect&lt;/code&gt;作为connection string。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;message.max.bytes
    &lt;ul&gt;
      &lt;li&gt;1000000&lt;/li&gt;
      &lt;li&gt;The maximum size of a message that the server can receive. It is important that this property be in sync with the maximum fetch size your consumers use or else an unruly producer will be able to publish messages too large for consumers to consume.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;num.network.threads
    &lt;ul&gt;
      &lt;li&gt;3&lt;/li&gt;
      &lt;li&gt;The number of network threads that the server uses for handling network requests. You probably don’t need to change this.（处理网络请求所设定的线程数，通常不用调整这个参数）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;num.io.threads
    &lt;ul&gt;
      &lt;li&gt;8&lt;/li&gt;
      &lt;li&gt;The number of I/O threads that the server uses for executing requests. You should have at least as many threads as you have disks.（server执行request时，启动的I/O线程数目，建议与磁盘个数相同）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;background.threads
    &lt;ul&gt;
      &lt;li&gt;4&lt;/li&gt;
      &lt;li&gt;The number of threads to use for various background processing tasks such as file deletion. You should not need to change this.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queued.max.requests
    &lt;ul&gt;
      &lt;li&gt;500&lt;/li&gt;
      &lt;li&gt;The number of requests that can be queued up for processing by the I/O threads before the network threads stop reading in new requests.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;host.name
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;Hostname of broker. If this is set, it will only bind to this address. If this is not set, it will bind to all interfaces, and publish one to ZK.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;advertised.host.name
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;If this is set this is the hostname that will be given out to producers, consumers, and other brokers to connect to.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;advertised.port
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;The port to give out to producers, consumers, and other brokers to use in establishing connections. This only needs to be set if this port is different from the port the server should bind to.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;socket.send.buffer.bytes	100 * 1024	The SO_SNDBUFF buffer the server prefers for socket connections.&lt;/li&gt;
  &lt;li&gt;socket.receive.buffer.bytes	100 * 1024	The SO_RCVBUFF buffer the server prefers for socket connections.&lt;/li&gt;
  &lt;li&gt;socket.request.max.bytes	100 * 1024 * 1024	The maximum request size the server will allow. This prevents the server from running out of memory and should be smaller than the Java heap size.&lt;/li&gt;
  &lt;li&gt;num.partitions	1	The default number of partitions per topic if a partition count isn’t given at topic creation time.&lt;/li&gt;
  &lt;li&gt;log.segment.bytes	1024 * 1024 * 1024	The log for a topic partition is stored as a directory of segment files. This setting controls the size to which a segment file will grow before a new segment is rolled over in the log. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.roll.hours	24 * 7	This setting will force Kafka to roll a new log segment even if the log.segment.bytes size has not been reached. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.cleanup.policy	delete	This can take either the value delete or compact. If delete is set, log segments will be deleted when they reach the size or time limits set. If compact is set log compaction will be used to clean out obsolete records. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.retention.{minutes,hours}	7 days	The amount of time to keep a log segment before it is deleted, i.e. the default data retention window for all topics. Note that if both log.retention.minutes and log.retention.bytes are both set we delete a segment when either limit is exceeded. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.retention.bytes	-1	The amount of data to retain in the log for each topic-partitions. Note that this is the limit per-partition so multiply by the number of partitions to get the total data retained for the topic. Also note that if both log.retention.hours and log.retention.bytes are both set we delete a segment when either limit is exceeded. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.retention.check.interval.ms	5 minutes	The period with which we check whether any log segment is eligible for deletion to meet the retention policies.&lt;/li&gt;
  &lt;li&gt;log.cleaner.enable	false	This configuration must be set to true for log compaction to run.&lt;/li&gt;
  &lt;li&gt;log.cleaner.threads	1	The number of threads to use for cleaning logs in log compaction.&lt;/li&gt;
  &lt;li&gt;log.cleaner.io.max.bytes.per.second	None	The maximum amount of I/O the log cleaner can do while performing log compaction. This setting allows setting a limit for the cleaner to avoid impacting live request serving.&lt;/li&gt;
  &lt;li&gt;log.cleaner.dedupe.buffer.size	500&lt;em&gt;1024&lt;/em&gt;1024	The size of the buffer the log cleaner uses for indexing and deduplicating logs during cleaning. Larger is better provided you have sufficient memory.&lt;/li&gt;
  &lt;li&gt;log.cleaner.io.buffer.size	512*1024	The size of the I/O chunk used during log cleaning. You probably don’t need to change this.&lt;/li&gt;
  &lt;li&gt;log.cleaner.io.buffer.load.factor	0.9	The load factor of the hash table used in log cleaning. You probably don’t need to change this.&lt;/li&gt;
  &lt;li&gt;log.cleaner.backoff.ms	15000	The interval between checks to see if any logs need cleaning.&lt;/li&gt;
  &lt;li&gt;log.cleaner.min.cleanable.ratio	0.5	This configuration controls how frequently the log compactor will attempt to clean the log (assuming log compaction is enabled). By default we will avoid cleaning a log where more than 50% of the log has been compacted. This ratio bounds the maximum space wasted in the log by duplicates (at 50% at most 50% of the log could be duplicates). A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.cleaner.delete.retention.ms	1 day	The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan). This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.index.size.max.bytes	10 * 1024 * 1024	The maximum size in bytes we allow for the offset index for each log segment. Note that we will always pre-allocate a sparse file with this much space and shrink it down when the log rolls. If the index fills up we will roll a new log segment even if we haven’t reached the log.segment.bytes limit. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.index.interval.bytes	4096	The byte interval at which we add an entry to the offset index. When executing a fetch request the server must do a linear scan for up to this many bytes to find the correct position in the log to begin and end the fetch. So setting this value to be larger will mean larger index files (and a bit more memory usage) but less scanning. However the server will never add more than one index entry per log append (even if more than log.index.interval worth of messages are appended). In general you probably don’t need to mess with this value.&lt;/li&gt;
  &lt;li&gt;log.flush.interval.messages	None	The number of messages written to a log partition before we force an fsync on the log. Setting this lower will sync data to disk more often but will have a major impact on performance. We generally recommend that people make use of replication for durability rather than depending on single-server fsync, however this setting can be used to be extra certain.&lt;/li&gt;
  &lt;li&gt;log.flush.scheduler.interval.ms	3000	The frequency in ms that the log flusher checks whether any log is eligible to be flushed to disk.&lt;/li&gt;
  &lt;li&gt;log.flush.interval.ms	None	The maximum time between fsync calls on the log. If used in conjuction with log.flush.interval.messages the log will be flushed when either criteria is met.&lt;/li&gt;
  &lt;li&gt;log.delete.delay.ms	60000	The period of time we hold log files around after they are removed from the in-memory segment index. This period of time allows any in-progress reads to complete uninterrupted without locking. You generally don’t need to change this.&lt;/li&gt;
  &lt;li&gt;log.flush.offset.checkpoint.interval.ms	60000	The frequency with which we checkpoint the last flush point for logs for recovery. You should not need to change this.&lt;/li&gt;
  &lt;li&gt;auto.create.topics.enable	true	Enable auto creation of topic on the server. If this is set to true then attempts to produce, consume, or fetch metadata for a non-existent topic will automatically create it with the default replication factor and number of partitions.&lt;/li&gt;
  &lt;li&gt;controller.socket.timeout.ms	30000	The socket timeout for commands from the partition management controller to the replicas.&lt;/li&gt;
  &lt;li&gt;controller.message.queue.size	10	The buffer size for controller-to-broker-channels&lt;/li&gt;
  &lt;li&gt;default.replication.factor	1	The default replication factor for automatically created topics.&lt;/li&gt;
  &lt;li&gt;replica.lag.time.max.ms	10000	If a follower hasn’t sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.&lt;/li&gt;
  &lt;li&gt;replica.lag.max.messages	4000	If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.&lt;/li&gt;
  &lt;li&gt;replica.socket.timeout.ms	30 * 1000	The socket timeout for network requests to the leader for replicating data.&lt;/li&gt;
  &lt;li&gt;replica.socket.receive.buffer.bytes	64 * 1024	The socket receive buffer for network requests to the leader for replicating data.&lt;/li&gt;
  &lt;li&gt;replica.fetch.max.bytes	1024 * 1024	The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.&lt;/li&gt;
  &lt;li&gt;replica.fetch.wait.max.ms	500	The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.&lt;/li&gt;
  &lt;li&gt;replica.fetch.min.bytes	1	Minimum bytes expected for each fetch response for the fetch requests from the replica to the leader. If not enough bytes, wait up to replica.fetch.wait.max.ms for this many bytes to arrive.&lt;/li&gt;
  &lt;li&gt;num.replica.fetchers	1	&lt;/li&gt;
  &lt;li&gt;Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;replica.high.watermark.checkpoint.interval.ms	5000	The frequency with which each replica saves its high watermark to disk to handle recovery.&lt;/li&gt;
  &lt;li&gt;fetch.purgatory.purge.interval.requests	10000	The purge interval (in number of requests) of the fetch request purgatory.&lt;/li&gt;
  &lt;li&gt;producer.purgatory.purge.interval.requests	10000	The purge interval (in number of requests) of the producer request purgatory.&lt;/li&gt;
  &lt;li&gt;zookeeper.session.timeout.ms	6000	ZooKeeper session timeout. If the server fails to heartbeat to ZooKeeper within this period of time it is considered dead. If you set this too low the server may be falsely considered dead; if you set it too high it may take too long to recognize a truly dead server.&lt;/li&gt;
  &lt;li&gt;zookeeper.connection.timeout.ms	6000	The maximum amount of time that the client waits to establish a connection to zookeeper.&lt;/li&gt;
  &lt;li&gt;zookeeper.sync.time.ms	2000	How far a ZK follower can be behind a ZK leader.&lt;/li&gt;
  &lt;li&gt;controlled.shutdown.enable	false	Enable controlled shutdown of the broker. If enabled, the broker will move all leaders on it to some other brokers before shutting itself down. This reduces the unavailability window during shutdown.&lt;/li&gt;
  &lt;li&gt;controlled.shutdown.max.retries	3	Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.&lt;/li&gt;
  &lt;li&gt;controlled.shutdown.retry.backoff.ms	5000	Backoff time between shutdown retries.&lt;/li&gt;
  &lt;li&gt;auto.leader.rebalance.enable	false	If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the “preferred” replica for each partition if it is available.&lt;/li&gt;
  &lt;li&gt;leader.imbalance.per.broker.percentage	10	The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.&lt;/li&gt;
  &lt;li&gt;leader.imbalance.check.interval.seconds	300	The frequency with which to check for leader imbalance.&lt;/li&gt;
  &lt;li&gt;offset.metadata.max.bytes	1024	The maximum amount of metadata to allow clients to save with their offsets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details about broker configuration can be found in the scala class &lt;code&gt;kafka.server.KafkaConfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Topic-level configuration&lt;/p&gt;

&lt;p&gt;Configurations pertinent to topics have both a global default as well an optional per-topic override. If no per-topic configuration is given the global default is used. The override can be set at topic creation time by giving one or more –config options. This example creates a topic named my-topic with a custom max message size and flush rate:
 &amp;gt; bin/kafka-topics.sh –zookeeper localhost:2181 –create –topic my-topic –partitions 1 
        –replication-factor 1 –config max.message.bytes=64000 –config flush.messages=1
Overrides can also be changed or set later using the alter topic command. This example updates the max message size for my-topic:
 &amp;gt; bin/kafka-topics.sh –zookeeper localhost:2181 –alter –topic my-topic 
    –config max.message.bytes=128000
To remove an override you can do
 &amp;gt; bin/kafka-topics.sh –zookeeper localhost:2181 –alter –topic my-topic 
    –deleteConfig max.message.bytes
The following are the topic-level configurations. The server’s default configuration for this property is given under the Server Default Property heading, setting this default in the server config allows you to change the default given to topics that have no override specified.
Property	Default	Server Default Property	Description
cleanup.policy	delete	log.cleanup.policy	A string that is either “delete” or “compact”. This string designates the retention policy to use on old log segments. The default policy (“delete”) will discard old segments when their retention time or size limit has been reached. The “compact” setting will enable log compaction on the topic.
delete.retention.ms	86400000 (24 hours)	log.cleaner.delete.retention.ms	The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan).
flush.messages	None	log.flush.interval.messages	This setting allows specifying an interval at which we will force an fsync of data written to the log. For example if this was set to 1 we would fsync after every message; if it were 5 we would fsync after every five messages. In general we recommend you not set this and use replication for durability and allow the operating system’s background flush capabilities as it is more efficient. This setting can be overridden on a per-topic basis (see the per-topic configuration section).
flush.ms	None	log.flush.interval.ms	This setting allows specifying a time interval at which we will force an fsync of data written to the log. For example if this was set to 1000 we would fsync after 1000 ms had passed. In general we recommend you not set this and use replication for durability and allow the operating system’s background flush capabilities as it is more efficient.
index.interval.bytes	4096	log.index.interval.bytes	This setting controls how frequently Kafka adds an index entry to it’s offset index. The default setting ensures that we index a message roughly every 4096 bytes. More indexing allows reads to jump closer to the exact position in the log but makes the index larger. You probably don’t need to change this.
max.message.bytes	1,000,000	message.max.bytes	This is largest message size Kafka will allow to be appended to this topic. Note that if you increase this size you must also increase your consumer’s fetch size so they can fetch messages this large.
min.cleanable.dirty.ratio	0.5	log.cleaner.min.cleanable.ratio	This configuration controls how frequently the log compactor will attempt to clean the log (assuming log compaction is enabled). By default we will avoid cleaning a log where more than 50% of the log has been compacted. This ratio bounds the maximum space wasted in the log by duplicates (at 50% at most 50% of the log could be duplicates). A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log.
retention.bytes	None	log.retention.bytes	This configuration controls the maximum size a log can grow to before we will discard old log segments to free up space if we are using the “delete” retention policy. By default there is no size limit only a time limit.
retention.ms	7 days	log.retention.minutes	This configuration controls the maximum time we will retain a log before we will discard old log segments to free up space if we are using the “delete” retention policy. This represents an SLA on how soon consumers must read their data.
segment.bytes	1 GB	log.segment.bytes	This configuration controls the segment file size for the log. Retention and cleaning is always done a file at a time so a larger segment size means fewer files but less granular control over retention.
segment.index.bytes	10 MB	log.index.size.max.bytes	This configuration controls the size of the index that maps offsets to file positions. We preallocate this index file and shrink it only after log rolls. You generally should not need to change this setting.
segment.ms	7 days	log.roll.hours	This configuration controls the period of time after which Kafka will force the log to roll even if the segment file isn’t full to ensure that retention can delete or compact old data.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1：Consumer API and Consumer Configs</title>
     <link href="http://ningg.github.com/kafka-api-consumer"/>
     <updated>2014-11-09T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-api-consumer</id>
     <content type="html">&lt;h2 id=&quot;consumer-api&quot;&gt;Consumer API&lt;/h2&gt;

&lt;p&gt;如何从Kafka中读取数据？三种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High Level Consumer API；&lt;/li&gt;
  &lt;li&gt;Simple Consumer API；&lt;/li&gt;
  &lt;li&gt;Kafka Hadoop Consumer API；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;high-level-consumer-api&quot;&gt;High Level Consumer API&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;class Consumer {
  /**
   *  Create a ConsumerConnector
   *
   *  @param config  at the minimum, need to specify the groupid of the consumer and the zookeeper
   *                 connection string zookeeper.connect.
   */
  public static kafka.javaapi.consumer.ConsumerConnector createJavaConsumerConnector(ConsumerConfig config);
}

/**
 *  V: type of the message
 *  K: type of the optional key assciated with the message
 */
public interface kafka.javaapi.consumer.ConsumerConnector {
  /**
   *  Create a list of message streams of type T for each topic.
   *
   *  @param topicCountMap  a map of (topic, #streams) pair
   *  @param decoder a decoder that converts from Message to T
   *  @return a map of (topic, list of  KafkaStream) pairs.
   *          The number of items in the list is #streams. Each stream supports
   *          an iterator over message/metadata pairs.
   */
  public &amp;lt;K,V&amp;gt; Map&amp;lt;String, List&amp;lt;KafkaStream&amp;lt;K,V&amp;gt;&amp;gt;&amp;gt;
	createMessageStreams(Map&amp;lt;String, Integer&amp;gt; topicCountMap, Decoder&amp;lt;K&amp;gt; keyDecoder, Decoder&amp;lt;V&amp;gt; valueDecoder);

  /**
   *  Create a list of message streams of type T for each topic, using the default decoder.
   */
  public Map&amp;lt;String, List&amp;lt;KafkaStream&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; createMessageStreams(Map&amp;lt;String, Integer&amp;gt; topicCountMap);

  /**
   *  Create a list of message streams for topics matching a wildcard.
   *
   *  @param topicFilter a TopicFilter that specifies which topics to
   *                    subscribe to (encapsulates a whitelist or a blacklist).
   *  @param numStreams the number of message streams to return.
   *  @param keyDecoder a decoder that decodes the message key
   *  @param valueDecoder a decoder that decodes the message itself
   *  @return a list of KafkaStream. Each stream supports an
   *          iterator over its MessageAndMetadata elements.
   */
  public &amp;lt;K,V&amp;gt; List&amp;lt;KafkaStream&amp;lt;K,V&amp;gt;&amp;gt;
	createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams, Decoder&amp;lt;K&amp;gt; keyDecoder, Decoder&amp;lt;V&amp;gt; valueDecoder);

  /**
   *  Create a list of message streams for topics matching a wildcard, using the default decoder.
   */
  public List&amp;lt;KafkaStream&amp;lt;byte[], byte[]&amp;gt;&amp;gt; createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams);

  /**
   *  Create a list of message streams for topics matching a wildcard, using the default decoder, with one stream.
   */
  public List&amp;lt;KafkaStream&amp;lt;byte[], byte[]&amp;gt;&amp;gt; createMessageStreamsByFilter(TopicFilter topicFilter);

  /**
   *  Commit the offsets of all topic/partitions connected by this connector.
   */
  public void commitOffsets();

  /**
   *  Shut down the connector
   **/
  public void shutdown();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can follow &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example&quot;&gt;this example&lt;/a&gt; to learn how to use the high level consumer api.&lt;/p&gt;

&lt;h3 id=&quot;simple-consumer-api&quot;&gt;Simple Consumer API&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;class kafka.javaapi.consumer.SimpleConsumer {
/**
*  Fetch a set of messages from a topic.
*
*  @param request specifies the topic name, topic partition, starting byte offset, maximum bytes to be fetched.
*  @return a set of fetched messages
*/
public FetchResponse fetch(kafka.javaapi.FetchRequest request);

/**
*  Fetch metadata for a sequence of topics.
*
*  @param request specifies the versionId, clientId, sequence of topics.
*  @return metadata for each topic in the request.
*/
public kafka.javaapi.TopicMetadataResponse send(kafka.javaapi.TopicMetadataRequest request);

/**
*  Get a list of valid offsets (up to maxSize) before the given time.
*
*  @param request a [[kafka.javaapi.OffsetRequest]] object.
*  @return a [[kafka.javaapi.OffsetResponse]] object.
*/
public kafak.javaapi.OffsetResponse getOffsetsBefore(OffsetRequest request);

/**
* Close the SimpleConsumer.
*/
public void close();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For most applications, the high level consumer Api is good enough. Some applications want features not exposed to the high level consumer yet (e.g., set initial offset when restarting the consumer). They can instead use our low level SimpleConsumer Api. The logic will be a bit more complicated and you can follow the example in &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;kafka-hadoop-consumer-api&quot;&gt;Kafka Hadoop Consumer API&lt;/h3&gt;

&lt;p&gt;Providing a horizontally scalable solution for aggregating and loading data into Hadoop was one of our basic use cases. To support this use case, we provide a Hadoop-based consumer which spawns off many map tasks to pull data from the Kafka cluster in parallel. This provides extremely fast pull-based Hadoop data load capabilities (we were able to fully saturate the network with only a handful of Kafka servers).&lt;/p&gt;

&lt;p&gt;Usage information on the hadoop consumer can be found &lt;a href=&quot;https://github.com/linkedin/camus/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;consumer-configs&quot;&gt;Consumer Configs&lt;/h2&gt;

&lt;p&gt;The essential consumer configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;group.id&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下文将详细介绍这些参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Property
    &lt;ul&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;group.id
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;A string that uniquely identifies the group of consumer processes to which this consumer belongs. By setting the same group id multiple processes indicate that they are all part of the same consumer group.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：consumer group？复习一下，为什么有这个？本质：Kafka中一条message，发送到哪些地方呢？一种是群发给Consumer，一种是只发送给某一个满足条件的Consumer；同时message要求在同一个Consumer中保证message的处理顺序，在满足这一功能需求的情况下，同时为了改善性能，增加了一个概念：consumer group，同一个group下可以包含多个consumer，每次group接收到message，就实例化其内部的一个consumer，如果一个partition中的message就发送给一个group，则顺序处理；否则就是并发处理。疑问：一个consumer group中只包含一个consumer就能够实现串行顺序处理了，为什么还要放置多个consumer？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;zookeeper.connect
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;Specifies the ZooKeeper connection string in the form &lt;code&gt;hostname:port&lt;/code&gt; where host and port are the host and port of a ZooKeeper server. To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down you can also specify multiple hosts in the form &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;The server may also have a ZooKeeper &lt;code&gt;chroot&lt;/code&gt; path as part of it’s ZooKeeper connection string which puts its data under some path in the global ZooKeeper namespace. If so the consumer should use the same chroot path in its connection string. For example to give a chroot path of &lt;code&gt;/chroot/path&lt;/code&gt; you would give the connection string as &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3/chroot/path&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：在设置&lt;code&gt;zookeeper.connect&lt;/code&gt;时，可以设置zookeeper的&lt;code&gt;chroot&lt;/code&gt;，&lt;code&gt;chroot&lt;/code&gt;的含义：改变元数据在global Zookeeper namespace中的存储位置；一旦修改了&lt;code&gt;chroot&lt;/code&gt;，就需要在链接Zookeeper时，也用上&lt;code&gt;chroot&lt;/code&gt;，具体形式：&lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3/chroot/path&lt;/code&gt;。（当前理解，前面的&lt;code&gt;/chroot/path&lt;/code&gt;对&lt;code&gt;hostname1:port1&lt;/code&gt;也是有效的）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;consumer.id
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;Generated automatically if not set.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;socket.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;30 * 1000&lt;/li&gt;
      &lt;li&gt;The socket timeout for network requests. The actual timeout set will be &lt;code&gt;max.fetch.wait&lt;/code&gt; + &lt;code&gt;socket.timeout.ms&lt;/code&gt;.（等待message的时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;socket.receive.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;64 * 1024&lt;/li&gt;
      &lt;li&gt;The socket receive buffer for network requests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fetch.message.max.bytes
    &lt;ul&gt;
      &lt;li&gt;1024 * 1024&lt;/li&gt;
      &lt;li&gt;The number of byes of messages to attempt to fetch for each topic-partition in each fetch request. These bytes will be read into memory for each partition, so this helps control the memory used by the consumer. The fetch request size must be at least as large as the maximum message size the server allows or else it is possible for the producer to send messages larger than the consumer can fetch.（consumer单次请求messages时，最大字节数；通常要求&lt;code&gt;fetch.message.max.bytes&lt;/code&gt;至少为maximum message size）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto.commit.enable
    &lt;ul&gt;
      &lt;li&gt;true&lt;/li&gt;
      &lt;li&gt;If true, periodically commit to ZooKeeper the offset of messages already fetched by the consumer. This committed offset will be used when the process fails as the position from which the new consumer will begin.（默认&lt;code&gt;true&lt;/code&gt;，表示当Consumer成功获取message后，向zookeeper发送message的offset表示commit；committed offset的作用：当consumer process失败后，新的consumer从这一offset，重新开始处理）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto.commit.interval.ms
    &lt;ul&gt;
      &lt;li&gt;60 * 1000&lt;/li&gt;
      &lt;li&gt;The frequency in ms that the consumer offsets are committed to zookeeper.（Consumer多长时间提交一次offset）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：难道不是consumer每成功fetch一个message，就commit一次offset？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;queued.max.message.chunks
    &lt;ul&gt;
      &lt;li&gt;10&lt;/li&gt;
      &lt;li&gt;Max number of message chunks buffered for consumption. Each chunk can be up to &lt;code&gt;fetch.message.max.bytes&lt;/code&gt;.（允许缓存的message chunk的个数）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message chunk什么意思？有用吗？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rebalance.max.retries
    &lt;ul&gt;
      &lt;li&gt;4&lt;/li&gt;
      &lt;li&gt;When a new consumer joins a consumer group the set of consumers attempt to “rebalance” the load to assign partitions to each consumer. If the set of consumers changes while this assignment is taking place the rebalance will fail and retry. This setting controls the maximum number of attempts before giving up.（新的consumer加入到consumer group后，真个consumer group承担的所有partition会进行再分配，如果分配过程中，这些consumer set右发生变化，则会尝试重新执行，此参数，表示尝试的次数。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fetch.min.bytes
    &lt;ul&gt;
      &lt;li&gt;1&lt;/li&gt;
      &lt;li&gt;The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request.（server向fetch request返回的最小字节数，如果data不足，则会等待累积足够的数据之后，再进行响应。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fetch.wait.max.ms
    &lt;ul&gt;
      &lt;li&gt;100&lt;/li&gt;
      &lt;li&gt;The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy &lt;code&gt;fetch.min.bytes&lt;/code&gt;（设定了&lt;code&gt;fetch.min.bytes&lt;/code&gt;，如果没有足够数据，则，最长等待时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;rebalance.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;2000&lt;/li&gt;
      &lt;li&gt;Backoff time between retries during rebalance.（reblalance时，不同的retry之间的退避时长，即，两次retry之间的间隔时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;refresh.leader.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;200&lt;/li&gt;
      &lt;li&gt;Backoff time to wait before trying to determine the leader of a partition that has just lost its leader.（失去leader后，再次请求leader的退避时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto.offset.reset
    &lt;ul&gt;
      &lt;li&gt;largest&lt;/li&gt;
      &lt;li&gt;What to do when there is no initial offset in ZooKeeper or if an offset is out of range:（当Zookeeper中没有initial offset或者offset超出范围时，如何自动设置offset？）
        &lt;ul&gt;
          &lt;li&gt;smallest : automatically reset the offset to the smallest offset&lt;/li&gt;
          &lt;li&gt;largest : automatically reset the offset to the largest offset&lt;/li&gt;
          &lt;li&gt;anything else: throw exception to the consumer&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;consumer.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;-1&lt;/li&gt;
      &lt;li&gt;Throw a timeout exception to the consumer if no message is available for consumption after the specified interval（如果没有consumer可用的message，等待多长时间后，系统抛出异常）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;client.id
    &lt;ul&gt;
      &lt;li&gt;group id value&lt;/li&gt;
      &lt;li&gt;The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.（用于追踪调用过程）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zookeeper.session.timeout.ms
 	* 6000
    &lt;ul&gt;
      &lt;li&gt;ZooKeeper session timeout. If the consumer fails to heartbeat to ZooKeeper for this period of time it is considered dead and a rebalance will occur.（一段时间内consumer如果失去与Zookeeper之间的心跳，就认定consumer已经丢失，会在consumer group内进行rebalance）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：参数&lt;code&gt;zookeeper.session.timeout.ms&lt;/code&gt;与参数&lt;code&gt;auto.commit.interval.ms&lt;/code&gt;之间的关系，前者衡量的是heartbeat，而后者负责的是offset commit。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;zookeeper.connection.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;6000&lt;/li&gt;
      &lt;li&gt;The max time that the client waits while establishing a connection to zookeeper.（client与zookeeper保持连接的时间，超过这一时间，自动释放）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zookeeper.sync.time.ms
 	* 2000
    &lt;ul&gt;
      &lt;li&gt;How far a ZK follower can be behind a ZK leader（&lt;strong&gt;什么意思&lt;/strong&gt;？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details about consumer configuration can be found in the scala class &lt;code&gt;kafka.consumer.ConsumerConfig&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example&quot;&gt;Consumer Group Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example&quot;&gt;Simple Consumer Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/linkedin/camus/&quot;&gt;Kafka Hadoop Consumer Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1：Producer API and Producer Configs</title>
     <link href="http://ningg.github.com/kafka-api-producer"/>
     <updated>2014-11-07T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-api-producer</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近在做Flume与Kafka的整合，其中用到了一个工程：&lt;a href=&quot;https://github.com/thilinamb/flume-ng-kafka-sink&quot;&gt;flume-ng-kafka-sink&lt;/a&gt;，本质上就是Flume的一个插件：Kafka sink。遇到一个问题：Kafka sink通过设置kafak broker的&lt;code&gt;ip:port&lt;/code&gt;来寻找broker，那就有一个问题，如果设置连接的kafka broker 宕掉了，flume的数据是不是就送不出去了？&lt;/p&gt;

&lt;h2 id=&quot;producer&quot;&gt;Producer&lt;/h2&gt;

&lt;p&gt;开始介绍Producer之前，说个小问题：上面&lt;strong&gt;背景&lt;/strong&gt;中一直在说Flume的Sink：Kafka Sink，那与Kafka producer什么关系呢？为什么这次标题是&lt;strong&gt;Kafka Producer&lt;/strong&gt;，而丝毫未提&lt;strong&gt;Flume Sink&lt;/strong&gt;？这个问题很好，说明读者在思考，大概说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flume架构中有Sink，是用来将Flume收集到的数据送出去的；Flume下的Kafka Sink插件，在Flume看来，就是个Sink；&lt;/li&gt;
  &lt;li&gt;Kafka架构中有Producer，是用来向Kafka broker中送入数据的；Flume下的Kafka Sink插件，在Kafka看来，就是个Producer；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此次，主要站在Kafka的角度来看一个Producer可以进行的配置。&lt;/p&gt;

&lt;h3 id=&quot;kafka-producer-api&quot;&gt;Kafka Producer API&lt;/h3&gt;

&lt;p&gt;下面是&lt;code&gt;kafka.javaapi.producer.Producer&lt;/code&gt;类的java API，实际上这个类是scala编写的，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 *  V: type of the message
 *  K: type of the optional key associated with the message
 */
 
class kafka.javaapi.producer.Producer&amp;lt;K,V&amp;gt; {

  public Producer(ProducerConfig config);

  /**
   * Sends the data to a single topic, partitioned by key, using either the
   * synchronous or the asynchronous producer
   * @param message the producer data object that encapsulates the topic, key and message data
   */
  public void send(KeyedMessage&amp;lt;K,V&amp;gt; message);

  /**
   * Use this API to send data to multiple topics
   * @param messages list of producer data objects that encapsulate the topic, key and message data
   */
  public void send(List&amp;lt;KeyedMessage&amp;lt;K,V&amp;gt;&amp;gt; messages);

  /**
   * Close API to close the producer pool connections to all Kafka brokers.
   */
  public void close();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体如何使用上述Producer API，可参考&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example&quot;&gt;0.8.0 Producer Example&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;producer-example&quot;&gt;0.8.0 Producer Example&lt;/h3&gt;

&lt;p&gt;研究要深入，上面提到的&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example&quot;&gt;0.8.0 Producer Example&lt;/a&gt;，下面简要介绍一下。&lt;/p&gt;

&lt;p&gt;The Producer class is used to create new messages for a specific Topic and optional Partition.&lt;/p&gt;

&lt;p&gt;If using Java you need to include a few packages for the Producer and supporting classes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step in your code is to define properties for how the Producer finds the cluster, serializes the messages and if appropriate directs the message to a specific Partition.&lt;/p&gt;

&lt;p&gt;代码本质体现的是逻辑，首先需要确定几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Producer如何找到Kafka Cluster；&lt;/li&gt;
  &lt;li&gt;message传输的格式；（serialize，序列化）&lt;/li&gt;
  &lt;li&gt;如何将message存入指定的Partition中；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties are defined in the standard Java Properties object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Properties props = new Properties();
 
props.put(&quot;metadata.broker.list&quot;, &quot;broker1:9092,broker2:9092&quot;);
props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);
props.put(&quot;partitioner.class&quot;, &quot;example.producer.SimplePartitioner&quot;);
props.put(&quot;request.required.acks&quot;, &quot;1&quot;);
 
ProducerConfig config = new ProducerConfig(props);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first property, “metadata.broker.list” defines where the Producer can find a one or more Brokers to determine the Leader for each topic. This does not need to be the full set of Brokers in your cluster but should include at least two in case the first Broker is not available. No need to worry about figuring out which Broker is the leader for the topic (and partition), the Producer knows how to connect to the Broker and ask for the meta data then connect to the correct Broker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一项参数&lt;/strong&gt;&lt;code&gt;metadata.broker.list&lt;/code&gt;，用于配置可用的broker列表，可以只配置一个broker，不过建议最好至少配置2个broker，这样即使有一个broker宕机了，另一个也能及时接替工作；这些broker中，也不用指定不同topic的leader，因为Producer会主动连接Broker并且请求到meta数据，然后连接到topic的leader。&lt;/p&gt;

&lt;p&gt;The second property “serializer.class” defines what Serializer to use when preparing the message for transmission to the Broker. In our example we use a simple String encoder provided as part of Kafka. Note that the encoder must accept the same type as defined in the KeyedMessage object in the next step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二项参数&lt;/strong&gt;&lt;code&gt;serializer.class&lt;/code&gt;，设定了将message从Producer发送到Broker的序列化方式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：“Note that the encoder must accept the same type as defined in the KeyedMessage object in the next step.” 什么含义？ &lt;code&gt;KeyedMessage&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;It is possible to change the Serializer for the Key (see below) of the message by defining “key.serializer.class” appropriately. By default it is set to the same value as “serializer.class”.&lt;/p&gt;

&lt;p&gt;参数&lt;code&gt;key.serializer.class&lt;/code&gt;用于设置key序列化的方法，key将在序列化之后，与message一同从Producer发送到Broker；&lt;code&gt;key.serializer.class&lt;/code&gt;的默认值与&lt;code&gt;serializer.class&lt;/code&gt;相同。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Kafka是按照key进行partition的，每个message绑定的key也是需要传输到broker的，传输过程中也需要进行序列化，&lt;/p&gt;

&lt;p&gt;The third property  “partitioner.class” defines what class to use to determine which Partition in the Topic the message is to be sent to. This is optional, but for any non-trivial implementation you are going to want to implement a partitioning scheme. More about the implementation of this class later. If you include a value for the key but haven’t defined a partitioner.class Kafka will use the default partitioner. If the key is null, then the Producer will assign the message to a random Partition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第三项参数&lt;/strong&gt;&lt;code&gt;partitioner.class&lt;/code&gt;用于设定message与Partition的映射关系，简单来说，每个message都发送给broker的某个对应的Topic，但message真正存储对应的是Topic下的partition，那么，参数&lt;code&gt;partitioner.class&lt;/code&gt;就是用于设定message–partition之间映射关系的。&lt;/p&gt;

&lt;p&gt;The last property “request.required.acks” tells Kafka that you want your Producer to require an acknowledgement from the Broker that the message was received. Without this setting the Producer will ‘fire and forget’ possibly leading to data loss. Additional information can be found &lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后一项参数&lt;/strong&gt;&lt;code&gt;request.required.acks&lt;/code&gt;，设定Broker在接收到message之后，是否返回一个确认信息（ack）。如果没有这个信息，那么很有可能&lt;code&gt;fire and forget&lt;/code&gt;并且丢失数据。更多Kafka的相关配置信息，参考：&lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;Kafka Configuration&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：有个问题，即使Broker在接收到message之后，返回了ack信息，那Producer提供了重发机制吗？还是Producer只是进行登记？&lt;/p&gt;

&lt;p&gt;Next you define the Producer object itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Producer&amp;lt;String, String&amp;gt; producer = new Producer&amp;lt;String, String&amp;gt;(config);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the Producer is a Java Generic and you need to tell it the type of two parameters. The first is the type of the Partition key, the second the type of the message. In this example they are both Strings, which also matches to what we defined in the Properties above.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Producer&lt;/code&gt;是一个Java Generic（泛型），需要输入两个参数，&lt;code&gt;&amp;lt;String, String&amp;gt;&lt;/code&gt;，第一个参数是Partition key的类型，第二个是message的类型&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：java中Generic的用法、注意事项有哪些？上面说的Partition key，到底指什么？是properties中的属性和属性值吗？不是的，查看源代码，Partition key就是按照key进行partition的key。&lt;/p&gt;

&lt;p&gt;Now build your message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Random rnd = new Random();
long runtime = new Date().getTime();
String ip = “192.168.2.” + rnd.nextInt(255);
String msg = runtime + “,www.example.com,” + ip;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example we are faking a message for a website visit by IP address. First part of the comma-separated message is the timestamp of the event, the second is the website and the third is the IP address of the requester. We use the Java Random class here to make the last octet of the IP vary so we can see how Partitioning works.（上面&lt;code&gt;msg&lt;/code&gt;中是伪造的一个网站访问记录）&lt;/p&gt;

&lt;p&gt;Finally write the message to the Broker:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KeyedMessage&amp;lt;String, String&amp;gt; data = new KeyedMessage&amp;lt;String, String&amp;gt;(&quot;page_visits&quot;, ip, msg);
 
producer.send(data);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The “page_visits” is the Topic to write to. Here we are passing the IP as the partition key. Note that if you do not include a key, even if you’ve defined a partitioner class, Kafka will assign the message to a random partition.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;KeyedMessage&amp;lt;String, String&amp;gt;(topic, message)&lt;/code&gt;或者&lt;code&gt;KeyedMessage&amp;lt;String, String&amp;gt;(topic, key, message)&lt;/code&gt;，如果没输入key，那么即使设定了&lt;code&gt;partitioner.class&lt;/code&gt;也不会对message分发到相应partition的，原因很简单，因为真的没有key。&lt;/p&gt;

&lt;p&gt;Full Source:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import java.util.*;
 
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
 
public class TestProducer {
	public static void main(String[] args) {
		long events = Long.parseLong(args[0]);
		Random rnd = new Random();
 
		Properties props = new Properties();
		props.put(&quot;metadata.broker.list&quot;, &quot;broker1:9092,broker2:9092 &quot;);
		props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);
		props.put(&quot;partitioner.class&quot;, &quot;example.producer.SimplePartitioner&quot;);
		props.put(&quot;request.required.acks&quot;, &quot;1&quot;);
 
		ProducerConfig config = new ProducerConfig(props);
 
		Producer&amp;lt;String, String&amp;gt; producer = new Producer&amp;lt;String, String&amp;gt;(config);
 
		for (long nEvents = 0; nEvents &amp;lt; events; nEvents++) { 
			   long runtime = new Date().getTime();  
			   String ip = “192.168.2.” + rnd.nextInt(255); 
			   String msg = runtime + “,www.example.com,” + ip; 
			   KeyedMessage&amp;lt;String, String&amp;gt; data = new KeyedMessage&amp;lt;String, String&amp;gt;(&quot;page_visits&quot;, ip, msg);
			   producer.send(data);
		}
		producer.close();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Partitioning Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import kafka.producer.Partitioner;
import kafka.utils.VerifiableProperties;
 
public class SimplePartitioner implements Partitioner {
	public SimplePartitioner (VerifiableProperties props) {
 
	}
 
	public int partition(Object key, int a_numPartitions) {
		int partition = 0;
		String stringKey = (String) key;
		int offset = stringKey.lastIndexOf(&#39;.&#39;);
		if (offset &amp;gt; 0) {
		   partition = Integer.parseInt( stringKey.substring(offset+1)) % a_numPartitions;
		}
	   return partition;
  }
 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The logic takes the key, which we expect to be the IP address, finds the last octet and does a modulo operation on the number of partitions defined within Kafka for the topic. The benefit of this partitioning logic is all web visits from the same source IP end up in the same Partition. Of course so do other IPs, but your consumer logic will need to know how to handle that.
（将有时间顺序的message放到同一个partition中）&lt;/p&gt;

&lt;p&gt;Before running this, make sure you have created the Topic page_visits. From the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-create-topic.sh --topic page_visits --replica 3 --zookeeper localhost:2181 --partition 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you include a &lt;code&gt;--partition&lt;/code&gt; option so you create more than one.
（要使用&lt;code&gt;--partition&lt;/code&gt;来创建多个partition，否则可能只有一个）&lt;/p&gt;

&lt;p&gt;Now compile and run your Producer and data will be written to Kafka.&lt;/p&gt;

&lt;p&gt;To confirm you have data, use the command line tool to see what was written:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic page_visits --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用Maven进行Producer开发时，需要添加的POM配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;kafka_2.9.2&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;0.8.1.1&amp;lt;/version&amp;gt;
  &amp;lt;scope&amp;gt;compile&amp;lt;/scope&amp;gt;
  &amp;lt;exclusions&amp;gt;
	&amp;lt;exclusion&amp;gt;
	  &amp;lt;artifactId&amp;gt;jmxri&amp;lt;/artifactId&amp;gt;
	  &amp;lt;groupId&amp;gt;com.sun.jmx&amp;lt;/groupId&amp;gt;
	&amp;lt;/exclusion&amp;gt;
	&amp;lt;exclusion&amp;gt;
	  &amp;lt;artifactId&amp;gt;jms&amp;lt;/artifactId&amp;gt;
	  &amp;lt;groupId&amp;gt;javax.jms&amp;lt;/groupId&amp;gt;
	&amp;lt;/exclusion&amp;gt;
	&amp;lt;exclusion&amp;gt;
	  &amp;lt;artifactId&amp;gt;jmxtools&amp;lt;/artifactId&amp;gt;
	  &amp;lt;groupId&amp;gt;com.sun.jdmk&amp;lt;/groupId&amp;gt;
	&amp;lt;/exclusion&amp;gt;
  &amp;lt;/exclusions&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-1&quot;&gt;几个情况&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;思考1&lt;/strong&gt;：Kafka 0.7.2版本中，直接在Producer中配置Zookeeper，Producer通过Zookeeper来获知Broker的位置，简单来说，应用与Kafka之间是解耦的，可以在不修改Producer信息的情况下，动态增减Broker。&lt;/p&gt;

&lt;p&gt;当前，通过&lt;code&gt;metadata.broker.list&lt;/code&gt;来设置broker的列表，有几个问题，稍微梳理一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果只在&lt;code&gt;metadata.broker.list&lt;/code&gt;中配置一个broker，那么Producer能够识别出其他broker吗？&lt;/li&gt;
  &lt;li&gt;如果能够识别出未配置的broker，那么，只配置一个broker不就行了吗？&lt;/li&gt;
  &lt;li&gt;如果不能识别出未配置的broker，那Kafka集群中动态增加了broker，岂不是需要重新启动flume？（因为&lt;code&gt;metadata.broker.list&lt;/code&gt;实际上是flume的配置，要更新这一参数配置，就需要重启flume）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;思考2&lt;/strong&gt;：如果network interrupt，producer会如何动作？记录log？还是抛出异常？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;思考3&lt;/strong&gt;：如果某一个flume挂了，能否能自动重启？&lt;/p&gt;

&lt;h2 id=&quot;producer-1&quot;&gt;Producer配置的详细参数&lt;/h2&gt;

&lt;p&gt;针对Kafka 0.8.1版本，这一部分介绍的Producer配置信息，主要参考两个地方：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;Kafka Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html#producerconfigs&quot;&gt;Kafka Producer Config&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Essential configuration properties for the producer include:（Producer必须的参数有几个，如下）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list，broker列表；&lt;/li&gt;
  &lt;li&gt;request.required.acks，broker收到producer发来的message后，是否ack？&lt;/li&gt;
  &lt;li&gt;producer.type，这个什么滴干活？&lt;/li&gt;
  &lt;li&gt;serializer.class，message从producer发往broker的过程中，需要序列化；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体参数如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Property
    &lt;ul&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metadata.broker.list
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas). The socket connections for sending the actual data will be established based on the broker information returned in the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a subset of brokers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;request.required.acks
    &lt;ul&gt;
      &lt;li&gt;0	&lt;/li&gt;
      &lt;li&gt;This value controls when a produce request is considered completed. Specifically, how many other brokers must have committed the data to their log and acknowledged this to the leader? Typical values are&lt;/li&gt;
      &lt;li&gt;0, which means that the producer never waits for an acknowledgement from the broker (the same behavior as 0.7). This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails).（不等待ack信息）&lt;/li&gt;
      &lt;li&gt;1, which means that the producer gets an acknowledgement after the leader replica has received the data. This option provides better durability as the client waits until the server acknowledges the request as successful (only messages that were written to the now-dead leader but not yet replicated will be lost).（leader完成数据写入）&lt;/li&gt;
      &lt;li&gt;-1, which means that the producer gets an acknowledgement after all in-sync replicas have received the data. This option provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.（所有replica都完成数据写入）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;request.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;10000&lt;/li&gt;
      &lt;li&gt;The amount of time the broker will wait trying to meet the &lt;code&gt;request.required.acks&lt;/code&gt; requirement before sending back an error to the client.（broker收到producer发来的message后，如果需要返回ack信息，那这个参数设定了broker返回ack信息的时间限制，如果超过这个时间，则broker向producer返回一个error信息）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;producer.type	
    &lt;ul&gt;
      &lt;li&gt;sync&lt;/li&gt;
      &lt;li&gt;This parameter specifies whether the messages are sent asynchronously in a background thread. Valid values are (1) &lt;code&gt;async&lt;/code&gt; for asynchronous send and (2) &lt;code&gt;sync&lt;/code&gt; for synchronous send. By setting the producer to async we allow batching together of requests (which is great for throughput) but open the possibility of a failure of the client machine dropping unsent data.（producer发送message的方式：同步、异步；设置为异步时，producer处理的吞吐量会提升，但可能丢失数据）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;serializer.class	
    &lt;ul&gt;
      &lt;li&gt;kafka.serializer.DefaultEncoder&lt;/li&gt;
      &lt;li&gt;The serializer class for messages. The default encoder takes a &lt;code&gt;byte[]&lt;/code&gt; and returns the same &lt;code&gt;byte[]&lt;/code&gt;.（message的序列化方法，默认是&lt;code&gt;byte[]&lt;/code&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;key.serializer.class
    &lt;ul&gt;
      &lt;li&gt;defaults to the same as for messages if nothing is given.&lt;/li&gt;
      &lt;li&gt;The serializer class for keys .&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;partitioner.class
    &lt;ul&gt;
      &lt;li&gt;kafka.producer.DefaultPartitioner（The default partitioner is based on the hash of the key.）&lt;/li&gt;
      &lt;li&gt;The partitioner class for partitioning messages amongst sub-topics. （将message放入哪个partition中）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;compression.codec	
    &lt;ul&gt;
      &lt;li&gt;none	&lt;/li&gt;
      &lt;li&gt;This parameter allows you to specify the compression codec for all data generated by this producer. Valid values are “none”, “gzip” and “snappy”.（producer向broker发送的信息，是否进行压缩，包含：key、message信息。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;compressed.topics	
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;This parameter allows you to set whether compression should be turned on for particular topics. If the compression codec is anything other than NoCompressionCodec, enable compression only for specified topics if any. If the list of compressed topics is empty, then enable the specified compression codec for all topics. If the compression codec is NoCompressionCodec, compression is disabled for all topics（当开启&lt;code&gt;compression.codec&lt;/code&gt;时，通过设置&lt;code&gt;compressed.topics&lt;/code&gt;，设置只针对某些特定的topic进行压缩，默认，对所有的topic都进行压缩）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;message.send.max.retries
    &lt;ul&gt;
      &lt;li&gt;3	&lt;/li&gt;
      &lt;li&gt;This property will cause the producer to automatically retry a failed send request. This property specifies the number of retries when such failures occur. Note that setting a non-zero value here can lead to duplicates in the case of network errors that cause a message to be sent but the acknowledgement to be lost.（当producer发送message失败后，尝试重新发送的次数；&lt;strong&gt;特别说明&lt;/strong&gt;：如果message发送成功，但broker返回的ack信息丢失时，会有message重发，即，此处有消息重复发送）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;retry.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;100&lt;/li&gt;
      &lt;li&gt;Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.（producer在进行重新发送message之前，都会refresh metadata，主要目标，查看是否更新了topic的leader；因为leader election需要一段时间，因此，在refresh metadata之前，需要等待一段时间，&lt;code&gt;retry.backoff.ms&lt;/code&gt;参数设置的就是等待的时间，等待选出新的topic leader）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;topic.metadata.refresh.interval.ms	
    &lt;ul&gt;
      &lt;li&gt;600 * 1000	&lt;/li&gt;
      &lt;li&gt;The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). （正常情况，多长时间刷新一次broker metadata，即，刷新间隔）&lt;/li&gt;
      &lt;li&gt;If you set this to a &lt;code&gt;negative value&lt;/code&gt;, metadata will only get refreshed on failure. （&lt;code&gt;&amp;lt;0&lt;/code&gt;时，仅当发送message失败时，才刷新）&lt;/li&gt;
      &lt;li&gt;If you set this to zero, the metadata will get refreshed after each message sent (not recommended). （&lt;code&gt;0&lt;/code&gt;，每次发送完message之后，都刷新，&lt;strong&gt;不推荐&lt;/strong&gt;）&lt;/li&gt;
      &lt;li&gt;Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed（&lt;strong&gt;重要提示&lt;/strong&gt;：无论设置刷新间隔为多少，具体刷新metadata都发生在producer发送message之后，因此，如果一直没有message发送，就不会有metadata刷新）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queue.buffering.max.ms
    &lt;ul&gt;
      &lt;li&gt;5000&lt;/li&gt;
      &lt;li&gt;Maximum time to buffer data when using &lt;code&gt;async&lt;/code&gt; mode. For example a setting of 100 will try to batch together 100ms of messages to send at once. This will improve throughput but adds message delivery latency due to the buffering.（当使用&lt;code&gt;producer.type&lt;/code&gt;为async模式时，这一参数才有用，含义：一时间为单位，将这一时间单位内的message一起发送给broker，这样有利于提高throughput，但会增加时延。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queue.buffering.max.messages
    &lt;ul&gt;
      &lt;li&gt;10000&lt;/li&gt;
      &lt;li&gt;The maximum number of unsent messages that can be queued up the producer when using async mode before either the producer must be blocked or data must be dropped.（当&lt;code&gt;producer.type&lt;/code&gt;使用async时，producer能够缓存的unsent message的数量，如果超过这一数量，producer就会blocked？message就会被丢弃？具体什么情况？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queue.enqueue.timeout.ms	
    &lt;ul&gt;
      &lt;li&gt;-1	&lt;/li&gt;
      &lt;li&gt;The amount of time to block before dropping messages when running in async mode and the buffer has reached queue.buffering.max.messages. （当&lt;code&gt;queue.buffering.max.message&lt;/code&gt;设定的值已经触顶，等待多久block，之后就开始丢弃message）&lt;/li&gt;
      &lt;li&gt;If set to 0 events will be enqueued immediately or dropped if the queue is full (the producer send call will never block). &lt;/li&gt;
      &lt;li&gt;If set to -1 the producer will block indefinitely and never willingly drop a send.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch.num.messages
    &lt;ul&gt;
      &lt;li&gt;200&lt;/li&gt;
      &lt;li&gt;The number of messages to send in one batch when using async mode. The producer will wait until either this number of messages are ready to send or queue.buffer.max.ms is reached.（在&lt;code&gt;async&lt;/code&gt;模式下，当message数量达到&lt;code&gt;batch.num.messages&lt;/code&gt;时，或者，当等待时间达到&lt;code&gt;queue.buffer.max.ms&lt;/code&gt;时，producer都会发送一次缓存的message）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;send.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;100 * 1024&lt;/li&gt;
      &lt;li&gt;Socket write buffer size（socket写缓存的大小）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;client.id	
    &lt;ul&gt;
      &lt;li&gt;””&lt;/li&gt;
      &lt;li&gt;The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.（用户自己定义的producer标识，会伴随发送的message一起发送，用于追踪message的来源）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details about producer configuration can be found in the scala class &lt;code&gt;kafka.producer.ProducerConfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：几个新的理解：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list：本质是从一些broker中请求metadata（topic、partition、replicas），而真正的socket链接，是根据收到的metadata来进行的；因此，可以只配置一部分的broker，或者说只配置部分VIP broker，必要时，探查此深层的原因；每一个broker上都保存了整个Kafka cluster的完整metadata吗？&lt;/li&gt;
  &lt;li&gt;producer.type：sync、async，当设置为async时，能够提升吞吐量，但是会丢失数据？丢失，不能重发吗？&lt;/li&gt;
  &lt;li&gt;request.required.acks：设置是否需要broker在完成数据写入后，向producer返回ack信息；一个问题：如果broker上数据写入失败，那，producer会进行重发吗？有没有类似的机制？&lt;/li&gt;
  &lt;li&gt;queue.enqueue.timeout.me：其中说明的producer block什么含义？还会继续缓存未发送的message吗？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：突然想到一个问题，记录一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Server上，有进程监听port后，在服务器上无法再启动一个进程来监听这一port；&lt;/li&gt;
  &lt;li&gt;在远端通过telnet能够与server上这一port建立连接，并且，多个client都能与server上这一port建立连接；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个问题，我不知到深层原因，归根结底是对socket建立的底层原因不清晰。&lt;/p&gt;

&lt;h2 id=&quot;new-producer-configs&quot;&gt;New Producer Configs（补充）&lt;/h2&gt;

&lt;p&gt;下面是今后Kafka Producer会采用的新的配置参数，当前，可以有一个基本的了解。&lt;/p&gt;

&lt;p&gt;We are working on a replacement for our existing producer. The code is available in trunk now and can be considered beta quality. Below is the configuration for the new producer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name
    &lt;ul&gt;
      &lt;li&gt;Type&lt;/li&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Importance&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;bootstrap.servers
    &lt;ul&gt;
      &lt;li&gt;list&lt;/li&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. Data will be load balanced over all servers irrespective of which servers are specified here for bootstrapping—this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form host1:port1,host2:port2,…. Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down). If no server in this list is available sending data will fail until on becomes available.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;acks
    &lt;ul&gt;
      &lt;li&gt;string&lt;/li&gt;
      &lt;li&gt;1&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are common:&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;acks=0&lt;/code&gt; If set to zero then the producer will not wait for any acknowledgment from the server at all. The record will be immediately added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won’t generally know of any failures). The offset given back for each record will always be set to -1.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;acks=1&lt;/code&gt; This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.（leader将message写入local log后，直接返回ack信息；如果leader，返回ack信息后，leader宕机了，那其他follwer上并没有这条message，将导致message丢失）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;acks=all&lt;/code&gt; This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.&lt;/li&gt;
      &lt;li&gt;Other settings such as acks=2 are also possible, and will require the given number of acknowledgements but this is generally less useful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;buffer.memory
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;33554432&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will either block or throw an exception based on the preference specified by &lt;code&gt;block.on.buffer.full&lt;/code&gt;.（用于存储未发送出去的message，当producer接收到的message速度大于发送message速度时，producer will block，或者抛出异常）&lt;/li&gt;
      &lt;li&gt;This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests.（&lt;strong&gt;什么含义&lt;/strong&gt;？需仔细琢磨）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;compression.type
    &lt;ul&gt;
      &lt;li&gt;string&lt;/li&gt;
      &lt;li&gt;none&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, or snappy. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;retries
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;0&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries will potentially change the ordering of records because if two records are sent to a single partition, and the first fails and is retried but the second succeeds, then the second record may appear first.（设定，message尝试重发的次数；这个重发机制，可能会改变message之间的相互顺序）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch.size
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;16384&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes.（将发送到同一partition的多条message集中起来发送，构成一个batch）&lt;/li&gt;
      &lt;li&gt;No attempt will be made to batch records larger than this size.&lt;/li&gt;
      &lt;li&gt;Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent.（发送给broker的request包含多个batch？每一个batch对应一个partition）&lt;/li&gt;
      &lt;li&gt;A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;client.id
    &lt;ul&gt;
      &lt;li&gt;string&lt;/li&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included with the request. The application can set any string it wants as this has no functional purpose other than in logging and metrics.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;linger.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;0&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay—that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle’s algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will ‘linger’ for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absense of load.（当producer收到一个message后，不直接发送出去，而是，等待&lt;code&gt;linger.ms&lt;/code&gt;时间，目的：相同partition的多个message同时发送。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;max.request.size
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;1048576&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The maximum size of a request. This is also effectively a cap on the maximum record size. Note that the server has its own cap on record size which may be different from this. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.（限制单个request的大小）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;receive.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;32768&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The size of the TCP receive buffer to use when reading data（上层读取TCP数据时，一次读取的缓冲单元？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;send.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;131072&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The size of the TCP send buffer to use when sending data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;timeout.ms
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;30000&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The configuration controls the maximum amount of time the server will wait for acknowledgments from followers to meet the acknowledgment requirements the producer has specified with the acks configuration. If the requested number of acknowledgments are not met when the timeout elapses an error will be returned. This timeout is measured on the server side and does not include the network latency of the request.（server等待follower返回ack信息的时间，这个时间是指server端的时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;block.on.buffer.full
    &lt;ul&gt;
      &lt;li&gt;boolean&lt;/li&gt;
      &lt;li&gt;true&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;When our memory buffer is exhausted we must either stop accepting new records (block) or throw errors. By default this setting is true and we block, however in some scenarios blocking is not desirable and it is better to immediately give an error. Setting this to false will accomplish that: the producer will throw a BufferExhaustedException if a recrord is sent and the buffer space is full.（默认&lt;code&gt;true&lt;/code&gt;，当memory buffer中内容满了之后，producer不再接收新的message；如果设置为&lt;code&gt;false&lt;/code&gt;，则当memory buffer中内容满了之后，producer会直接抛出异常&lt;code&gt;BufferExhaustedException&lt;/code&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metadata.fetch.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;60000&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The first time data is sent to a topic we must fetch metadata about that topic to know which servers host the topic’s partitions. This configuration controls the maximum amount of time we will block waiting for the metadata fetch to succeed before throwing an exception back to the client.（当第一次向topic中传入数据时，需要从server请求metadata，参数&lt;code&gt;metadata.fetch.timeout.ms&lt;/code&gt;设定了发送metadata请求后，producer等待的时间，如果超时，则抛出异常。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metadata.max.age.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;300000&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The period of time in milliseconds after which we force a refresh of metadata even if we haven’t seen any partition leadership changes to proactively discover any new brokers or partitions.（定期请求metadata的时常）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metric.reporters
    &lt;ul&gt;
      &lt;li&gt;list&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;A list of classes to use as metrics reporters. Implementing the &lt;code&gt;MetricReporter&lt;/code&gt; interface allows plugging in classes that will be notified of new metric creation. The &lt;code&gt;JmxReporter&lt;/code&gt; is always included to register &lt;code&gt;JMX&lt;/code&gt; statistics.（&lt;strong&gt;什么含义&lt;/strong&gt;？生成测试报告？测试什么？为什么要测试？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metrics.num.samples
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;2&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The number of samples maintained to compute metrics.（计算指标时，保留的samples的个数）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metrics.sample.window.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;30000&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The metrics system maintains a configurable number of samples over a fixed window size. This configuration controls the size of the window. For example we might maintain two samples each measured over a 30 second period. When a window expires we erase and overwrite the oldest window.（选出sample的window大小）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;reconnect.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;10&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The amount of time to wait before attempting to reconnect to a given host when a connection fails. This avoids a scenario where the client repeatedly attempts to connect to a host in a tight loop.（当与一个host断开连接后，等待多长时间，再去进行连接，避免过于频繁的无效连接）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;retry.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;100&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The amount of time to wait before attempting to retry a failed produce request to a given topic partition. This avoids repeated sending-and-failing in a tight loop.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;metrics&lt;/code&gt;的含义？为什么有这个？干什么的？&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/thilinamb/flume-ng-kafka-sink&quot;&gt;flume-ng-kafka-sink&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example&quot;&gt;0.8.0 Producer Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;Kafka Configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;人是有差异的，特别是视野上的差异，有些东西，如果一个人没有见识过，同时想象力也不行，或者说胆小如鼠不敢想象，这样的人脑袋不行、胆子也不行，要疏远；另，信任是金子，别人对我的绝对信任，我对别人的绝对信任，都是很难建立的，要如同珍惜脑袋一样，珍惜这些信任。（注：绝对信任：无论做什么事，都相信是在做一件值得做的事，无论速度、计划怎样，都是信任，甚至当有流言蜚语产生时，都能力排众议对其信任。这种信任，大都是建立在对人格的熟知上。）&lt;/p&gt;

&lt;p&gt;整理东西，突然想到：做一件事，怎样才能做成？做事情需要几个条件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;做事的方向对不对？&lt;/li&gt;
  &lt;li&gt;做事的人脑袋是否灵光？&lt;/li&gt;
  &lt;li&gt;做事的人，是否投入了足够的时间？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对上面的几点，大概说一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;做事的方向对不对&lt;/strong&gt;，只要针对做的事情，当前能够达到基本一致，方向基本确定，而不是一团乱麻，那就可以开始做下去了，而在后期的过程中，可能会涉及到多次调整、迭代，这些都是可以预见的；&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;做事的人脑袋是否灵光&lt;/strong&gt;，事情是由人来做的，做的人脑袋行不行？基础理论、基本操作技能，基本的世界观：劳动创造价值，获得报酬；还是，跟着大牛有肉吃？（这本质是希望拿牛人的劳动价值，换取自己的报酬）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;做事的人时间投入&lt;/strong&gt;，天资尚可的人就行了，团队高低档次的人都需要，但是，有一点，如果不投入时间，或者时间很少，那又如何保证产出？特别是针对以前就没有涉足的领域，需要不畏艰险、持续的投入时间，才能有所理解、有所深入；脑袋还可以，但做事不投入充足时间的人，不是傻就是懒。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今天突然想起一件事，几年前，跟某位好友一起走路，无意间说起坚韧这种性格，我就问道：如果要在午门城墙上打一个洞，如何才能做到？谈到锲而不舍，如果一个人没有这种精神，那遇到困难的事情，就难办了；后来又说起，今后工作的打算，我们基本达成一致：精挑细选公司，一旦入门后，就当自己是公司的创始人，然后，返老还童，恢复到20多岁年轻小伙儿的年纪 ，只不过，返老还童的代价是放弃对于公司的所有权、职务等，以这种心态去工作，重塑自己的公司、再造辉煌，可以说想象还是比较大胆的；基于这种定位，每次做事，都是创始人心态，全力做好。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm 0.9.2：Trident</title>
     <link href="http://ningg.github.com/storm-trident"/>
     <updated>2014-11-06T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-trident</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Trident-tutorial.html&quot;&gt;Trident tutorial&lt;/a&gt;，本文采用&lt;code&gt;英文原文+中文批注&lt;/code&gt;方式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Trident is a high-level abstraction for doing realtime computing on top of Storm. It allows you to seamlessly intermix high throughput (millions of messages per second), stateful stream processing with low latency distributed querying. If you’re familiar with high level batch processing tools like &lt;code&gt;Pig&lt;/code&gt; or &lt;code&gt;Cascading&lt;/code&gt;, the concepts of &lt;code&gt;Trident&lt;/code&gt; will be very familiar – Trident has &lt;strong&gt;joins&lt;/strong&gt;, &lt;strong&gt;aggregations&lt;/strong&gt;, &lt;strong&gt;grouping&lt;/strong&gt;, &lt;strong&gt;functions&lt;/strong&gt;, and &lt;strong&gt;filters&lt;/strong&gt;. In addition to these, Trident adds primitives for doing stateful, incremental processing on top of any database or persistence store. Trident has consistent, exactly-once semantics, so it is easy to reason about Trident topologies.&lt;/p&gt;

&lt;p&gt;Trident，说几点：&lt;/p&gt;

&lt;p&gt;能够支撑高吞吐量、有状态stream的低延迟分布式查询；
与批量处理工具类似&lt;code&gt;Pig&lt;/code&gt;、&lt;code&gt;Cascading&lt;/code&gt;，Trident包含：&lt;strong&gt;joins&lt;/strong&gt;, &lt;strong&gt;aggregations&lt;/strong&gt;, &lt;strong&gt;grouping&lt;/strong&gt;, &lt;strong&gt;functions&lt;/strong&gt;, and &lt;strong&gt;filters&lt;/strong&gt;一系列操作；
Trident能够支撑stateful、incremental processing；
Trident支撑consistent、exactly-once semantics；&lt;/p&gt;

&lt;h2 id=&quot;illustrative-example&quot;&gt;Illustrative example&lt;/h2&gt;

&lt;p&gt;Let’s look at an illustrative example of Trident. This example will do two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute streaming word count from an input stream of sentences&lt;/li&gt;
  &lt;li&gt;Implement queries to get the sum of the counts for a list of words&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例子做两件事：&lt;/p&gt;

&lt;p&gt;统计一个输入stream中的word；
查询一组输入word的统计结果；&lt;/p&gt;

&lt;p&gt;For the purposes of illustration, this example will read an infinite stream of sentences from the following source:
（从如下source中读取数据，进行处理）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
FixedBatchSpout spout = new FixedBatchSpout(new Fields(&quot;sentence&quot;), 3, 
					new Values(&quot;the cow jumped over the moon&quot;), 
					new Values(&quot;the man went to the store and bought some candy&quot;), 
					new Values(&quot;four score and seven years ago&quot;), 
					new Values(&quot;how many apples can you eat&quot;));

spout.setCycle(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：spout中&lt;code&gt;setCycle()&lt;/code&gt;的含义。&lt;/p&gt;

&lt;p&gt;This spout cycles through that set of sentences over and over to produce the sentence stream. Here’s the code to do the streaming word count part of the computation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
TridentTopology topology = new TridentTopology(); 
TridentState wordCounts = topology.newStream(&quot;spout1&quot;, spout)
				.each(new Fields(&quot;sentence&quot;), new Split(), new Fields(&quot;word&quot;))
				.groupBy(new Fields(&quot;word&quot;))
				.persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;))
				.parallelismHint(6);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s go through the code line by line. First a TridentTopology object is created, which exposes the interface for constructing Trident computations. TridentTopology has a method called newStream that creates a new stream of data in the topology reading from an input source. In this case, the input source is just the FixedBatchSpout defined from before. Input sources can also be queue brokers like Kestrel or Kafka. Trident keeps track of a small amount of state for each input source (metadata about what it has consumed) in Zookeeper, and the “spout1” string here specifies the node in Zookeeper where Trident should keep that metadata.&lt;/p&gt;

&lt;p&gt;几个点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TridentTopology类：提供构造Trident computation的接口；&lt;/li&gt;
  &lt;li&gt;newStream()方法：从一个Spout中读取数据，构造Stream；
    &lt;ul&gt;
      &lt;li&gt;上述例子中，使用FixedBatchSpout作为数据源(Source);&lt;/li&gt;
      &lt;li&gt;上面Spout也可使用queue broker代替，例，Kestrel、Kafka；&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;newStream(&quot;spout1&quot;, spout)&lt;/code&gt;，其中&lt;code&gt;spout1&lt;/code&gt;标识了在zookeeper中当前spout存储&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Trident在Zookeeper中记录了每个Spout的处理状态数据（metadata：Spout中数据处理进展）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trident processes the stream as small batches of tuples. For example, the incoming stream of sentences might be divided into batches like so:&lt;/p&gt;

&lt;p&gt;Trident将Stream中的tuple分割为一些小的batch，按照batch来进行处理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-trident/batched-stream.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Generally the size of those small batches will be on the order of thousands or millions of tuples, depending on your incoming throughput. 
（通常，将相邻的tuple组合成一个batch，通过调整tuple的输入Storm顺序，可实现将类似的tuple放入相同的batch）&lt;/p&gt;

&lt;p&gt;Trident provides a fully fledged batch processing API to process those small batches. The API is very similar to what you see in high level abstractions for Hadoop like Pig or Cascading: you can do group by’s, joins, aggregations, run functions, run filters, and so on. Of course, processing each small batch in isolation isn’t that interesting, so Trident provides functions for doing aggregations across batches and persistently storing those aggregations – whether in memory, in Memcached, in Cassandra, or some other store. Finally, Trident has first-class functions for querying sources of realtime state. That state could be updated by Trident (like in this example), or it could be an independent source of state.
（Trident提供了处理batch的API，这些API与Pig、Cascading的处理类似）&lt;/p&gt;

&lt;p&gt;Back to the example, the spout emits a stream containing one field called “sentence”. The next line of the topology definition applies the Split function to each tuple in the stream, taking the “sentence” field and splitting it into words. Each sentence tuple creates potentially many word tuples – for instance, the sentence “the cow jumped over the moon” creates six “word” tuples. Here’s the definition of &lt;code&gt;Split&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：通过&lt;code&gt;TridentTopology#newStream()&lt;/code&gt;将Spout中tuple构造为stream时，也可以进行干预（定制），即，将Spout中读取的原始tuple转换为其他格式的tuple。&lt;/p&gt;

&lt;p&gt;// java 
public class Split extends BaseFunction { &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void execute(TridentTuple tuple, TridentCollector collector) { 
	String sentence = tuple.getString(0);
	for(String word: sentence.split(&quot; &quot;)) { 
		collector.emit(new Values(word)); 
	} 
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;As you can see, it’s really simple. It simply grabs the sentence, splits it on whitespace, and emits a tuple for each word.&lt;/p&gt;

&lt;p&gt;The rest of the topology computes word count and keeps the results persistently stored. First the stream is grouped by the “word” field. Then, each group is persistently aggregated using the Count aggregator. The &lt;code&gt;persistentAggregate&lt;/code&gt; function knows how to store and update the results of the aggregation in a source of state. In this example, the word counts are kept in memory, but this can be trivially swapped to use Memcached, Cassandra, or any other persistent store. Swapping this topology to store counts in Memcached is as simple as replacing the persistentAggregate line with this (using &lt;a href=&quot;https://github.com/nathanmarz/trident-memcached&quot;&gt;trident-memcached&lt;/a&gt;), where the “serverLocations” is a list of host/ports for the Memcached cluster:
（&lt;code&gt;persistentAggregate&lt;/code&gt;函数，负责存储和更新aggregation result，其中&lt;code&gt;MemoryMapState.Factory()&lt;/code&gt;表示利用内存存储，也可以使用Memcached、Cassandra以及其他的持久化数据库）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// MemcachedState.transactional()
.persistentAggregate(MemcachedState.transactional(serverLocations), new Count(), new Fields(&quot;count&quot;)) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The values stored by persistentAggregate represents the aggregation of all batches ever emitted by the stream.&lt;/p&gt;

&lt;p&gt;One of the cool things about Trident is that it has fully fault-tolerant, exactly-once processing semantics. This makes it easy to reason about your realtime processing. Trident persists state in a way so that if failures occur and retries are necessary, it won’t perform multiple updates to the database for the same source data.
（Trident最迷人的一点：fully fault-tolerant、exactly-once processing semantics）&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;persistentAggregate&lt;/code&gt; method transforms a &lt;code&gt;Stream&lt;/code&gt; into a &lt;code&gt;TridentState object&lt;/code&gt;. In this case the TridentState object represents all the word counts. We will use this TridentState object to implement the distributed query portion of the computation.
（&lt;code&gt;persistentAggregate&lt;/code&gt;方法将Stream转换为TridentState Object，其用于实现distributed query）&lt;/p&gt;

&lt;p&gt;The next part of the topology implements a low latency distributed query on the word counts. The query takes as input a whitespace separated list of words and return the sum of the counts for those words. These queries are executed just like normal RPC calls, except they are parallelized in the background. Here’s an example of how you might invoke one of these queries:
（topology的next part实现了一个low latency、distributed query：接收输入的word，并返回这些word的统计次数。实际上，这些query看起来就是normal RPC calls，只是他们在背后是并行执行的。下面是调用query的方法）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
DRPCClient client = new DRPCClient(&quot;drpc.server.location&quot;, 3772); 
// prints the JSON-encoded result, e.g.: &quot;[[5078]]&quot;
System.out.println(client.execute(&quot;words&quot;, &quot;cat dog the man&quot;); 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, it looks just like a regular remote procedure call (RPC), except it’s executing in parallel across a Storm cluster. The latency for small queries like this are typically around 10ms. More intense DRPC queries can take longer of course, although the latency largely depends on how many resources you have allocated for the computation.&lt;/p&gt;

&lt;p&gt;The implementation of the distributed query portion of the topology looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
topology.newDRPCStream(&quot;words&quot;)
	   .each(new Fields(&quot;args&quot;), new Split(), new Fields(&quot;word&quot;)) 
	   .groupBy(new Fields(&quot;word&quot;)) 
	   .stateQuery(wordCounts, new Fields(&quot;word&quot;), new MapGet(), new Fields(&quot;count&quot;)) 
	   .each(new Fields(&quot;count&quot;), new FilterNull()) 
	   .aggregate(new Fields(&quot;count&quot;), new Sum(), new Fields(&quot;sum&quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same TridentTopology object is used to create the DRPC stream, and the function is named “words”. The function name corresponds to the function name given in the first argument of execute when using a DRPCClient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：DRPC，distributed RPC？&lt;/p&gt;

&lt;p&gt;Each DRPC request is treated as its own little batch processing job that takes as input a single tuple representing the request. The tuple contains one field called “args” that contains the argument provided by the client. In this case, the argument is a whitespace separated list of words.&lt;/p&gt;

&lt;p&gt;First, the Split function is used to split the arguments for the request into its constituent words. The stream is grouped by “word”, and the stateQuery operator is used to query the TridentState object that the first part of the topology generated. stateQuery takes in a source of state – in this case, the word counts computed by the other portion of the topology – and a function for querying that state. In this case, the MapGet function is invoked, which gets the count for each word. Since the DRPC stream is grouped the exact same way as the TridentState was (by the “word” field), each word query is routed to the exact partition of the TridentState object that manages updates for that word.&lt;/p&gt;

&lt;p&gt;Next, words that didn’t have a count are filtered out via the FilterNull filter and the counts are summed using the Sum aggregator to get the result. Then, Trident automatically sends the result back to the waiting client.&lt;/p&gt;

&lt;p&gt;Trident is intelligent about how it executes a topology to maximize performance. There’s two interesting things happening automatically in this topology:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Operations that read from or write to state (like persistentAggregate and stateQuery) automatically batch operations to that state. So if there’s 20 updates that need to be made to the database for the current batch of processing, rather than do 20 read requests and 20 writes requests to the database, Trident will automatically batch up the reads and writes, doing only 1 read request and 1 write request (and in many cases, you can use caching in your State implementation to eliminate the read request). So you get the best of both words of convenience – being able to express your computation in terms of what should be done with each tuple – and performance.&lt;/li&gt;
  &lt;li&gt;Trident aggregators are heavily optimized. Rather than transfer all tuples for a group to the same machine and then run the aggregator, Trident will do partial aggregations when possible before sending tuples over the network. For example, the Count aggregator computes the count on each partition, sends the partial count over the network, and then sums together all the partial counts to get the total count. This technique is similar to the use of combiners in MapReduce.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s look at another example of Trident.&lt;/p&gt;

&lt;h2 id=&quot;reach&quot;&gt;Reach&lt;/h2&gt;

&lt;p&gt;The next example is a pure DRPC topology that computes the reach of a URL on demand. Reach is the number of unique people exposed to a URL on Twitter. To compute reach, you need to fetch all the people who ever tweeted a URL, fetch all the followers of all those people, unique that set of followers, and that count that uniqued set. Computing reach is too intense for a single machine – it can require thousands of database calls and tens of millions of tuples. With Storm and Trident, you can parallelize the computation of each step across a cluster.&lt;/p&gt;

&lt;p&gt;This topology will read from two sources of state. One database maps URLs to a list of people who tweeted that URL. The other database maps a person to a list of followers for that person. The topology definition looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TridentState urlToTweeters = topology.newStaticState(getUrlToTweetersState()); 
TridentState tweetersToFollowers = topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream(“reach”) 
		.stateQuery(urlToTweeters, new Fields(“args”), new MapGet(), new Fields(“tweeters”)) 
		.each(new Fields(“tweeters”), new ExpandList(), new Fields(“tweeter”)) 
		.shuffle() 
		.stateQuery(tweetersToFollowers, new Fields(“tweeter”), new MapGet(), new Fields(“followers”)) 
		.parallelismHint(200) 
		.each(new Fields(“followers”), new ExpandList(), new Fields(“follower”)) 
		.groupBy(new Fields(“follower”)) 
		.aggregate(new One(), new Fields(“one”)) 
		.parallelismHint(20) 
		.aggregate(new Count(), new Fields(“reach”));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The topology creates TridentState objects representing each external database using the newStaticState method. These can then be queried in the topology. Like all sources of state, queries to these databases will be automatically batched for maximum efficiency.&lt;/p&gt;

&lt;p&gt;The topology definition is straightforward – it’s just a simple batch processing job. First, the urlToTweeters database is queried to get the list of people who tweeted the URL for this request. That returns a list, so the ExpandList function is invoked to create a tuple for each tweeter.&lt;/p&gt;

&lt;p&gt;Next, the followers for each tweeter must be fetched. It’s important that this step be parallelized, so shuffle is invoked to evenly distribute the tweeters among all workers for the topology. Then, the followers database is queried to get the list of followers for each tweeter. You can see that this portion of the topology is given a large parallelism since this is the most intense portion of the computation.&lt;/p&gt;

&lt;p&gt;Next, the set of followers is uniqued and counted. This is done in two steps. First a “group by” is done on the batch by “follower”, running the “One” aggregator on each group. The “One” aggregator simply emits a single tuple containing the number one for each group. Then, the ones are summed together to get the unique count of the followers set. Here’s the definition of the “One” aggregator:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class One implements CombinerAggregator { 

	public Integer init(TridentTuple tuple) { 
		return 1; 
	}

	public Integer combine(Integer val1, Integer val2) { 
		return 1; 
	}

	public Integer zero() { return 1; } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a “combiner aggregator”, which knows how to do partial aggregations before transferring tuples over the network to maximize efficiency. Sum is also defined as a combiner aggregator, so the global sum done at the end of the topology will be very efficient.&lt;/p&gt;

&lt;p&gt;Let’s now look at Trident in more detail.&lt;/p&gt;

&lt;h2 id=&quot;fields-and-tuples&quot;&gt;Fields and tuples&lt;/h2&gt;

&lt;p&gt;The Trident data model is the TridentTuple which is a named list of values. During a topology, tuples are incrementally built up through a sequence of operations. Operations generally take in a set of input fields and emit a set of “function fields”. The input fields are used to select a subset of the tuple as input to the operation, while the “function fields” name the fields the operation emits.&lt;/p&gt;

&lt;p&gt;Consider this example. Suppose you have a stream called “stream” that contains the fields “x”, “y”, and “z”. To run a filter MyFilter that takes in “y” as input, you would say:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.each(new Fields(&quot;y&quot;), new MyFilter())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Suppose the implementation of MyFilter is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyFilter extends BaseFilter { 

	public boolean isKeep(TridentTuple tuple){ 
		return tuple.getInteger(0) &amp;lt; 10; 
	} 

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will keep all tuples whose “y” field is less than 10. The TridentTuple given as input to MyFilter will only contain the “y” field. Note that Trident is able to project a subset of a tuple extremely efficiently when selecting the input fields: the projection is essentially free.&lt;/p&gt;

&lt;p&gt;Let’s now look at how “function fields” work. Suppose you had this function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class AddAndMultiply extends BaseFunction { 
	public void execute(TridentTuple tuple, TridentCollector collector) {
		int i1 = tuple.getInteger(0); 
		int i2 = tuple.getInteger(1); 
		collector.emit(new Values(i1 + i2, i1 * i2)); 
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function takes two numbers as input and emits two new values: the addition of the numbers and the multiplication of the numbers. Suppose you had a stream with the fields “x”, “y”, and “z”. You would use this function like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.each(new Fields(&quot;x&quot;, &quot;y&quot;), new AddAndMultiply(), new Fields(&quot;added&quot;, &quot;multiplied&quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of functions is additive: the fields are added to the input tuple. So the output of this each call would contain tuples with the five fields “x”, “y”, “z”, “added”, and “multiplied”. “added” corresponds to the first value emitted by AddAndMultiply, while “multiplied” corresponds to the second value.&lt;/p&gt;

&lt;p&gt;With aggregators, on the other hand, the function fields replace the input tuples. So if you had a stream containing the fields “val1” and “val2”, and you did this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.aggregate(new Fields(&quot;val2&quot;), new Sum(), new Fields(&quot;sum&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output stream would only contain a single tuple with a single field called “sum”, representing the sum of all “val2” fields in that batch.&lt;/p&gt;

&lt;p&gt;With grouped streams, the output will contain the grouping fields followed by the fields emitted by the aggregator. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.groupBy(new Fields(&quot;val1&quot;)) .aggregate(new Fields(&quot;val2&quot;), new Sum(), new Fields(&quot;sum&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the output will contain the fields “val1” and “sum”.&lt;/p&gt;

&lt;h2 id=&quot;state&quot;&gt;State&lt;/h2&gt;

&lt;p&gt;A key problem to solve with realtime computation is how to manage state so that updates are idempotent in the face of failures and retries. It’s impossible to eliminate failures, so when a node dies or something else goes wrong, batches need to be retried. The question is – how do you do state updates (whether external databases or state internal to the topology) so that it’s like each message was only processed only once?&lt;/p&gt;

&lt;p&gt;This is a tricky problem, and can be illustrated with the following example. Suppose that you’re doing a count aggregation of your stream and want to store the running count in a database. If you store only the count in the database and it’s time to apply a state update for a batch, there’s no way to know if you applied that state update before. The batch could have been attempted before, succeeded in updating the database, and then failed at a later step. Or the batch could have been attempted before and failed to update the database. You just don’t know.&lt;/p&gt;

&lt;p&gt;Trident solves this problem by doing two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each batch is given a unique id called the “transaction id”. If a batch is retried it will have the exact same transaction id.&lt;/li&gt;
  &lt;li&gt;State updates are ordered among batches. That is, the state updates for batch 3 won’t be applied until the state updates for batch 2 have succeeded.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With these two primitives, you can achieve exactly-once semantics with your state updates. Rather than store just the count in the database, what you can do instead is store the transaction id with the count in the database as an atomic value. Then, when updating the count, you can just compare the transaction id in the database with the transaction id for the current batch. If they’re the same, you skip the update – because of the strong ordering, you know for sure that the value in the database incorporates the current batch. If they’re different, you increment the count.&lt;/p&gt;

&lt;p&gt;Of course, you don’t have to do this logic manually in your topologies. This logic is wrapped by the State abstraction and done automatically. Nor is your State object required to implement the transaction id trick: if you don’t want to pay the cost of storing the transaction id in the database, you don’t have to. In that case the State will have at-least-once-processing semantics in the case of failures (which may be fine for your application). You can read more about how to implement a State and the various fault-tolerance tradeoffs possible &lt;a href=&quot;http://storm.apache.org/documentation/Trident-state&quot;&gt;in this doc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A State is allowed to use whatever strategy it wants to store state. So it could store state in an external database or it could keep the state in-memory but backed by HDFS (like how HBase works). State’s are not required to hold onto state forever. For example, you could have an in-memory State implementation that only keeps the last X hours of data available and drops anything older. Take a look at the implementation of the &lt;a href=&quot;https://github.com/nathanmarz/trident-memcached/blob/master/src/jvm/trident/memcached/MemcachedState.java&quot;&gt;Memcached integration&lt;/a&gt; for an example State implementation.&lt;/p&gt;

&lt;h2 id=&quot;execution-of-trident-topologies&quot;&gt;Execution of Trident topologies&lt;/h2&gt;

&lt;p&gt;Trident topologies compile down into as efficient of a Storm topology as possible. Tuples are only sent over the network when a repartitioning of the data is required, such as if you do a groupBy or a shuffle. So if you had this Trident topology:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-trident/trident-to-storm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It would compile into Storm spouts/bolts like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-trident/trident-to-storm2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Trident makes realtime computation elegant. You’ve seen how high throughput stream processing, state manipulation, and low-latency querying can be seamlessly intermixed via Trident’s API. Trident lets you express your realtime computations in a natural way while still getting maximal performance.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;h2 id=&quot;section-1&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;今天几个人讨论某事，说来是鸡毛蒜皮的小事，不过也大小是个活动，通过及时通讯软件进行讨论，参与人员基本上都装死，很冷清，大部分人都是在看消息，不过内心里，大家都是愿意参加这么个活动的，这样只有一个人积极发言，没有交互、讨论，内心还是挺凄凉的，这种情况怎么办？时势造英雄，需要一个人花点时间整理一下活动的方方面面细则：时间、地点、人员、路线、内容、费用、事项（每项的负责人），然后出一个基本稿，发到群中，针对基本稿进行讨论，就有目标了；讨论结束再来个总结，算是定稿，妥妥的。&lt;/p&gt;

&lt;p&gt;说起这个，又想起今天早上一起去装机的事情了，&lt;code&gt;**&lt;/code&gt;去重装服务器，回来后说：已经装完了，并且测试外网能够访问。OK，他是个有良好习惯的人，不仅把事情做了，而且做了验证(测试)，确保事情做好了。想想之前我去重装系统，也是这么操作的，并且旁边站了2位同事，其中就有&lt;code&gt;**&lt;/code&gt;，当时安装完系统后，我要测试一下是否完成配置，他们两位没有说话，我想他们应该是支持花费时间进行测试的，因为他们并没有反对测试，我想在我要花时间测试的时候，肯定有一种人会说：都已经安装、配置好了，还测试什么，浪费时间，走吧。说了这么多，其实是想说一件事：人是有差异的，有优秀的人，有的就平庸，有的就是需要剔除的坏因素；识别优秀的人，亲近他们，疏离除此之外的任何人，年轻成长的时候，更是要如此。&lt;/p&gt;

&lt;p&gt;把事情做成，并测试事情是否已经做成，以此来确保已经把事情做成，这只是起步，往上还有空间：把事做成，确保做成，把事做好，确保做好。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>Eclipse下搭建Kafka的开发环境</title>
     <link href="http://ningg.github.com/kafka-dev-env-with-eclipse"/>
     <updated>2014-11-01T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-dev-env-with-eclipse</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近要进行Kafka开发，在&lt;a href=&quot;http://kafka.apache.org/code.html&quot;&gt;官网&lt;/a&gt;看到可以在IDE下开发，赶紧点进去看了看，并且在本地Eclipse下搭建了个Kafka的开发环境，主要参考来源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Developer+Setup&quot;&gt;Kafka Developer Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;编译环境(非代理)&lt;/h2&gt;

&lt;p&gt;查看自己机器的环境：我用笔记本来编译的，是win 7（x64）操作系统；更详细的编译环境信息通过如下方式查看：&lt;code&gt;CMD&lt;/code&gt;–&amp;gt;&lt;code&gt;systeminfo&lt;/code&gt;，这个命令收集系统信息，需要花费40s，稍等一会儿，得到如下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\Administrator&amp;gt;systeminfo

OS 名称:          Microsoft Windows 7 旗舰版
OS 版本:          6.1.7601 Service Pack 1 Build 7601

系统类型:         x64-based PC
处理器:           安装了 1 个处理器。
	 [01]: Intel64 Family 6 Model 23 Stepping 6 GenuineIntel ~785 Mhz

物理内存总量:     2,968 MB
可用的物理内存:   819 MB
虚拟内存: 最大值: 5,934 MB
虚拟内存: 可用:   2,196 MB
虚拟内存: 使用中: 3,738 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-2&quot;&gt;开始编译&lt;/h3&gt;

&lt;p&gt;需要提前下载几个东西：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka源码包：&lt;a href=&quot;http://kafka.apache.org/downloads.html&quot;&gt;kafka-0.8.1.1-src.tgz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse下的Scala 2.10.x IDE plugin：&lt;a href=&quot;http://scala-ide.org/download/current.html&quot;&gt;For Scala 2.10.4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse下的IvyIDE plugin：&lt;a href=&quot;http://ant.apache.org/ivy/ivyde/download.cgi&quot;&gt; apache-ivyde-2.2.0.final-201311091524-RELEASE.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eclipse&quot;&gt;Eclipse下安装插件&lt;/h3&gt;

&lt;p&gt;基本步骤：打开Eclipse–Help–Install new Software，具体见下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/install-new-software.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/install-plugins.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于IvyDE，如果上述办法添加插件出错，则，进行如下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IvyDE features &lt;code&gt;features/org.apache.ivyde.*.jar&lt;/code&gt; to put in your &lt;code&gt;$ECLIPSE_HOME/features&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;IvyDE plugins &lt;code&gt;plugins/org.apache.ivyde.*.jar&lt;/code&gt; to put in your &lt;code&gt;$ECLIPSE_HOME/plugins&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eclipse-project-file&quot;&gt;生成Eclipse project file&lt;/h3&gt;

&lt;p&gt;由于我的电脑是Windowns 7，因此安装了Cygwin，下面的操作都是在Cygwin下进行的，具体是，到Kafka源码包的路径下，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd $KAFKA_SRC_HOME
./gradlew eclipse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;kafkaeclipse&quot;&gt;kafka工程导入Eclipse&lt;/h3&gt;

&lt;p&gt;将上一步生成的project导入到Eclipse中，具体：&lt;code&gt;File&lt;/code&gt; -&amp;gt; &lt;code&gt;Import&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Existing Projects into Workspace&lt;/code&gt;，结果如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/kafka-src.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;问题及解决办法&lt;/h3&gt;

&lt;p&gt;上述kafka工程导入Eclipse后，实质是几个工程：perf、examples、core、contrib、clients；其中perf、core工程是scala工程，其余为java工程；但是examples工程中提示多个问题，列出一个看一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# The import kafka.consumer cannot be resolved
import kafka.consumer.ConsumerConfig;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述问题，在网上搜了一圈，最终StackOverFlow上找到了：&lt;a href=&quot;http://stackoverflow.com/questions/22102257/eclipse-scala-object-cannot-be-resolved&quot;&gt;Eclipse scala.object cannot be resolved&lt;/a&gt;，不过，上面的提示对于当前的问题，好像没有用；因为，examples工程以调用的是core工程的核心代码，而不是scala-library中的代码；并且，examples工程在&lt;code&gt;Java Build Path&lt;/code&gt;–&lt;code&gt;Projects&lt;/code&gt;(Required projects on the build path)中已经添加了core工程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/examples-core-build-path.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;怎么还是有错？奥，core工程是scala工程，而examples工程是java工程，并且在examples中引用的core中代码也都是*.scala代码，OK，将examples工程也转换为scala工程吧：点击examples工程，&lt;code&gt;右键&lt;/code&gt;–&lt;code&gt;Configure&lt;/code&gt;–&lt;code&gt;add Scala Nature&lt;/code&gt;，clean一下examples工程，OK，examples工程的错误没啦。不过core工程仍然还有错误，大意如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka.utils.nonthreadsafe  required: scala.annotation.Annotation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：打开core工程下&lt;code&gt;Annotations_2.8.scala&lt;/code&gt;文件，添加一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import scala.annotation.StaticAnnotation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;clean一下core工程，OK，这次总算搞定了，开始开发吧。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;编译环境(代理)&lt;/h2&gt;

&lt;p&gt;利用公司内网进行编译，开始之前，先查询一下机器配置，命令&lt;code&gt;CMD&lt;/code&gt;–&lt;code&gt;systeminfo&lt;/code&gt;，显示结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Documents and Settings\ningg&amp;gt;systeminfo

OS 名称:      Microsoft Windows XP Professional
OS 版本:      5.1.2600 Service Pack 3 Build 2600
OS 制造商:    Microsoft Corporation
OS 构件类型:  Multiprocessor Free
系统制造商:   LENOVO
系统型号:     ThinkCentre M6400t-N000
系统类型:     X86-based PC
处理器:       安装了 1 个处理器。
       [01]: x86 Family 6 Model 58 Stepping 9 GenuineIntel ~3392 Mhz
BIOS 版本:    LENOVO - 14f0
物理内存总量: 3,546 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;编译环境(代理)&lt;/strong&gt;与&lt;strong&gt;编译环境(非代理)&lt;/strong&gt;基本一致，不过需要配置一下代理，仅此而已，下面只列出两者有差异的地方。&lt;/p&gt;

&lt;h3 id=&quot;eclipse-project-file-1&quot;&gt;生成Eclipse project file&lt;/h3&gt;

&lt;p&gt;目标：通过代理方式要Cygwin能够联网，并且在Cygwin下编译Kafka对应的eclipse环境。&lt;/p&gt;

&lt;h4 id=&quot;cygwinhttp&quot;&gt;设置Cygwin的http代理&lt;/h4&gt;

&lt;p&gt;在Cygwin下无法上网，并且没有&lt;code&gt;wget&lt;/code&gt;命令，那好，重装一下Cygwin，并且安装过程中增加&lt;code&gt;wget&lt;/code&gt;工具；然后，配置Cygwin的代理：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# set http proxy
export http_proxy=http://username:passwd@ip:port

# test whether http_proxy is useful.
wget www.baidu.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照当前的设置，Cygwin环境就联网了。另，仍有个问题，每次重启Cygwin的终端窗口，都需要重新设置http_proxy，如何解决？&lt;/p&gt;

&lt;h4 id=&quot;gradle&quot;&gt;gradle代理设置&lt;/h4&gt;

&lt;p&gt;上述设置，并无法执行&lt;code&gt;./gradlew eclipse&lt;/code&gt;，具体出错信息是：无法下载一些maven的中央仓库中的依赖。gradle是第一次接触，说是与Maven功能类似的自动构建工具，细节不多说，既然是工具，应该跟Maven类似，可以配置代理，查询一下，OK，果真能设置，具体操作：在工程的根目录下，创建文件&lt;code&gt;gradle.properties&lt;/code&gt;，其中添加proxy设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemProp.http.proxyHost=IP
systemProp.http.proxyPort=port
systemProp.http.proxyUser=username
systemProp.http.proxyPassword=password
systemProp.http.nonProxyHosts=*.nonproxyrepos.com|localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置好代理之后，在Cgywin下执行命令&lt;code&gt;./gradlew eclipse&lt;/code&gt;即可；另外，也可在&lt;code&gt;gradle.properties&lt;/code&gt;中修改编译源码时，对应的scala版本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：gradle是什么，要有个基本的了解，当前&lt;a href=&quot;http://stevex.blog.51cto.com/4300375/1339735&quot;&gt;参考&lt;/a&gt;，其中提到两个地方：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gradle入门：&lt;a href=&quot;http://spring.io/guides/gs/gradle/&quot;&gt;http://spring.io/guides/gs/gradle/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;gradle官网有免费电子书：《Building and Testing with Gradle》&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;今天在公司，折腾一下午，网络问题，脑袋都大了，回来后，不到30mins就搞定了，顺便还整理了下，形成了此文；没有稳定的网络，对于有追求的工程师，就如同拿着锄头的特战队员，能力再牛，照样被拿微冲的小白恐吓。&lt;/p&gt;

&lt;p&gt;补充：今天（2014-11-03）到公司，还是不死心，重新尝试了一下在代理环境中编译Kafka源码；具体参考上面&lt;strong&gt;编译环境(代理)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;今天无意间看到某句话，贴出来，算是勉励自己：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;大部分事情，多用心、多用时间，都是可以超过大部分人的。人做事其实是习惯，只有这样的人，才是优秀的。人生里，要识别出这样的人，和优秀者接近，远离平庸。不要急，多练多体会，水平会逐步提高。我每次都是搜索菜谱，然后用心比较，选择正确的步骤整合。&lt;/p&gt;
&lt;/blockquote&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm 0.9.2：如何从Kafka读取数据</title>
     <link href="http://ningg.github.com/storm-with-kafka"/>
     <updated>2014-10-31T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-with-kafka</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;之前研读了&lt;a href=&quot;/in-stream-big-data-processing&quot;&gt;In-Stream Big Data Processing&lt;/a&gt;，组里将基于此实现一个实时的数据分析系统，基本选定三个组件：Flume、Kafka、Storm，其中，Flume负责数据采集，Kafka是一个MQ:负责数据采集与数据分析之间解耦，Storm负责进行流式处理。&lt;/p&gt;

&lt;p&gt;把这3个东西串起来，可以吗？可以的，之前整理了&lt;a href=&quot;/flume-with-kafka&quot;&gt;Flume与Kafka整合&lt;/a&gt;的文章，那Storm能够与Kafka整合吗？Storm官网有介绍：
&lt;a href=&quot;http://storm.apache.org/about/integrates.html&quot;&gt;Storm Integrates&lt;/a&gt;，其中给出了Storm与Kafka集成的&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/external/storm-kafk&quot;&gt;方案&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;storm&quot;&gt;回顾Storm&lt;/h2&gt;

&lt;p&gt;之前都是以原文+注释方式，来阅读Storm的官方文档，现在集中整理一下。Storm集群的构成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;包含两种节点：master和worker；&lt;/li&gt;
  &lt;li&gt;master上运行&lt;code&gt;Nimbus&lt;/code&gt;，负责：distribute code、assign task、monitor failutes；&lt;/li&gt;
  &lt;li&gt;worker上运行&lt;code&gt;Supervisor&lt;/code&gt;，负责：监听&lt;code&gt;Nimbus&lt;/code&gt;分配的任务，并启停worker precess；&lt;/li&gt;
  &lt;li&gt;zookeeper负责协调&lt;code&gt;Nimbus&lt;/code&gt;和&lt;code&gt;Supervisor&lt;/code&gt;之间的关系，所有状态信息都存储在zookeeper or local host；因此，重启Nimbus or Supervisor进程，对用户来说无影响；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/storm-cluster.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于 spout 和 bolt ，说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;spout（龙卷风、气旋）： source of stream，向topology中拉入数据的原点；&lt;/li&gt;
  &lt;li&gt;bolt（闪电）：处理 stream，通过run functions、filter tuples、do streaming aggregations、do streaming join、talk to database… 来做任何事情；&lt;/li&gt;
  &lt;li&gt;topology：由spout、bolt以及他们之间的关系构成，是client提交给Storm cluster执行的基本单元；&lt;/li&gt;
  &lt;li&gt;topology中所有node都是并发运行的，可以配置每个node的并发数；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：topology中node是什么概念？spout、bolt？master、worker？jvm process？thread？
&lt;strong&gt;RE&lt;/strong&gt;：master、worker对应Storm的node，master负责控制，worker负责具体执行；spout、bolt是逻辑上的，并且分布在不同的worker上；每个spout、bolt可配置并发数，这个并发数对应启动的thread；不同的spout、bolt对应不同的thread，thread间不能共用；这些所有的thread由所有的worker process来执行，举例，累计300个thread，启动了30个worker，则平均每个worker process对应执行10个thread（前面的说法对吗？哈哈）&lt;/p&gt;

&lt;p&gt;关于数据模型，即数据的结构，说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storm中，使用tuple结构来存储数据，tuple由fields构成，field可以为任意类型；&lt;/li&gt;
  &lt;li&gt;topology中spout、bolt必须声明其emit的tuple格式：&lt;code&gt;declareOutputFields()&lt;/code&gt;；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setSpout&lt;/code&gt;/&lt;code&gt;setBolt&lt;/code&gt;用于定义spout和bolt，输入参数：node id、processing logic、amount of parallelism；&lt;/li&gt;
  &lt;li&gt;processing logic对应类spout/bolt，需要implement &lt;code&gt;IRichSpout&lt;/code&gt;/&lt;code&gt;IRichBolt&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Storm有两种执行模式，&lt;code&gt;local mode&lt;/code&gt;和&lt;code&gt;distributed mode&lt;/code&gt;，补充几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;local mode，在本地的process中通过thread模拟worker，多用于testing和development topology；更多参考&lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;资料&lt;/a&gt;；&lt;/li&gt;
  &lt;li&gt;distributed mode，用户向master提交topology以及运行topology所需的所有code；master向worker分发topology；更多参考&lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;资料&lt;/a&gt;；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于Stream groupings，几点：&lt;/p&gt;

&lt;p&gt;*　stream grouping解决的问题：多个执行spout逻辑的thread都输出tuple，这些tuple要发送给bolt对应的多个thread，问题来了，tuple发给bolt的哪个thread？即，stream grouping解决：tuple在不同task之间传递关系；
*　shuffle grouping，随机分发；field grouping，根据给定的field进行分发；更多&lt;a href=&quot;http://storm.apache.org/documentation/Concepts.html&quot;&gt;参考&lt;/a&gt;；&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology-tasks.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;stromkafka&quot;&gt;Strom整合Kafka&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;版本信息&lt;/h3&gt;

&lt;p&gt;Storm与Kafka的版本信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storm：apache-storm-0.9.2-incubating&lt;/li&gt;
  &lt;li&gt;Kafka：kafka_2.9.2-0.8.1.1.tgz&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;基础知识&lt;/h3&gt;

&lt;p&gt;实现Storm读取Kafka中的数据，参考&lt;a href=&quot;http://storm.apache.org/about/integrates.html&quot;&gt;官网介绍&lt;/a&gt;， 本部分主要参考自&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/external/storm-kafk&quot;&gt;storm-kafka&lt;/a&gt;的README。&lt;/p&gt;

&lt;p&gt;Strom从Kafka中读取数据，本质：实现一个Storm中的Spout，来读取Kafka中的数据；这个Spout，可以称为Kafka Spout。实现一个Kafka Spout有两条路：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;core storm spout；&lt;/li&gt;
  &lt;li&gt;Trident spout；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;无论用哪种方式实现Kafka Spout，都分为两步走：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;实现BrokerHost接口：用于记录Kafka broker host与partition之间的映射关系；具体两种实现方式：
    &lt;ul&gt;
      &lt;li&gt;ZkHosts类：从zookeeper中动态的获取kafka broker与partition之间的映射关系；初始化时，需要配置zookeeper的&lt;code&gt;ip:port&lt;/code&gt;；默认，每60s从zookeeper中请求一次映射关系；&lt;/li&gt;
      &lt;li&gt;StaticHosts类：当broker–partition之间的映射关系是静态时，常使用此方法；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;继承KafkaConfig类：用于存储Kafka相关的参数；将上面实例的BrokerHost对象，作为参数传入KafkaConfig，例，Kafka的一个构造方法为&lt;code&gt;KafkaConfig(BrokerHosts hosts, String topic)&lt;/code&gt;；当前其实现方式有两个：
    &lt;ul&gt;
      &lt;li&gt;SpoutConfig：Core KafkaSpout只接受此配置方式；&lt;/li&gt;
      &lt;li&gt;TridentKafkaConfig：TridentKafkaEmitter只接受此配置方式；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KafkaConfig类中涉及到的配置参数默认值如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public int fetchSizeBytes = 1024 * 1024;
public int socketTimeoutMs = 10000;
public int fetchMaxWait = 10000;
public int bufferSizeBytes = 1024 * 1024;
public MultiScheme scheme = new RawMultiScheme();
public boolean forceFromStart = false;
public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
public long maxOffsetBehind = Long.MAX_VALUE;
public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
public int metricsTimeBucketSizeInSecs = 60;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的MultiScheme类型的参数shceme，其负责：将Kafka中取出的byte[]转换为storm所需的tuple，这是一个扩展点，默认是原文输出。两种实现：&lt;code&gt;SchemeAsMultiScheme&lt;/code&gt;和&lt;code&gt;KeyValueSchemeAsMultiScheme&lt;/code&gt;可将读取的byte[]转换为String。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：几个疑问，列在下面了&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;ZkHosts&lt;/code&gt;类的一个构造方法&lt;code&gt;ZkHosts(String brokerZkStr, String brokerZkPath)&lt;/code&gt;，其中&lt;code&gt;brokerZkPath&lt;/code&gt;的含义，原始给出的说法是：”rokerZkPath is the root directory under which all the topics and partition information is stored. by Default this is &lt;code&gt;/brokers&lt;/code&gt; which is what default kafka implementation uses.”&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SpoutConfig(BrokerHosts hosts, String topic, String zkRoot, String id)&lt;/code&gt;，其中，&lt;code&gt;zkRoot&lt;/code&gt;是一个root目录，用于存储consumer的offset；那这个&lt;code&gt;zkRoot&lt;/code&gt;对应的目录物理上在哪台机器？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;配置实例&lt;/h3&gt;

&lt;h4 id=&quot;core-kafka-spout&quot;&gt;Core Kafka Spout&lt;/h4&gt;

&lt;p&gt;本质是设置一个读取Kafka中数据的Kafka Spout，然后，将从替换原始local mode下，topology中的Spout即可。下面是一个已经验证过的实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TopologyBuilder builder = new TopologyBuilder();

BrokerHosts hosts = new ZkHosts(&quot;121.7.2.12:2181&quot;);
SpoutConfig spoutConfig = new SpoutConfig(hosts, &quot;ningg&quot;, &quot;/&quot; + &quot;ningg&quot;, UUID.randomUUID().toString());
spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

// set Spout.
builder.setSpout(&quot;word&quot;, kafkaSpout, 3);
builder.setBolt(&quot;result&quot;, new ExclamationBolt(), 3).shuffleGrouping(&quot;word&quot;);

Config conf = new Config();
conf.setDebug(true);

// submit topology in local mode
LocalCluster cluster = new LocalCluster();
cluster.submitTopology(&quot;test&quot;, conf, builder.createTopology());
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;trident-kafka-spouttodo&quot;&gt;Trident Kafka Spout（todo）&lt;/h3&gt;

&lt;p&gt;todo&lt;/p&gt;

&lt;p&gt;下面的样例并还没验证：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TridentTopology topology = new TridentTopology();
BrokerHosts zk = new ZkHosts(&quot;localhost&quot;);
TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, &quot;test-topic&quot;);
spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());
OpaqueTridentKafkaSpout spout = new OpaqueTridentKafkaSpout(spoutConf);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/about/integrates.html&quot;&gt;Storm Integrates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/external/storm-kafk&quot;&gt;storm-kafka&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
 
</feed>
