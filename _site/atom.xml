<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>NingG.github.com</title>
   <link href="http://ningg.github.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://ningg.github.com" rel="alternate" type="text/html" />
   <updated>2014-11-09T23:01:38+08:00</updated>
   <id>http://ningg.github.com</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>Kafka 0.8.1：broker config（doing）</title>
     <link href="http://ningg.github.com/kafka-broker-config"/>
     <updated>2014-11-09T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-broker-config</id>
     <content type="html">&lt;h2 id=&quot;kafka&quot;&gt;Kafka整体架构的回顾&lt;/h2&gt;

&lt;p&gt;（todo）&lt;/p&gt;

&lt;p&gt;要点：broker、producer、consumer的定位。&lt;/p&gt;

&lt;h2 id=&quot;broker-configs&quot;&gt;Broker Configs&lt;/h2&gt;

&lt;p&gt;The essential configurations are the following:（必要的配置如下）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;broker.id&lt;/li&gt;
  &lt;li&gt;log.dirs&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Topic-level configurations and defaults are discussed in &lt;a href=&quot;http://kafka.apache.org/documentation.html#topic-config&quot;&gt;more detail below&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：什么叫作&lt;strong&gt;Topic-level configuration&lt;/strong&gt;？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Property
    &lt;ul&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;broker.id
    &lt;ul&gt;
      &lt;li&gt;null(non-negative integer id)&lt;/li&gt;
      &lt;li&gt;Each broker is uniquely identified by a non-negative integer id. This id serves as the broker’s “name” and allows the broker to be moved to a different host/port without confusing consumers. You can choose any number you like so long as it is unique.（唯一标识broker，目的：当broker移动到另一个&lt;code&gt;host:port&lt;/code&gt;后，不会对consumer造成影响）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;log.dirs
    &lt;ul&gt;
      &lt;li&gt;/tmp/kafka-logs&lt;/li&gt;
      &lt;li&gt;A comma-separated list of one or more directories in which Kafka data is stored. Each new partition that is created will be placed in the directory which currently has the fewest partitions.（以逗号&lt;code&gt;,&lt;/code&gt;分割，Kafka data的存储位置；新建的partition将会被放置在当前partition数最小的目录下）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;port
    &lt;ul&gt;
      &lt;li&gt;6667&lt;/li&gt;
      &lt;li&gt;The port on which the server accepts client connections.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zookeeper.connect
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;Specifies the ZooKeeper connection string in the form &lt;code&gt;hostname:port&lt;/code&gt;, where hostname and port are the host and port for a node in your ZooKeeper cluster. To allow connecting through other ZooKeeper nodes when that host is down you can also specify multiple hosts in the form &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;ZooKeeper also allows you to add a “chroot” path which will make all kafka data for this cluster appear under a particular path. This is a way to setup multiple Kafka clusters or other applications on the same ZooKeeper cluster. To do this give a connection string in the form &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3/chroot/path&lt;/code&gt; which would put all this cluster’s data under the path &lt;code&gt;/chroot/path&lt;/code&gt;. Note that you must create this path yourself prior to starting the broker and consumers must use the same connection string.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于&lt;code&gt;zookeeper&lt;/code&gt;参数，几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka集群需要借助Zookeeper来进行管理，因此，需要设定Zookeeper集群的位置，可以只设置一个Zookeeper，也可以设置一个列表，疑问：设置一个zookeeper与一个zookeeper列表有差异吗？当只设置一个zookeeper服务器时，是否会自动获取zookeeper列表？&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;可以设置&lt;code&gt;chroot&lt;/code&gt;目录，用于存储kafka集群相关数据，这么做的原因：方便同一个zookeeper集群，管理多个应用（例如，kafka集群）；但需要在启动broker之前，提前创建&lt;code&gt;chroot&lt;/code&gt;目录，并且consumer需要使用相同的&lt;code&gt;zookeeper.connect&lt;/code&gt;作为connection string。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;message.max.bytes
    &lt;ul&gt;
      &lt;li&gt;1000000&lt;/li&gt;
      &lt;li&gt;The maximum size of a message that the server can receive. It is important that this property be in sync with the maximum fetch size your consumers use or else an unruly producer will be able to publish messages too large for consumers to consume.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;num.network.threads
    &lt;ul&gt;
      &lt;li&gt;3&lt;/li&gt;
      &lt;li&gt;The number of network threads that the server uses for handling network requests. You probably don’t need to change this.（处理网络请求所设定的线程数，通常不用调整这个参数）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;num.io.threads
    &lt;ul&gt;
      &lt;li&gt;8&lt;/li&gt;
      &lt;li&gt;The number of I/O threads that the server uses for executing requests. You should have at least as many threads as you have disks.（server执行request时，启动的I/O线程数目，建议与磁盘个数相同）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;background.threads
    &lt;ul&gt;
      &lt;li&gt;4&lt;/li&gt;
      &lt;li&gt;The number of threads to use for various background processing tasks such as file deletion. You should not need to change this.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queued.max.requests
    &lt;ul&gt;
      &lt;li&gt;500&lt;/li&gt;
      &lt;li&gt;The number of requests that can be queued up for processing by the I/O threads before the network threads stop reading in new requests.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;host.name
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;Hostname of broker. If this is set, it will only bind to this address. If this is not set, it will bind to all interfaces, and publish one to ZK.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;advertised.host.name
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;If this is set this is the hostname that will be given out to producers, consumers, and other brokers to connect to.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;advertised.port
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;The port to give out to producers, consumers, and other brokers to use in establishing connections. This only needs to be set if this port is different from the port the server should bind to.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;socket.send.buffer.bytes	100 * 1024	The SO_SNDBUFF buffer the server prefers for socket connections.&lt;/li&gt;
  &lt;li&gt;socket.receive.buffer.bytes	100 * 1024	The SO_RCVBUFF buffer the server prefers for socket connections.&lt;/li&gt;
  &lt;li&gt;socket.request.max.bytes	100 * 1024 * 1024	The maximum request size the server will allow. This prevents the server from running out of memory and should be smaller than the Java heap size.&lt;/li&gt;
  &lt;li&gt;num.partitions	1	The default number of partitions per topic if a partition count isn’t given at topic creation time.&lt;/li&gt;
  &lt;li&gt;log.segment.bytes	1024 * 1024 * 1024	The log for a topic partition is stored as a directory of segment files. This setting controls the size to which a segment file will grow before a new segment is rolled over in the log. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.roll.hours	24 * 7	This setting will force Kafka to roll a new log segment even if the log.segment.bytes size has not been reached. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.cleanup.policy	delete	This can take either the value delete or compact. If delete is set, log segments will be deleted when they reach the size or time limits set. If compact is set log compaction will be used to clean out obsolete records. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.retention.{minutes,hours}	7 days	The amount of time to keep a log segment before it is deleted, i.e. the default data retention window for all topics. Note that if both log.retention.minutes and log.retention.bytes are both set we delete a segment when either limit is exceeded. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.retention.bytes	-1	The amount of data to retain in the log for each topic-partitions. Note that this is the limit per-partition so multiply by the number of partitions to get the total data retained for the topic. Also note that if both log.retention.hours and log.retention.bytes are both set we delete a segment when either limit is exceeded. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.retention.check.interval.ms	5 minutes	The period with which we check whether any log segment is eligible for deletion to meet the retention policies.&lt;/li&gt;
  &lt;li&gt;log.cleaner.enable	false	This configuration must be set to true for log compaction to run.&lt;/li&gt;
  &lt;li&gt;log.cleaner.threads	1	The number of threads to use for cleaning logs in log compaction.&lt;/li&gt;
  &lt;li&gt;log.cleaner.io.max.bytes.per.second	None	The maximum amount of I/O the log cleaner can do while performing log compaction. This setting allows setting a limit for the cleaner to avoid impacting live request serving.&lt;/li&gt;
  &lt;li&gt;log.cleaner.dedupe.buffer.size	500&lt;em&gt;1024&lt;/em&gt;1024	The size of the buffer the log cleaner uses for indexing and deduplicating logs during cleaning. Larger is better provided you have sufficient memory.&lt;/li&gt;
  &lt;li&gt;log.cleaner.io.buffer.size	512*1024	The size of the I/O chunk used during log cleaning. You probably don’t need to change this.&lt;/li&gt;
  &lt;li&gt;log.cleaner.io.buffer.load.factor	0.9	The load factor of the hash table used in log cleaning. You probably don’t need to change this.&lt;/li&gt;
  &lt;li&gt;log.cleaner.backoff.ms	15000	The interval between checks to see if any logs need cleaning.&lt;/li&gt;
  &lt;li&gt;log.cleaner.min.cleanable.ratio	0.5	This configuration controls how frequently the log compactor will attempt to clean the log (assuming log compaction is enabled). By default we will avoid cleaning a log where more than 50% of the log has been compacted. This ratio bounds the maximum space wasted in the log by duplicates (at 50% at most 50% of the log could be duplicates). A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.cleaner.delete.retention.ms	1 day	The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan). This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.index.size.max.bytes	10 * 1024 * 1024	The maximum size in bytes we allow for the offset index for each log segment. Note that we will always pre-allocate a sparse file with this much space and shrink it down when the log rolls. If the index fills up we will roll a new log segment even if we haven’t reached the log.segment.bytes limit. This setting can be overridden on a per-topic basis (see the per-topic configuration section).&lt;/li&gt;
  &lt;li&gt;log.index.interval.bytes	4096	The byte interval at which we add an entry to the offset index. When executing a fetch request the server must do a linear scan for up to this many bytes to find the correct position in the log to begin and end the fetch. So setting this value to be larger will mean larger index files (and a bit more memory usage) but less scanning. However the server will never add more than one index entry per log append (even if more than log.index.interval worth of messages are appended). In general you probably don’t need to mess with this value.&lt;/li&gt;
  &lt;li&gt;log.flush.interval.messages	None	The number of messages written to a log partition before we force an fsync on the log. Setting this lower will sync data to disk more often but will have a major impact on performance. We generally recommend that people make use of replication for durability rather than depending on single-server fsync, however this setting can be used to be extra certain.&lt;/li&gt;
  &lt;li&gt;log.flush.scheduler.interval.ms	3000	The frequency in ms that the log flusher checks whether any log is eligible to be flushed to disk.&lt;/li&gt;
  &lt;li&gt;log.flush.interval.ms	None	The maximum time between fsync calls on the log. If used in conjuction with log.flush.interval.messages the log will be flushed when either criteria is met.&lt;/li&gt;
  &lt;li&gt;log.delete.delay.ms	60000	The period of time we hold log files around after they are removed from the in-memory segment index. This period of time allows any in-progress reads to complete uninterrupted without locking. You generally don’t need to change this.&lt;/li&gt;
  &lt;li&gt;log.flush.offset.checkpoint.interval.ms	60000	The frequency with which we checkpoint the last flush point for logs for recovery. You should not need to change this.&lt;/li&gt;
  &lt;li&gt;auto.create.topics.enable	true	Enable auto creation of topic on the server. If this is set to true then attempts to produce, consume, or fetch metadata for a non-existent topic will automatically create it with the default replication factor and number of partitions.&lt;/li&gt;
  &lt;li&gt;controller.socket.timeout.ms	30000	The socket timeout for commands from the partition management controller to the replicas.&lt;/li&gt;
  &lt;li&gt;controller.message.queue.size	10	The buffer size for controller-to-broker-channels&lt;/li&gt;
  &lt;li&gt;default.replication.factor	1	The default replication factor for automatically created topics.&lt;/li&gt;
  &lt;li&gt;replica.lag.time.max.ms	10000	If a follower hasn’t sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.&lt;/li&gt;
  &lt;li&gt;replica.lag.max.messages	4000	If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.&lt;/li&gt;
  &lt;li&gt;replica.socket.timeout.ms	30 * 1000	The socket timeout for network requests to the leader for replicating data.&lt;/li&gt;
  &lt;li&gt;replica.socket.receive.buffer.bytes	64 * 1024	The socket receive buffer for network requests to the leader for replicating data.&lt;/li&gt;
  &lt;li&gt;replica.fetch.max.bytes	1024 * 1024	The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.&lt;/li&gt;
  &lt;li&gt;replica.fetch.wait.max.ms	500	The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.&lt;/li&gt;
  &lt;li&gt;replica.fetch.min.bytes	1	Minimum bytes expected for each fetch response for the fetch requests from the replica to the leader. If not enough bytes, wait up to replica.fetch.wait.max.ms for this many bytes to arrive.&lt;/li&gt;
  &lt;li&gt;num.replica.fetchers	1	&lt;/li&gt;
  &lt;li&gt;Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;replica.high.watermark.checkpoint.interval.ms	5000	The frequency with which each replica saves its high watermark to disk to handle recovery.&lt;/li&gt;
  &lt;li&gt;fetch.purgatory.purge.interval.requests	10000	The purge interval (in number of requests) of the fetch request purgatory.&lt;/li&gt;
  &lt;li&gt;producer.purgatory.purge.interval.requests	10000	The purge interval (in number of requests) of the producer request purgatory.&lt;/li&gt;
  &lt;li&gt;zookeeper.session.timeout.ms	6000	ZooKeeper session timeout. If the server fails to heartbeat to ZooKeeper within this period of time it is considered dead. If you set this too low the server may be falsely considered dead; if you set it too high it may take too long to recognize a truly dead server.&lt;/li&gt;
  &lt;li&gt;zookeeper.connection.timeout.ms	6000	The maximum amount of time that the client waits to establish a connection to zookeeper.&lt;/li&gt;
  &lt;li&gt;zookeeper.sync.time.ms	2000	How far a ZK follower can be behind a ZK leader.&lt;/li&gt;
  &lt;li&gt;controlled.shutdown.enable	false	Enable controlled shutdown of the broker. If enabled, the broker will move all leaders on it to some other brokers before shutting itself down. This reduces the unavailability window during shutdown.&lt;/li&gt;
  &lt;li&gt;controlled.shutdown.max.retries	3	Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.&lt;/li&gt;
  &lt;li&gt;controlled.shutdown.retry.backoff.ms	5000	Backoff time between shutdown retries.&lt;/li&gt;
  &lt;li&gt;auto.leader.rebalance.enable	false	If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the “preferred” replica for each partition if it is available.&lt;/li&gt;
  &lt;li&gt;leader.imbalance.per.broker.percentage	10	The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.&lt;/li&gt;
  &lt;li&gt;leader.imbalance.check.interval.seconds	300	The frequency with which to check for leader imbalance.&lt;/li&gt;
  &lt;li&gt;offset.metadata.max.bytes	1024	The maximum amount of metadata to allow clients to save with their offsets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details about broker configuration can be found in the scala class &lt;code&gt;kafka.server.KafkaConfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Topic-level configuration&lt;/p&gt;

&lt;p&gt;Configurations pertinent to topics have both a global default as well an optional per-topic override. If no per-topic configuration is given the global default is used. The override can be set at topic creation time by giving one or more –config options. This example creates a topic named my-topic with a custom max message size and flush rate:
 &amp;gt; bin/kafka-topics.sh –zookeeper localhost:2181 –create –topic my-topic –partitions 1 
        –replication-factor 1 –config max.message.bytes=64000 –config flush.messages=1
Overrides can also be changed or set later using the alter topic command. This example updates the max message size for my-topic:
 &amp;gt; bin/kafka-topics.sh –zookeeper localhost:2181 –alter –topic my-topic 
    –config max.message.bytes=128000
To remove an override you can do
 &amp;gt; bin/kafka-topics.sh –zookeeper localhost:2181 –alter –topic my-topic 
    –deleteConfig max.message.bytes
The following are the topic-level configurations. The server’s default configuration for this property is given under the Server Default Property heading, setting this default in the server config allows you to change the default given to topics that have no override specified.
Property	Default	Server Default Property	Description
cleanup.policy	delete	log.cleanup.policy	A string that is either “delete” or “compact”. This string designates the retention policy to use on old log segments. The default policy (“delete”) will discard old segments when their retention time or size limit has been reached. The “compact” setting will enable log compaction on the topic.
delete.retention.ms	86400000 (24 hours)	log.cleaner.delete.retention.ms	The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan).
flush.messages	None	log.flush.interval.messages	This setting allows specifying an interval at which we will force an fsync of data written to the log. For example if this was set to 1 we would fsync after every message; if it were 5 we would fsync after every five messages. In general we recommend you not set this and use replication for durability and allow the operating system’s background flush capabilities as it is more efficient. This setting can be overridden on a per-topic basis (see the per-topic configuration section).
flush.ms	None	log.flush.interval.ms	This setting allows specifying a time interval at which we will force an fsync of data written to the log. For example if this was set to 1000 we would fsync after 1000 ms had passed. In general we recommend you not set this and use replication for durability and allow the operating system’s background flush capabilities as it is more efficient.
index.interval.bytes	4096	log.index.interval.bytes	This setting controls how frequently Kafka adds an index entry to it’s offset index. The default setting ensures that we index a message roughly every 4096 bytes. More indexing allows reads to jump closer to the exact position in the log but makes the index larger. You probably don’t need to change this.
max.message.bytes	1,000,000	message.max.bytes	This is largest message size Kafka will allow to be appended to this topic. Note that if you increase this size you must also increase your consumer’s fetch size so they can fetch messages this large.
min.cleanable.dirty.ratio	0.5	log.cleaner.min.cleanable.ratio	This configuration controls how frequently the log compactor will attempt to clean the log (assuming log compaction is enabled). By default we will avoid cleaning a log where more than 50% of the log has been compacted. This ratio bounds the maximum space wasted in the log by duplicates (at 50% at most 50% of the log could be duplicates). A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log.
retention.bytes	None	log.retention.bytes	This configuration controls the maximum size a log can grow to before we will discard old log segments to free up space if we are using the “delete” retention policy. By default there is no size limit only a time limit.
retention.ms	7 days	log.retention.minutes	This configuration controls the maximum time we will retain a log before we will discard old log segments to free up space if we are using the “delete” retention policy. This represents an SLA on how soon consumers must read their data.
segment.bytes	1 GB	log.segment.bytes	This configuration controls the segment file size for the log. Retention and cleaning is always done a file at a time so a larger segment size means fewer files but less granular control over retention.
segment.index.bytes	10 MB	log.index.size.max.bytes	This configuration controls the size of the index that maps offsets to file positions. We preallocate this index file and shrink it only after log rolls. You generally should not need to change this setting.
segment.ms	7 days	log.roll.hours	This configuration controls the period of time after which Kafka will force the log to roll even if the segment file isn’t full to ensure that retention can delete or compact old data.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1：Consumer API and Consumer Configs</title>
     <link href="http://ningg.github.com/kafka-api-consumer"/>
     <updated>2014-11-09T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-api-consumer</id>
     <content type="html">&lt;h2 id=&quot;consumer-api&quot;&gt;Consumer API&lt;/h2&gt;

&lt;p&gt;如何从Kafka中读取数据？三种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High Level Consumer API；&lt;/li&gt;
  &lt;li&gt;Simple Consumer API；&lt;/li&gt;
  &lt;li&gt;Kafka Hadoop Consumer API；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;high-level-consumer-api&quot;&gt;High Level Consumer API&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;class Consumer {
  /**
   *  Create a ConsumerConnector
   *
   *  @param config  at the minimum, need to specify the groupid of the consumer and the zookeeper
   *                 connection string zookeeper.connect.
   */
  public static kafka.javaapi.consumer.ConsumerConnector createJavaConsumerConnector(ConsumerConfig config);
}

/**
 *  V: type of the message
 *  K: type of the optional key assciated with the message
 */
public interface kafka.javaapi.consumer.ConsumerConnector {
  /**
   *  Create a list of message streams of type T for each topic.
   *
   *  @param topicCountMap  a map of (topic, #streams) pair
   *  @param decoder a decoder that converts from Message to T
   *  @return a map of (topic, list of  KafkaStream) pairs.
   *          The number of items in the list is #streams. Each stream supports
   *          an iterator over message/metadata pairs.
   */
  public &amp;lt;K,V&amp;gt; Map&amp;lt;String, List&amp;lt;KafkaStream&amp;lt;K,V&amp;gt;&amp;gt;&amp;gt;
	createMessageStreams(Map&amp;lt;String, Integer&amp;gt; topicCountMap, Decoder&amp;lt;K&amp;gt; keyDecoder, Decoder&amp;lt;V&amp;gt; valueDecoder);

  /**
   *  Create a list of message streams of type T for each topic, using the default decoder.
   */
  public Map&amp;lt;String, List&amp;lt;KafkaStream&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; createMessageStreams(Map&amp;lt;String, Integer&amp;gt; topicCountMap);

  /**
   *  Create a list of message streams for topics matching a wildcard.
   *
   *  @param topicFilter a TopicFilter that specifies which topics to
   *                    subscribe to (encapsulates a whitelist or a blacklist).
   *  @param numStreams the number of message streams to return.
   *  @param keyDecoder a decoder that decodes the message key
   *  @param valueDecoder a decoder that decodes the message itself
   *  @return a list of KafkaStream. Each stream supports an
   *          iterator over its MessageAndMetadata elements.
   */
  public &amp;lt;K,V&amp;gt; List&amp;lt;KafkaStream&amp;lt;K,V&amp;gt;&amp;gt;
	createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams, Decoder&amp;lt;K&amp;gt; keyDecoder, Decoder&amp;lt;V&amp;gt; valueDecoder);

  /**
   *  Create a list of message streams for topics matching a wildcard, using the default decoder.
   */
  public List&amp;lt;KafkaStream&amp;lt;byte[], byte[]&amp;gt;&amp;gt; createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams);

  /**
   *  Create a list of message streams for topics matching a wildcard, using the default decoder, with one stream.
   */
  public List&amp;lt;KafkaStream&amp;lt;byte[], byte[]&amp;gt;&amp;gt; createMessageStreamsByFilter(TopicFilter topicFilter);

  /**
   *  Commit the offsets of all topic/partitions connected by this connector.
   */
  public void commitOffsets();

  /**
   *  Shut down the connector
   **/
  public void shutdown();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can follow &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example&quot;&gt;this example&lt;/a&gt; to learn how to use the high level consumer api.&lt;/p&gt;

&lt;h3 id=&quot;simple-consumer-api&quot;&gt;Simple Consumer API&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;class kafka.javaapi.consumer.SimpleConsumer {
/**
*  Fetch a set of messages from a topic.
*
*  @param request specifies the topic name, topic partition, starting byte offset, maximum bytes to be fetched.
*  @return a set of fetched messages
*/
public FetchResponse fetch(kafka.javaapi.FetchRequest request);

/**
*  Fetch metadata for a sequence of topics.
*
*  @param request specifies the versionId, clientId, sequence of topics.
*  @return metadata for each topic in the request.
*/
public kafka.javaapi.TopicMetadataResponse send(kafka.javaapi.TopicMetadataRequest request);

/**
*  Get a list of valid offsets (up to maxSize) before the given time.
*
*  @param request a [[kafka.javaapi.OffsetRequest]] object.
*  @return a [[kafka.javaapi.OffsetResponse]] object.
*/
public kafak.javaapi.OffsetResponse getOffsetsBefore(OffsetRequest request);

/**
* Close the SimpleConsumer.
*/
public void close();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For most applications, the high level consumer Api is good enough. Some applications want features not exposed to the high level consumer yet (e.g., set initial offset when restarting the consumer). They can instead use our low level SimpleConsumer Api. The logic will be a bit more complicated and you can follow the example in &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;kafka-hadoop-consumer-api&quot;&gt;Kafka Hadoop Consumer API&lt;/h3&gt;

&lt;p&gt;Providing a horizontally scalable solution for aggregating and loading data into Hadoop was one of our basic use cases. To support this use case, we provide a Hadoop-based consumer which spawns off many map tasks to pull data from the Kafka cluster in parallel. This provides extremely fast pull-based Hadoop data load capabilities (we were able to fully saturate the network with only a handful of Kafka servers).&lt;/p&gt;

&lt;p&gt;Usage information on the hadoop consumer can be found &lt;a href=&quot;https://github.com/linkedin/camus/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;consumer-configs&quot;&gt;Consumer Configs&lt;/h2&gt;

&lt;p&gt;The essential consumer configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;group.id&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下文将详细介绍这些参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Property
    &lt;ul&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;group.id
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;A string that uniquely identifies the group of consumer processes to which this consumer belongs. By setting the same group id multiple processes indicate that they are all part of the same consumer group.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：consumer group？复习一下，为什么有这个？本质：Kafka中一条message，发送到哪些地方呢？一种是群发给Consumer，一种是只发送给某一个满足条件的Consumer；同时message要求在同一个Consumer中保证message的处理顺序，在满足这一功能需求的情况下，同时为了改善性能，增加了一个概念：consumer group，同一个group下可以包含多个consumer，每次group接收到message，就实例化其内部的一个consumer，如果一个partition中的message就发送给一个group，则顺序处理；否则就是并发处理。疑问：一个consumer group中只包含一个consumer就能够实现串行顺序处理了，为什么还要放置多个consumer？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;zookeeper.connect
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;Specifies the ZooKeeper connection string in the form &lt;code&gt;hostname:port&lt;/code&gt; where host and port are the host and port of a ZooKeeper server. To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down you can also specify multiple hosts in the form &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;The server may also have a ZooKeeper &lt;code&gt;chroot&lt;/code&gt; path as part of it’s ZooKeeper connection string which puts its data under some path in the global ZooKeeper namespace. If so the consumer should use the same chroot path in its connection string. For example to give a chroot path of &lt;code&gt;/chroot/path&lt;/code&gt; you would give the connection string as &lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3/chroot/path&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：在设置&lt;code&gt;zookeeper.connect&lt;/code&gt;时，可以设置zookeeper的&lt;code&gt;chroot&lt;/code&gt;，&lt;code&gt;chroot&lt;/code&gt;的含义：改变元数据在global Zookeeper namespace中的存储位置；一旦修改了&lt;code&gt;chroot&lt;/code&gt;，就需要在链接Zookeeper时，也用上&lt;code&gt;chroot&lt;/code&gt;，具体形式：&lt;code&gt;hostname1:port1,hostname2:port2,hostname3:port3/chroot/path&lt;/code&gt;。（当前理解，前面的&lt;code&gt;/chroot/path&lt;/code&gt;对&lt;code&gt;hostname1:port1&lt;/code&gt;也是有效的）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;consumer.id
    &lt;ul&gt;
      &lt;li&gt;null	&lt;/li&gt;
      &lt;li&gt;Generated automatically if not set.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;socket.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;30 * 1000&lt;/li&gt;
      &lt;li&gt;The socket timeout for network requests. The actual timeout set will be &lt;code&gt;max.fetch.wait&lt;/code&gt; + &lt;code&gt;socket.timeout.ms&lt;/code&gt;.（等待message的时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;socket.receive.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;64 * 1024&lt;/li&gt;
      &lt;li&gt;The socket receive buffer for network requests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fetch.message.max.bytes
    &lt;ul&gt;
      &lt;li&gt;1024 * 1024&lt;/li&gt;
      &lt;li&gt;The number of byes of messages to attempt to fetch for each topic-partition in each fetch request. These bytes will be read into memory for each partition, so this helps control the memory used by the consumer. The fetch request size must be at least as large as the maximum message size the server allows or else it is possible for the producer to send messages larger than the consumer can fetch.（consumer单次请求messages时，最大字节数；通常要求&lt;code&gt;fetch.message.max.bytes&lt;/code&gt;至少为maximum message size）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto.commit.enable
    &lt;ul&gt;
      &lt;li&gt;true&lt;/li&gt;
      &lt;li&gt;If true, periodically commit to ZooKeeper the offset of messages already fetched by the consumer. This committed offset will be used when the process fails as the position from which the new consumer will begin.（默认&lt;code&gt;true&lt;/code&gt;，表示当Consumer成功获取message后，向zookeeper发送message的offset表示commit；committed offset的作用：当consumer process失败后，新的consumer从这一offset，重新开始处理）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto.commit.interval.ms
    &lt;ul&gt;
      &lt;li&gt;60 * 1000&lt;/li&gt;
      &lt;li&gt;The frequency in ms that the consumer offsets are committed to zookeeper.（Consumer多长时间提交一次offset）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：难道不是consumer每成功fetch一个message，就commit一次offset？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;queued.max.message.chunks
    &lt;ul&gt;
      &lt;li&gt;10&lt;/li&gt;
      &lt;li&gt;Max number of message chunks buffered for consumption. Each chunk can be up to &lt;code&gt;fetch.message.max.bytes&lt;/code&gt;.（允许缓存的message chunk的个数）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message chunk什么意思？有用吗？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rebalance.max.retries
    &lt;ul&gt;
      &lt;li&gt;4&lt;/li&gt;
      &lt;li&gt;When a new consumer joins a consumer group the set of consumers attempt to “rebalance” the load to assign partitions to each consumer. If the set of consumers changes while this assignment is taking place the rebalance will fail and retry. This setting controls the maximum number of attempts before giving up.（新的consumer加入到consumer group后，真个consumer group承担的所有partition会进行再分配，如果分配过程中，这些consumer set右发生变化，则会尝试重新执行，此参数，表示尝试的次数。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fetch.min.bytes
    &lt;ul&gt;
      &lt;li&gt;1&lt;/li&gt;
      &lt;li&gt;The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request.（server向fetch request返回的最小字节数，如果data不足，则会等待累积足够的数据之后，再进行响应。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fetch.wait.max.ms
    &lt;ul&gt;
      &lt;li&gt;100&lt;/li&gt;
      &lt;li&gt;The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy &lt;code&gt;fetch.min.bytes&lt;/code&gt;（设定了&lt;code&gt;fetch.min.bytes&lt;/code&gt;，如果没有足够数据，则，最长等待时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;rebalance.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;2000&lt;/li&gt;
      &lt;li&gt;Backoff time between retries during rebalance.（reblalance时，不同的retry之间的退避时长，即，两次retry之间的间隔时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;refresh.leader.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;200&lt;/li&gt;
      &lt;li&gt;Backoff time to wait before trying to determine the leader of a partition that has just lost its leader.（失去leader后，再次请求leader的退避时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auto.offset.reset
    &lt;ul&gt;
      &lt;li&gt;largest&lt;/li&gt;
      &lt;li&gt;What to do when there is no initial offset in ZooKeeper or if an offset is out of range:（当Zookeeper中没有initial offset或者offset超出范围时，如何自动设置offset？）
        &lt;ul&gt;
          &lt;li&gt;smallest : automatically reset the offset to the smallest offset&lt;/li&gt;
          &lt;li&gt;largest : automatically reset the offset to the largest offset&lt;/li&gt;
          &lt;li&gt;anything else: throw exception to the consumer&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;consumer.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;-1&lt;/li&gt;
      &lt;li&gt;Throw a timeout exception to the consumer if no message is available for consumption after the specified interval（如果没有consumer可用的message，等待多长时间后，系统抛出异常）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;client.id
    &lt;ul&gt;
      &lt;li&gt;group id value&lt;/li&gt;
      &lt;li&gt;The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.（用于追踪调用过程）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zookeeper.session.timeout.ms
 	* 6000
    &lt;ul&gt;
      &lt;li&gt;ZooKeeper session timeout. If the consumer fails to heartbeat to ZooKeeper for this period of time it is considered dead and a rebalance will occur.（一段时间内consumer如果失去与Zookeeper之间的心跳，就认定consumer已经丢失，会在consumer group内进行rebalance）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：参数&lt;code&gt;zookeeper.session.timeout.ms&lt;/code&gt;与参数&lt;code&gt;auto.commit.interval.ms&lt;/code&gt;之间的关系，前者衡量的是heartbeat，而后者负责的是offset commit。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;zookeeper.connection.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;6000&lt;/li&gt;
      &lt;li&gt;The max time that the client waits while establishing a connection to zookeeper.（client与zookeeper保持连接的时间，超过这一时间，自动释放）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;zookeeper.sync.time.ms
 	* 2000
    &lt;ul&gt;
      &lt;li&gt;How far a ZK follower can be behind a ZK leader（&lt;strong&gt;什么意思&lt;/strong&gt;？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details about consumer configuration can be found in the scala class &lt;code&gt;kafka.consumer.ConsumerConfig&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example&quot;&gt;Consumer Group Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example&quot;&gt;Simple Consumer Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/linkedin/camus/&quot;&gt;Kafka Hadoop Consumer Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1：Producer API and Producer Configs</title>
     <link href="http://ningg.github.com/kafka-api-producer"/>
     <updated>2014-11-07T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-api-producer</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近在做Flume与Kafka的整合，其中用到了一个工程：&lt;a href=&quot;https://github.com/thilinamb/flume-ng-kafka-sink&quot;&gt;flume-ng-kafka-sink&lt;/a&gt;，本质上就是Flume的一个插件：Kafka sink。遇到一个问题：Kafka sink通过设置kafak broker的&lt;code&gt;ip:port&lt;/code&gt;来寻找broker，那就有一个问题，如果设置连接的kafka broker 宕掉了，flume的数据是不是就送不出去了？&lt;/p&gt;

&lt;h2 id=&quot;producer&quot;&gt;Producer&lt;/h2&gt;

&lt;p&gt;开始介绍Producer之前，说个小问题：上面&lt;strong&gt;背景&lt;/strong&gt;中一直在说Flume的Sink：Kafka Sink，那与Kafka producer什么关系呢？为什么这次标题是&lt;strong&gt;Kafka Producer&lt;/strong&gt;，而丝毫未提&lt;strong&gt;Flume Sink&lt;/strong&gt;？这个问题很好，说明读者在思考，大概说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flume架构中有Sink，是用来将Flume收集到的数据送出去的；Flume下的Kafka Sink插件，在Flume看来，就是个Sink；&lt;/li&gt;
  &lt;li&gt;Kafka架构中有Producer，是用来向Kafka broker中送入数据的；Flume下的Kafka Sink插件，在Kafka看来，就是个Producer；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此次，主要站在Kafka的角度来看一个Producer可以进行的配置。&lt;/p&gt;

&lt;h3 id=&quot;kafka-producer-api&quot;&gt;Kafka Producer API&lt;/h3&gt;

&lt;p&gt;下面是&lt;code&gt;kafka.javaapi.producer.Producer&lt;/code&gt;类的java API，实际上这个类是scala编写的，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 *  V: type of the message
 *  K: type of the optional key associated with the message
 */
 
class kafka.javaapi.producer.Producer&amp;lt;K,V&amp;gt; {

  public Producer(ProducerConfig config);

  /**
   * Sends the data to a single topic, partitioned by key, using either the
   * synchronous or the asynchronous producer
   * @param message the producer data object that encapsulates the topic, key and message data
   */
  public void send(KeyedMessage&amp;lt;K,V&amp;gt; message);

  /**
   * Use this API to send data to multiple topics
   * @param messages list of producer data objects that encapsulate the topic, key and message data
   */
  public void send(List&amp;lt;KeyedMessage&amp;lt;K,V&amp;gt;&amp;gt; messages);

  /**
   * Close API to close the producer pool connections to all Kafka brokers.
   */
  public void close();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体如何使用上述Producer API，可参考&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example&quot;&gt;0.8.0 Producer Example&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;producer-example&quot;&gt;0.8.0 Producer Example&lt;/h3&gt;

&lt;p&gt;研究要深入，上面提到的&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example&quot;&gt;0.8.0 Producer Example&lt;/a&gt;，下面简要介绍一下。&lt;/p&gt;

&lt;p&gt;The Producer class is used to create new messages for a specific Topic and optional Partition.&lt;/p&gt;

&lt;p&gt;If using Java you need to include a few packages for the Producer and supporting classes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step in your code is to define properties for how the Producer finds the cluster, serializes the messages and if appropriate directs the message to a specific Partition.&lt;/p&gt;

&lt;p&gt;代码本质体现的是逻辑，首先需要确定几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Producer如何找到Kafka Cluster；&lt;/li&gt;
  &lt;li&gt;message传输的格式；（serialize，序列化）&lt;/li&gt;
  &lt;li&gt;如何将message存入指定的Partition中；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties are defined in the standard Java Properties object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Properties props = new Properties();
 
props.put(&quot;metadata.broker.list&quot;, &quot;broker1:9092,broker2:9092&quot;);
props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);
props.put(&quot;partitioner.class&quot;, &quot;example.producer.SimplePartitioner&quot;);
props.put(&quot;request.required.acks&quot;, &quot;1&quot;);
 
ProducerConfig config = new ProducerConfig(props);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first property, “metadata.broker.list” defines where the Producer can find a one or more Brokers to determine the Leader for each topic. This does not need to be the full set of Brokers in your cluster but should include at least two in case the first Broker is not available. No need to worry about figuring out which Broker is the leader for the topic (and partition), the Producer knows how to connect to the Broker and ask for the meta data then connect to the correct Broker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一项参数&lt;/strong&gt;&lt;code&gt;metadata.broker.list&lt;/code&gt;，用于配置可用的broker列表，可以只配置一个broker，不过建议最好至少配置2个broker，这样即使有一个broker宕机了，另一个也能及时接替工作；这些broker中，也不用指定不同topic的leader，因为Producer会主动连接Broker并且请求到meta数据，然后连接到topic的leader。&lt;/p&gt;

&lt;p&gt;The second property “serializer.class” defines what Serializer to use when preparing the message for transmission to the Broker. In our example we use a simple String encoder provided as part of Kafka. Note that the encoder must accept the same type as defined in the KeyedMessage object in the next step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二项参数&lt;/strong&gt;&lt;code&gt;serializer.class&lt;/code&gt;，设定了将message从Producer发送到Broker的序列化方式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：“Note that the encoder must accept the same type as defined in the KeyedMessage object in the next step.” 什么含义？ &lt;code&gt;KeyedMessage&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;It is possible to change the Serializer for the Key (see below) of the message by defining “key.serializer.class” appropriately. By default it is set to the same value as “serializer.class”.&lt;/p&gt;

&lt;p&gt;参数&lt;code&gt;key.serializer.class&lt;/code&gt;用于设置key序列化的方法，key将在序列化之后，与message一同从Producer发送到Broker；&lt;code&gt;key.serializer.class&lt;/code&gt;的默认值与&lt;code&gt;serializer.class&lt;/code&gt;相同。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Kafka是按照key进行partition的，每个message绑定的key也是需要传输到broker的，传输过程中也需要进行序列化，&lt;/p&gt;

&lt;p&gt;The third property  “partitioner.class” defines what class to use to determine which Partition in the Topic the message is to be sent to. This is optional, but for any non-trivial implementation you are going to want to implement a partitioning scheme. More about the implementation of this class later. If you include a value for the key but haven’t defined a partitioner.class Kafka will use the default partitioner. If the key is null, then the Producer will assign the message to a random Partition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第三项参数&lt;/strong&gt;&lt;code&gt;partitioner.class&lt;/code&gt;用于设定message与Partition的映射关系，简单来说，每个message都发送给broker的某个对应的Topic，但message真正存储对应的是Topic下的partition，那么，参数&lt;code&gt;partitioner.class&lt;/code&gt;就是用于设定message–partition之间映射关系的。&lt;/p&gt;

&lt;p&gt;The last property “request.required.acks” tells Kafka that you want your Producer to require an acknowledgement from the Broker that the message was received. Without this setting the Producer will ‘fire and forget’ possibly leading to data loss. Additional information can be found &lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后一项参数&lt;/strong&gt;&lt;code&gt;request.required.acks&lt;/code&gt;，设定Broker在接收到message之后，是否返回一个确认信息（ack）。如果没有这个信息，那么很有可能&lt;code&gt;fire and forget&lt;/code&gt;并且丢失数据。更多Kafka的相关配置信息，参考：&lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;Kafka Configuration&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：有个问题，即使Broker在接收到message之后，返回了ack信息，那Producer提供了重发机制吗？还是Producer只是进行登记？&lt;/p&gt;

&lt;p&gt;Next you define the Producer object itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Producer&amp;lt;String, String&amp;gt; producer = new Producer&amp;lt;String, String&amp;gt;(config);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the Producer is a Java Generic and you need to tell it the type of two parameters. The first is the type of the Partition key, the second the type of the message. In this example they are both Strings, which also matches to what we defined in the Properties above.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Producer&lt;/code&gt;是一个Java Generic（泛型），需要输入两个参数，&lt;code&gt;&amp;lt;String, String&amp;gt;&lt;/code&gt;，第一个参数是Partition key的类型，第二个是message的类型&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：java中Generic的用法、注意事项有哪些？上面说的Partition key，到底指什么？是properties中的属性和属性值吗？不是的，查看源代码，Partition key就是按照key进行partition的key。&lt;/p&gt;

&lt;p&gt;Now build your message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Random rnd = new Random();
long runtime = new Date().getTime();
String ip = “192.168.2.” + rnd.nextInt(255);
String msg = runtime + “,www.example.com,” + ip;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example we are faking a message for a website visit by IP address. First part of the comma-separated message is the timestamp of the event, the second is the website and the third is the IP address of the requester. We use the Java Random class here to make the last octet of the IP vary so we can see how Partitioning works.（上面&lt;code&gt;msg&lt;/code&gt;中是伪造的一个网站访问记录）&lt;/p&gt;

&lt;p&gt;Finally write the message to the Broker:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KeyedMessage&amp;lt;String, String&amp;gt; data = new KeyedMessage&amp;lt;String, String&amp;gt;(&quot;page_visits&quot;, ip, msg);
 
producer.send(data);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The “page_visits” is the Topic to write to. Here we are passing the IP as the partition key. Note that if you do not include a key, even if you’ve defined a partitioner class, Kafka will assign the message to a random partition.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;KeyedMessage&amp;lt;String, String&amp;gt;(topic, message)&lt;/code&gt;或者&lt;code&gt;KeyedMessage&amp;lt;String, String&amp;gt;(topic, key, message)&lt;/code&gt;，如果没输入key，那么即使设定了&lt;code&gt;partitioner.class&lt;/code&gt;也不会对message分发到相应partition的，原因很简单，因为真的没有key。&lt;/p&gt;

&lt;p&gt;Full Source:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import java.util.*;
 
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
 
public class TestProducer {
	public static void main(String[] args) {
		long events = Long.parseLong(args[0]);
		Random rnd = new Random();
 
		Properties props = new Properties();
		props.put(&quot;metadata.broker.list&quot;, &quot;broker1:9092,broker2:9092 &quot;);
		props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);
		props.put(&quot;partitioner.class&quot;, &quot;example.producer.SimplePartitioner&quot;);
		props.put(&quot;request.required.acks&quot;, &quot;1&quot;);
 
		ProducerConfig config = new ProducerConfig(props);
 
		Producer&amp;lt;String, String&amp;gt; producer = new Producer&amp;lt;String, String&amp;gt;(config);
 
		for (long nEvents = 0; nEvents &amp;lt; events; nEvents++) { 
			   long runtime = new Date().getTime();  
			   String ip = “192.168.2.” + rnd.nextInt(255); 
			   String msg = runtime + “,www.example.com,” + ip; 
			   KeyedMessage&amp;lt;String, String&amp;gt; data = new KeyedMessage&amp;lt;String, String&amp;gt;(&quot;page_visits&quot;, ip, msg);
			   producer.send(data);
		}
		producer.close();
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Partitioning Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import kafka.producer.Partitioner;
import kafka.utils.VerifiableProperties;
 
public class SimplePartitioner implements Partitioner {
	public SimplePartitioner (VerifiableProperties props) {
 
	}
 
	public int partition(Object key, int a_numPartitions) {
		int partition = 0;
		String stringKey = (String) key;
		int offset = stringKey.lastIndexOf(&#39;.&#39;);
		if (offset &amp;gt; 0) {
		   partition = Integer.parseInt( stringKey.substring(offset+1)) % a_numPartitions;
		}
	   return partition;
  }
 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The logic takes the key, which we expect to be the IP address, finds the last octet and does a modulo operation on the number of partitions defined within Kafka for the topic. The benefit of this partitioning logic is all web visits from the same source IP end up in the same Partition. Of course so do other IPs, but your consumer logic will need to know how to handle that.
（将有时间顺序的message放到同一个partition中）&lt;/p&gt;

&lt;p&gt;Before running this, make sure you have created the Topic page_visits. From the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-create-topic.sh --topic page_visits --replica 3 --zookeeper localhost:2181 --partition 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you include a &lt;code&gt;--partition&lt;/code&gt; option so you create more than one.
（要使用&lt;code&gt;--partition&lt;/code&gt;来创建多个partition，否则可能只有一个）&lt;/p&gt;

&lt;p&gt;Now compile and run your Producer and data will be written to Kafka.&lt;/p&gt;

&lt;p&gt;To confirm you have data, use the command line tool to see what was written:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic page_visits --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用Maven进行Producer开发时，需要添加的POM配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;kafka_2.9.2&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;0.8.1.1&amp;lt;/version&amp;gt;
  &amp;lt;scope&amp;gt;compile&amp;lt;/scope&amp;gt;
  &amp;lt;exclusions&amp;gt;
	&amp;lt;exclusion&amp;gt;
	  &amp;lt;artifactId&amp;gt;jmxri&amp;lt;/artifactId&amp;gt;
	  &amp;lt;groupId&amp;gt;com.sun.jmx&amp;lt;/groupId&amp;gt;
	&amp;lt;/exclusion&amp;gt;
	&amp;lt;exclusion&amp;gt;
	  &amp;lt;artifactId&amp;gt;jms&amp;lt;/artifactId&amp;gt;
	  &amp;lt;groupId&amp;gt;javax.jms&amp;lt;/groupId&amp;gt;
	&amp;lt;/exclusion&amp;gt;
	&amp;lt;exclusion&amp;gt;
	  &amp;lt;artifactId&amp;gt;jmxtools&amp;lt;/artifactId&amp;gt;
	  &amp;lt;groupId&amp;gt;com.sun.jdmk&amp;lt;/groupId&amp;gt;
	&amp;lt;/exclusion&amp;gt;
  &amp;lt;/exclusions&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-1&quot;&gt;几个情况&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;思考1&lt;/strong&gt;：Kafka 0.7.2版本中，直接在Producer中配置Zookeeper，Producer通过Zookeeper来获知Broker的位置，简单来说，应用与Kafka之间是解耦的，可以在不修改Producer信息的情况下，动态增减Broker。&lt;/p&gt;

&lt;p&gt;当前，通过&lt;code&gt;metadata.broker.list&lt;/code&gt;来设置broker的列表，有几个问题，稍微梳理一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果只在&lt;code&gt;metadata.broker.list&lt;/code&gt;中配置一个broker，那么Producer能够识别出其他broker吗？&lt;/li&gt;
  &lt;li&gt;如果能够识别出未配置的broker，那么，只配置一个broker不就行了吗？&lt;/li&gt;
  &lt;li&gt;如果不能识别出未配置的broker，那Kafka集群中动态增加了broker，岂不是需要重新启动flume？（因为&lt;code&gt;metadata.broker.list&lt;/code&gt;实际上是flume的配置，要更新这一参数配置，就需要重启flume）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;思考2&lt;/strong&gt;：如果network interrupt，producer会如何动作？记录log？还是抛出异常？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;思考3&lt;/strong&gt;：如果某一个flume挂了，能否能自动重启？&lt;/p&gt;

&lt;h2 id=&quot;producer-1&quot;&gt;Producer配置的详细参数&lt;/h2&gt;

&lt;p&gt;针对Kafka 0.8.1版本，这一部分介绍的Producer配置信息，主要参考两个地方：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;Kafka Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html#producerconfigs&quot;&gt;Kafka Producer Config&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Essential configuration properties for the producer include:（Producer必须的参数有几个，如下）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list，broker列表；&lt;/li&gt;
  &lt;li&gt;request.required.acks，broker收到producer发来的message后，是否ack？&lt;/li&gt;
  &lt;li&gt;producer.type，这个什么滴干活？&lt;/li&gt;
  &lt;li&gt;serializer.class，message从producer发往broker的过程中，需要序列化；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体参数如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Property
    &lt;ul&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metadata.broker.list
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas). The socket connections for sending the actual data will be established based on the broker information returned in the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a subset of brokers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;request.required.acks
    &lt;ul&gt;
      &lt;li&gt;0	&lt;/li&gt;
      &lt;li&gt;This value controls when a produce request is considered completed. Specifically, how many other brokers must have committed the data to their log and acknowledged this to the leader? Typical values are&lt;/li&gt;
      &lt;li&gt;0, which means that the producer never waits for an acknowledgement from the broker (the same behavior as 0.7). This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails).（不等待ack信息）&lt;/li&gt;
      &lt;li&gt;1, which means that the producer gets an acknowledgement after the leader replica has received the data. This option provides better durability as the client waits until the server acknowledges the request as successful (only messages that were written to the now-dead leader but not yet replicated will be lost).（leader完成数据写入）&lt;/li&gt;
      &lt;li&gt;-1, which means that the producer gets an acknowledgement after all in-sync replicas have received the data. This option provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.（所有replica都完成数据写入）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;request.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;10000&lt;/li&gt;
      &lt;li&gt;The amount of time the broker will wait trying to meet the &lt;code&gt;request.required.acks&lt;/code&gt; requirement before sending back an error to the client.（broker收到producer发来的message后，如果需要返回ack信息，那这个参数设定了broker返回ack信息的时间限制，如果超过这个时间，则broker向producer返回一个error信息）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;producer.type	
    &lt;ul&gt;
      &lt;li&gt;sync&lt;/li&gt;
      &lt;li&gt;This parameter specifies whether the messages are sent asynchronously in a background thread. Valid values are (1) &lt;code&gt;async&lt;/code&gt; for asynchronous send and (2) &lt;code&gt;sync&lt;/code&gt; for synchronous send. By setting the producer to async we allow batching together of requests (which is great for throughput) but open the possibility of a failure of the client machine dropping unsent data.（producer发送message的方式：同步、异步；设置为异步时，producer处理的吞吐量会提升，但可能丢失数据）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;serializer.class	
    &lt;ul&gt;
      &lt;li&gt;kafka.serializer.DefaultEncoder&lt;/li&gt;
      &lt;li&gt;The serializer class for messages. The default encoder takes a &lt;code&gt;byte[]&lt;/code&gt; and returns the same &lt;code&gt;byte[]&lt;/code&gt;.（message的序列化方法，默认是&lt;code&gt;byte[]&lt;/code&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;key.serializer.class
    &lt;ul&gt;
      &lt;li&gt;defaults to the same as for messages if nothing is given.&lt;/li&gt;
      &lt;li&gt;The serializer class for keys .&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;partitioner.class
    &lt;ul&gt;
      &lt;li&gt;kafka.producer.DefaultPartitioner（The default partitioner is based on the hash of the key.）&lt;/li&gt;
      &lt;li&gt;The partitioner class for partitioning messages amongst sub-topics. （将message放入哪个partition中）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;compression.codec	
    &lt;ul&gt;
      &lt;li&gt;none	&lt;/li&gt;
      &lt;li&gt;This parameter allows you to specify the compression codec for all data generated by this producer. Valid values are “none”, “gzip” and “snappy”.（producer向broker发送的信息，是否进行压缩，包含：key、message信息。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;compressed.topics	
    &lt;ul&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;This parameter allows you to set whether compression should be turned on for particular topics. If the compression codec is anything other than NoCompressionCodec, enable compression only for specified topics if any. If the list of compressed topics is empty, then enable the specified compression codec for all topics. If the compression codec is NoCompressionCodec, compression is disabled for all topics（当开启&lt;code&gt;compression.codec&lt;/code&gt;时，通过设置&lt;code&gt;compressed.topics&lt;/code&gt;，设置只针对某些特定的topic进行压缩，默认，对所有的topic都进行压缩）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;message.send.max.retries
    &lt;ul&gt;
      &lt;li&gt;3	&lt;/li&gt;
      &lt;li&gt;This property will cause the producer to automatically retry a failed send request. This property specifies the number of retries when such failures occur. Note that setting a non-zero value here can lead to duplicates in the case of network errors that cause a message to be sent but the acknowledgement to be lost.（当producer发送message失败后，尝试重新发送的次数；&lt;strong&gt;特别说明&lt;/strong&gt;：如果message发送成功，但broker返回的ack信息丢失时，会有message重发，即，此处有消息重复发送）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;retry.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;100&lt;/li&gt;
      &lt;li&gt;Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.（producer在进行重新发送message之前，都会refresh metadata，主要目标，查看是否更新了topic的leader；因为leader election需要一段时间，因此，在refresh metadata之前，需要等待一段时间，&lt;code&gt;retry.backoff.ms&lt;/code&gt;参数设置的就是等待的时间，等待选出新的topic leader）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;topic.metadata.refresh.interval.ms	
    &lt;ul&gt;
      &lt;li&gt;600 * 1000	&lt;/li&gt;
      &lt;li&gt;The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). （正常情况，多长时间刷新一次broker metadata，即，刷新间隔）&lt;/li&gt;
      &lt;li&gt;If you set this to a &lt;code&gt;negative value&lt;/code&gt;, metadata will only get refreshed on failure. （&lt;code&gt;&amp;lt;0&lt;/code&gt;时，仅当发送message失败时，才刷新）&lt;/li&gt;
      &lt;li&gt;If you set this to zero, the metadata will get refreshed after each message sent (not recommended). （&lt;code&gt;0&lt;/code&gt;，每次发送完message之后，都刷新，&lt;strong&gt;不推荐&lt;/strong&gt;）&lt;/li&gt;
      &lt;li&gt;Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed（&lt;strong&gt;重要提示&lt;/strong&gt;：无论设置刷新间隔为多少，具体刷新metadata都发生在producer发送message之后，因此，如果一直没有message发送，就不会有metadata刷新）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queue.buffering.max.ms
    &lt;ul&gt;
      &lt;li&gt;5000&lt;/li&gt;
      &lt;li&gt;Maximum time to buffer data when using &lt;code&gt;async&lt;/code&gt; mode. For example a setting of 100 will try to batch together 100ms of messages to send at once. This will improve throughput but adds message delivery latency due to the buffering.（当使用&lt;code&gt;producer.type&lt;/code&gt;为async模式时，这一参数才有用，含义：一时间为单位，将这一时间单位内的message一起发送给broker，这样有利于提高throughput，但会增加时延。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queue.buffering.max.messages
    &lt;ul&gt;
      &lt;li&gt;10000&lt;/li&gt;
      &lt;li&gt;The maximum number of unsent messages that can be queued up the producer when using async mode before either the producer must be blocked or data must be dropped.（当&lt;code&gt;producer.type&lt;/code&gt;使用async时，producer能够缓存的unsent message的数量，如果超过这一数量，producer就会blocked？message就会被丢弃？具体什么情况？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;queue.enqueue.timeout.ms	
    &lt;ul&gt;
      &lt;li&gt;-1	&lt;/li&gt;
      &lt;li&gt;The amount of time to block before dropping messages when running in async mode and the buffer has reached queue.buffering.max.messages. （当&lt;code&gt;queue.buffering.max.message&lt;/code&gt;设定的值已经触顶，等待多久block，之后就开始丢弃message）&lt;/li&gt;
      &lt;li&gt;If set to 0 events will be enqueued immediately or dropped if the queue is full (the producer send call will never block). &lt;/li&gt;
      &lt;li&gt;If set to -1 the producer will block indefinitely and never willingly drop a send.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch.num.messages
    &lt;ul&gt;
      &lt;li&gt;200&lt;/li&gt;
      &lt;li&gt;The number of messages to send in one batch when using async mode. The producer will wait until either this number of messages are ready to send or queue.buffer.max.ms is reached.（在&lt;code&gt;async&lt;/code&gt;模式下，当message数量达到&lt;code&gt;batch.num.messages&lt;/code&gt;时，或者，当等待时间达到&lt;code&gt;queue.buffer.max.ms&lt;/code&gt;时，producer都会发送一次缓存的message）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;send.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;100 * 1024&lt;/li&gt;
      &lt;li&gt;Socket write buffer size（socket写缓存的大小）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;client.id	
    &lt;ul&gt;
      &lt;li&gt;””&lt;/li&gt;
      &lt;li&gt;The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.（用户自己定义的producer标识，会伴随发送的message一起发送，用于追踪message的来源）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details about producer configuration can be found in the scala class &lt;code&gt;kafka.producer.ProducerConfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：几个新的理解：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list：本质是从一些broker中请求metadata（topic、partition、replicas），而真正的socket链接，是根据收到的metadata来进行的；因此，可以只配置一部分的broker，或者说只配置部分VIP broker，必要时，探查此深层的原因；每一个broker上都保存了整个Kafka cluster的完整metadata吗？&lt;/li&gt;
  &lt;li&gt;producer.type：sync、async，当设置为async时，能够提升吞吐量，但是会丢失数据？丢失，不能重发吗？&lt;/li&gt;
  &lt;li&gt;request.required.acks：设置是否需要broker在完成数据写入后，向producer返回ack信息；一个问题：如果broker上数据写入失败，那，producer会进行重发吗？有没有类似的机制？&lt;/li&gt;
  &lt;li&gt;queue.enqueue.timeout.me：其中说明的producer block什么含义？还会继续缓存未发送的message吗？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：突然想到一个问题，记录一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Server上，有进程监听port后，在服务器上无法再启动一个进程来监听这一port；&lt;/li&gt;
  &lt;li&gt;在远端通过telnet能够与server上这一port建立连接，并且，多个client都能与server上这一port建立连接；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个问题，我不知到深层原因，归根结底是对socket建立的底层原因不清晰。&lt;/p&gt;

&lt;h2 id=&quot;new-producer-configs&quot;&gt;New Producer Configs（补充）&lt;/h2&gt;

&lt;p&gt;下面是今后Kafka Producer会采用的新的配置参数，当前，可以有一个基本的了解。&lt;/p&gt;

&lt;p&gt;We are working on a replacement for our existing producer. The code is available in trunk now and can be considered beta quality. Below is the configuration for the new producer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name
    &lt;ul&gt;
      &lt;li&gt;Type&lt;/li&gt;
      &lt;li&gt;Default&lt;/li&gt;
      &lt;li&gt;Importance&lt;/li&gt;
      &lt;li&gt;Description&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;bootstrap.servers
    &lt;ul&gt;
      &lt;li&gt;list&lt;/li&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. Data will be load balanced over all servers irrespective of which servers are specified here for bootstrapping—this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form host1:port1,host2:port2,…. Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down). If no server in this list is available sending data will fail until on becomes available.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;acks
    &lt;ul&gt;
      &lt;li&gt;string&lt;/li&gt;
      &lt;li&gt;1&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are common:&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;acks=0&lt;/code&gt; If set to zero then the producer will not wait for any acknowledgment from the server at all. The record will be immediately added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won’t generally know of any failures). The offset given back for each record will always be set to -1.&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;acks=1&lt;/code&gt; This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.（leader将message写入local log后，直接返回ack信息；如果leader，返回ack信息后，leader宕机了，那其他follwer上并没有这条message，将导致message丢失）&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;acks=all&lt;/code&gt; This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.&lt;/li&gt;
      &lt;li&gt;Other settings such as acks=2 are also possible, and will require the given number of acknowledgements but this is generally less useful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;buffer.memory
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;33554432&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will either block or throw an exception based on the preference specified by &lt;code&gt;block.on.buffer.full&lt;/code&gt;.（用于存储未发送出去的message，当producer接收到的message速度大于发送message速度时，producer will block，或者抛出异常）&lt;/li&gt;
      &lt;li&gt;This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests.（&lt;strong&gt;什么含义&lt;/strong&gt;？需仔细琢磨）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;compression.type
    &lt;ul&gt;
      &lt;li&gt;string&lt;/li&gt;
      &lt;li&gt;none&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, or snappy. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;retries
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;0&lt;/li&gt;
      &lt;li&gt;high&lt;/li&gt;
      &lt;li&gt;Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries will potentially change the ordering of records because if two records are sent to a single partition, and the first fails and is retried but the second succeeds, then the second record may appear first.（设定，message尝试重发的次数；这个重发机制，可能会改变message之间的相互顺序）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch.size
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;16384&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes.（将发送到同一partition的多条message集中起来发送，构成一个batch）&lt;/li&gt;
      &lt;li&gt;No attempt will be made to batch records larger than this size.&lt;/li&gt;
      &lt;li&gt;Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent.（发送给broker的request包含多个batch？每一个batch对应一个partition）&lt;/li&gt;
      &lt;li&gt;A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;client.id
    &lt;ul&gt;
      &lt;li&gt;string&lt;/li&gt;
      &lt;li&gt;null&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included with the request. The application can set any string it wants as this has no functional purpose other than in logging and metrics.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;linger.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;0&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay—that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle’s algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will ‘linger’ for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absense of load.（当producer收到一个message后，不直接发送出去，而是，等待&lt;code&gt;linger.ms&lt;/code&gt;时间，目的：相同partition的多个message同时发送。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;max.request.size
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;1048576&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The maximum size of a request. This is also effectively a cap on the maximum record size. Note that the server has its own cap on record size which may be different from this. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.（限制单个request的大小）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;receive.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;32768&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The size of the TCP receive buffer to use when reading data（上层读取TCP数据时，一次读取的缓冲单元？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;send.buffer.bytes
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;131072&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The size of the TCP send buffer to use when sending data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;timeout.ms
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;30000&lt;/li&gt;
      &lt;li&gt;medium&lt;/li&gt;
      &lt;li&gt;The configuration controls the maximum amount of time the server will wait for acknowledgments from followers to meet the acknowledgment requirements the producer has specified with the acks configuration. If the requested number of acknowledgments are not met when the timeout elapses an error will be returned. This timeout is measured on the server side and does not include the network latency of the request.（server等待follower返回ack信息的时间，这个时间是指server端的时间）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;block.on.buffer.full
    &lt;ul&gt;
      &lt;li&gt;boolean&lt;/li&gt;
      &lt;li&gt;true&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;When our memory buffer is exhausted we must either stop accepting new records (block) or throw errors. By default this setting is true and we block, however in some scenarios blocking is not desirable and it is better to immediately give an error. Setting this to false will accomplish that: the producer will throw a BufferExhaustedException if a recrord is sent and the buffer space is full.（默认&lt;code&gt;true&lt;/code&gt;，当memory buffer中内容满了之后，producer不再接收新的message；如果设置为&lt;code&gt;false&lt;/code&gt;，则当memory buffer中内容满了之后，producer会直接抛出异常&lt;code&gt;BufferExhaustedException&lt;/code&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metadata.fetch.timeout.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;60000&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The first time data is sent to a topic we must fetch metadata about that topic to know which servers host the topic’s partitions. This configuration controls the maximum amount of time we will block waiting for the metadata fetch to succeed before throwing an exception back to the client.（当第一次向topic中传入数据时，需要从server请求metadata，参数&lt;code&gt;metadata.fetch.timeout.ms&lt;/code&gt;设定了发送metadata请求后，producer等待的时间，如果超时，则抛出异常。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metadata.max.age.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;300000&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The period of time in milliseconds after which we force a refresh of metadata even if we haven’t seen any partition leadership changes to proactively discover any new brokers or partitions.（定期请求metadata的时常）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metric.reporters
    &lt;ul&gt;
      &lt;li&gt;list&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;A list of classes to use as metrics reporters. Implementing the &lt;code&gt;MetricReporter&lt;/code&gt; interface allows plugging in classes that will be notified of new metric creation. The &lt;code&gt;JmxReporter&lt;/code&gt; is always included to register &lt;code&gt;JMX&lt;/code&gt; statistics.（&lt;strong&gt;什么含义&lt;/strong&gt;？生成测试报告？测试什么？为什么要测试？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metrics.num.samples
    &lt;ul&gt;
      &lt;li&gt;int&lt;/li&gt;
      &lt;li&gt;2&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The number of samples maintained to compute metrics.（计算指标时，保留的samples的个数）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;metrics.sample.window.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;30000&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The metrics system maintains a configurable number of samples over a fixed window size. This configuration controls the size of the window. For example we might maintain two samples each measured over a 30 second period. When a window expires we erase and overwrite the oldest window.（选出sample的window大小）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;reconnect.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;10&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The amount of time to wait before attempting to reconnect to a given host when a connection fails. This avoids a scenario where the client repeatedly attempts to connect to a host in a tight loop.（当与一个host断开连接后，等待多长时间，再去进行连接，避免过于频繁的无效连接）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;retry.backoff.ms
    &lt;ul&gt;
      &lt;li&gt;long&lt;/li&gt;
      &lt;li&gt;100&lt;/li&gt;
      &lt;li&gt;low&lt;/li&gt;
      &lt;li&gt;The amount of time to wait before attempting to retry a failed produce request to a given topic partition. This avoids repeated sending-and-failing in a tight loop.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;metrics&lt;/code&gt;的含义？为什么有这个？干什么的？&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/thilinamb/flume-ng-kafka-sink&quot;&gt;flume-ng-kafka-sink&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+Producer+Example&quot;&gt;0.8.0 Producer Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/08/configuration.html&quot;&gt;Kafka Configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;人是有差异的，特别是视野上的差异，有些东西，如果一个人没有见识过，同时想象力也不行，或者说胆小如鼠不敢想象，这样的人脑袋不行、胆子也不行，要疏远；另，信任是金子，别人对我的绝对信任，我对别人的绝对信任，都是很难建立的，要如同珍惜脑袋一样，珍惜这些信任。（注：绝对信任：无论做什么事，都相信是在做一件值得做的事，无论速度、计划怎样，都是信任，甚至当有流言蜚语产生时，都能力排众议对其信任。这种信任，大都是建立在对人格的熟知上。）&lt;/p&gt;

&lt;p&gt;整理东西，突然想到：做一件事，怎样才能做成？做事情需要几个条件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;做事的方向对不对？&lt;/li&gt;
  &lt;li&gt;做事的人脑袋是否灵光？&lt;/li&gt;
  &lt;li&gt;做事的人，是否投入了足够的时间？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对上面的几点，大概说一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;做事的方向对不对&lt;/strong&gt;，只要针对做的事情，当前能够达到基本一致，方向基本确定，而不是一团乱麻，那就可以开始做下去了，而在后期的过程中，可能会涉及到多次调整、迭代，这些都是可以预见的；&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;做事的人脑袋是否灵光&lt;/strong&gt;，事情是由人来做的，做的人脑袋行不行？基础理论、基本操作技能，基本的世界观：劳动创造价值，获得报酬；还是，跟着大牛有肉吃？（这本质是希望拿牛人的劳动价值，换取自己的报酬）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;做事的人时间投入&lt;/strong&gt;，天资尚可的人就行了，团队高低档次的人都需要，但是，有一点，如果不投入时间，或者时间很少，那又如何保证产出？特别是针对以前就没有涉足的领域，需要不畏艰险、持续的投入时间，才能有所理解、有所深入；脑袋还可以，但做事不投入充足时间的人，不是傻就是懒。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今天突然想起一件事，几年前，跟某位好友一起走路，无意间说起坚韧这种性格，我就问道：如果要在午门城墙上打一个洞，如何才能做到？谈到锲而不舍，如果一个人没有这种精神，那遇到困难的事情，就难办了；后来又说起，今后工作的打算，我们基本达成一致：精挑细选公司，一旦入门后，就当自己是公司的创始人，然后，返老还童，恢复到20多岁年轻小伙儿的年纪 ，只不过，返老还童的代价是放弃对于公司的所有权、职务等，以这种心态去工作，重塑自己的公司、再造辉煌，可以说想象还是比较大胆的；基于这种定位，每次做事，都是创始人心态，全力做好。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm 0.9.2：Trident</title>
     <link href="http://ningg.github.com/storm-trident"/>
     <updated>2014-11-06T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-trident</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Trident-tutorial.html&quot;&gt;Trident tutorial&lt;/a&gt;，本文采用&lt;code&gt;英文原文+中文批注&lt;/code&gt;方式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Trident is a high-level abstraction for doing realtime computing on top of Storm. It allows you to seamlessly intermix high throughput (millions of messages per second), stateful stream processing with low latency distributed querying. If you’re familiar with high level batch processing tools like &lt;code&gt;Pig&lt;/code&gt; or &lt;code&gt;Cascading&lt;/code&gt;, the concepts of &lt;code&gt;Trident&lt;/code&gt; will be very familiar – Trident has &lt;strong&gt;joins&lt;/strong&gt;, &lt;strong&gt;aggregations&lt;/strong&gt;, &lt;strong&gt;grouping&lt;/strong&gt;, &lt;strong&gt;functions&lt;/strong&gt;, and &lt;strong&gt;filters&lt;/strong&gt;. In addition to these, Trident adds primitives for doing stateful, incremental processing on top of any database or persistence store. Trident has consistent, exactly-once semantics, so it is easy to reason about Trident topologies.&lt;/p&gt;

&lt;p&gt;Trident，说几点：&lt;/p&gt;

&lt;p&gt;能够支撑高吞吐量、有状态stream的低延迟分布式查询；
与批量处理工具类似&lt;code&gt;Pig&lt;/code&gt;、&lt;code&gt;Cascading&lt;/code&gt;，Trident包含：&lt;strong&gt;joins&lt;/strong&gt;, &lt;strong&gt;aggregations&lt;/strong&gt;, &lt;strong&gt;grouping&lt;/strong&gt;, &lt;strong&gt;functions&lt;/strong&gt;, and &lt;strong&gt;filters&lt;/strong&gt;一系列操作；
Trident能够支撑stateful、incremental processing；
Trident支撑consistent、exactly-once semantics；&lt;/p&gt;

&lt;h2 id=&quot;illustrative-example&quot;&gt;Illustrative example&lt;/h2&gt;

&lt;p&gt;Let’s look at an illustrative example of Trident. This example will do two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute streaming word count from an input stream of sentences&lt;/li&gt;
  &lt;li&gt;Implement queries to get the sum of the counts for a list of words&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例子做两件事：&lt;/p&gt;

&lt;p&gt;统计一个输入stream中的word；
查询一组输入word的统计结果；&lt;/p&gt;

&lt;p&gt;For the purposes of illustration, this example will read an infinite stream of sentences from the following source:
（从如下source中读取数据，进行处理）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
FixedBatchSpout spout = new FixedBatchSpout(new Fields(&quot;sentence&quot;), 3, 
					new Values(&quot;the cow jumped over the moon&quot;), 
					new Values(&quot;the man went to the store and bought some candy&quot;), 
					new Values(&quot;four score and seven years ago&quot;), 
					new Values(&quot;how many apples can you eat&quot;));

spout.setCycle(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：spout中&lt;code&gt;setCycle()&lt;/code&gt;的含义。&lt;/p&gt;

&lt;p&gt;This spout cycles through that set of sentences over and over to produce the sentence stream. Here’s the code to do the streaming word count part of the computation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
TridentTopology topology = new TridentTopology(); 
TridentState wordCounts = topology.newStream(&quot;spout1&quot;, spout)
				.each(new Fields(&quot;sentence&quot;), new Split(), new Fields(&quot;word&quot;))
				.groupBy(new Fields(&quot;word&quot;))
				.persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields(&quot;count&quot;))
				.parallelismHint(6);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s go through the code line by line. First a TridentTopology object is created, which exposes the interface for constructing Trident computations. TridentTopology has a method called newStream that creates a new stream of data in the topology reading from an input source. In this case, the input source is just the FixedBatchSpout defined from before. Input sources can also be queue brokers like Kestrel or Kafka. Trident keeps track of a small amount of state for each input source (metadata about what it has consumed) in Zookeeper, and the “spout1” string here specifies the node in Zookeeper where Trident should keep that metadata.&lt;/p&gt;

&lt;p&gt;几个点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TridentTopology类：提供构造Trident computation的接口；&lt;/li&gt;
  &lt;li&gt;newStream()方法：从一个Spout中读取数据，构造Stream；
    &lt;ul&gt;
      &lt;li&gt;上述例子中，使用FixedBatchSpout作为数据源(Source);&lt;/li&gt;
      &lt;li&gt;上面Spout也可使用queue broker代替，例，Kestrel、Kafka；&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;newStream(&quot;spout1&quot;, spout)&lt;/code&gt;，其中&lt;code&gt;spout1&lt;/code&gt;标识了在zookeeper中当前spout存储&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Trident在Zookeeper中记录了每个Spout的处理状态数据（metadata：Spout中数据处理进展）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trident processes the stream as small batches of tuples. For example, the incoming stream of sentences might be divided into batches like so:&lt;/p&gt;

&lt;p&gt;Trident将Stream中的tuple分割为一些小的batch，按照batch来进行处理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-trident/batched-stream.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Generally the size of those small batches will be on the order of thousands or millions of tuples, depending on your incoming throughput. 
（通常，将相邻的tuple组合成一个batch，通过调整tuple的输入Storm顺序，可实现将类似的tuple放入相同的batch）&lt;/p&gt;

&lt;p&gt;Trident provides a fully fledged batch processing API to process those small batches. The API is very similar to what you see in high level abstractions for Hadoop like Pig or Cascading: you can do group by’s, joins, aggregations, run functions, run filters, and so on. Of course, processing each small batch in isolation isn’t that interesting, so Trident provides functions for doing aggregations across batches and persistently storing those aggregations – whether in memory, in Memcached, in Cassandra, or some other store. Finally, Trident has first-class functions for querying sources of realtime state. That state could be updated by Trident (like in this example), or it could be an independent source of state.
（Trident提供了处理batch的API，这些API与Pig、Cascading的处理类似）&lt;/p&gt;

&lt;p&gt;Back to the example, the spout emits a stream containing one field called “sentence”. The next line of the topology definition applies the Split function to each tuple in the stream, taking the “sentence” field and splitting it into words. Each sentence tuple creates potentially many word tuples – for instance, the sentence “the cow jumped over the moon” creates six “word” tuples. Here’s the definition of &lt;code&gt;Split&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：通过&lt;code&gt;TridentTopology#newStream()&lt;/code&gt;将Spout中tuple构造为stream时，也可以进行干预（定制），即，将Spout中读取的原始tuple转换为其他格式的tuple。&lt;/p&gt;

&lt;p&gt;// java 
public class Split extends BaseFunction { &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void execute(TridentTuple tuple, TridentCollector collector) { 
	String sentence = tuple.getString(0);
	for(String word: sentence.split(&quot; &quot;)) { 
		collector.emit(new Values(word)); 
	} 
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;As you can see, it’s really simple. It simply grabs the sentence, splits it on whitespace, and emits a tuple for each word.&lt;/p&gt;

&lt;p&gt;The rest of the topology computes word count and keeps the results persistently stored. First the stream is grouped by the “word” field. Then, each group is persistently aggregated using the Count aggregator. The &lt;code&gt;persistentAggregate&lt;/code&gt; function knows how to store and update the results of the aggregation in a source of state. In this example, the word counts are kept in memory, but this can be trivially swapped to use Memcached, Cassandra, or any other persistent store. Swapping this topology to store counts in Memcached is as simple as replacing the persistentAggregate line with this (using &lt;a href=&quot;https://github.com/nathanmarz/trident-memcached&quot;&gt;trident-memcached&lt;/a&gt;), where the “serverLocations” is a list of host/ports for the Memcached cluster:
（&lt;code&gt;persistentAggregate&lt;/code&gt;函数，负责存储和更新aggregation result，其中&lt;code&gt;MemoryMapState.Factory()&lt;/code&gt;表示利用内存存储，也可以使用Memcached、Cassandra以及其他的持久化数据库）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// MemcachedState.transactional()
.persistentAggregate(MemcachedState.transactional(serverLocations), new Count(), new Fields(&quot;count&quot;)) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The values stored by persistentAggregate represents the aggregation of all batches ever emitted by the stream.&lt;/p&gt;

&lt;p&gt;One of the cool things about Trident is that it has fully fault-tolerant, exactly-once processing semantics. This makes it easy to reason about your realtime processing. Trident persists state in a way so that if failures occur and retries are necessary, it won’t perform multiple updates to the database for the same source data.
（Trident最迷人的一点：fully fault-tolerant、exactly-once processing semantics）&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;persistentAggregate&lt;/code&gt; method transforms a &lt;code&gt;Stream&lt;/code&gt; into a &lt;code&gt;TridentState object&lt;/code&gt;. In this case the TridentState object represents all the word counts. We will use this TridentState object to implement the distributed query portion of the computation.
（&lt;code&gt;persistentAggregate&lt;/code&gt;方法将Stream转换为TridentState Object，其用于实现distributed query）&lt;/p&gt;

&lt;p&gt;The next part of the topology implements a low latency distributed query on the word counts. The query takes as input a whitespace separated list of words and return the sum of the counts for those words. These queries are executed just like normal RPC calls, except they are parallelized in the background. Here’s an example of how you might invoke one of these queries:
（topology的next part实现了一个low latency、distributed query：接收输入的word，并返回这些word的统计次数。实际上，这些query看起来就是normal RPC calls，只是他们在背后是并行执行的。下面是调用query的方法）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
DRPCClient client = new DRPCClient(&quot;drpc.server.location&quot;, 3772); 
// prints the JSON-encoded result, e.g.: &quot;[[5078]]&quot;
System.out.println(client.execute(&quot;words&quot;, &quot;cat dog the man&quot;); 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, it looks just like a regular remote procedure call (RPC), except it’s executing in parallel across a Storm cluster. The latency for small queries like this are typically around 10ms. More intense DRPC queries can take longer of course, although the latency largely depends on how many resources you have allocated for the computation.&lt;/p&gt;

&lt;p&gt;The implementation of the distributed query portion of the topology looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
topology.newDRPCStream(&quot;words&quot;)
	   .each(new Fields(&quot;args&quot;), new Split(), new Fields(&quot;word&quot;)) 
	   .groupBy(new Fields(&quot;word&quot;)) 
	   .stateQuery(wordCounts, new Fields(&quot;word&quot;), new MapGet(), new Fields(&quot;count&quot;)) 
	   .each(new Fields(&quot;count&quot;), new FilterNull()) 
	   .aggregate(new Fields(&quot;count&quot;), new Sum(), new Fields(&quot;sum&quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same TridentTopology object is used to create the DRPC stream, and the function is named “words”. The function name corresponds to the function name given in the first argument of execute when using a DRPCClient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：DRPC，distributed RPC？&lt;/p&gt;

&lt;p&gt;Each DRPC request is treated as its own little batch processing job that takes as input a single tuple representing the request. The tuple contains one field called “args” that contains the argument provided by the client. In this case, the argument is a whitespace separated list of words.&lt;/p&gt;

&lt;p&gt;First, the Split function is used to split the arguments for the request into its constituent words. The stream is grouped by “word”, and the stateQuery operator is used to query the TridentState object that the first part of the topology generated. stateQuery takes in a source of state – in this case, the word counts computed by the other portion of the topology – and a function for querying that state. In this case, the MapGet function is invoked, which gets the count for each word. Since the DRPC stream is grouped the exact same way as the TridentState was (by the “word” field), each word query is routed to the exact partition of the TridentState object that manages updates for that word.&lt;/p&gt;

&lt;p&gt;Next, words that didn’t have a count are filtered out via the FilterNull filter and the counts are summed using the Sum aggregator to get the result. Then, Trident automatically sends the result back to the waiting client.&lt;/p&gt;

&lt;p&gt;Trident is intelligent about how it executes a topology to maximize performance. There’s two interesting things happening automatically in this topology:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Operations that read from or write to state (like persistentAggregate and stateQuery) automatically batch operations to that state. So if there’s 20 updates that need to be made to the database for the current batch of processing, rather than do 20 read requests and 20 writes requests to the database, Trident will automatically batch up the reads and writes, doing only 1 read request and 1 write request (and in many cases, you can use caching in your State implementation to eliminate the read request). So you get the best of both words of convenience – being able to express your computation in terms of what should be done with each tuple – and performance.&lt;/li&gt;
  &lt;li&gt;Trident aggregators are heavily optimized. Rather than transfer all tuples for a group to the same machine and then run the aggregator, Trident will do partial aggregations when possible before sending tuples over the network. For example, the Count aggregator computes the count on each partition, sends the partial count over the network, and then sums together all the partial counts to get the total count. This technique is similar to the use of combiners in MapReduce.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s look at another example of Trident.&lt;/p&gt;

&lt;h2 id=&quot;reach&quot;&gt;Reach&lt;/h2&gt;

&lt;p&gt;The next example is a pure DRPC topology that computes the reach of a URL on demand. Reach is the number of unique people exposed to a URL on Twitter. To compute reach, you need to fetch all the people who ever tweeted a URL, fetch all the followers of all those people, unique that set of followers, and that count that uniqued set. Computing reach is too intense for a single machine – it can require thousands of database calls and tens of millions of tuples. With Storm and Trident, you can parallelize the computation of each step across a cluster.&lt;/p&gt;

&lt;p&gt;This topology will read from two sources of state. One database maps URLs to a list of people who tweeted that URL. The other database maps a person to a list of followers for that person. The topology definition looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TridentState urlToTweeters = topology.newStaticState(getUrlToTweetersState()); 
TridentState tweetersToFollowers = topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream(“reach”) 
		.stateQuery(urlToTweeters, new Fields(“args”), new MapGet(), new Fields(“tweeters”)) 
		.each(new Fields(“tweeters”), new ExpandList(), new Fields(“tweeter”)) 
		.shuffle() 
		.stateQuery(tweetersToFollowers, new Fields(“tweeter”), new MapGet(), new Fields(“followers”)) 
		.parallelismHint(200) 
		.each(new Fields(“followers”), new ExpandList(), new Fields(“follower”)) 
		.groupBy(new Fields(“follower”)) 
		.aggregate(new One(), new Fields(“one”)) 
		.parallelismHint(20) 
		.aggregate(new Count(), new Fields(“reach”));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The topology creates TridentState objects representing each external database using the newStaticState method. These can then be queried in the topology. Like all sources of state, queries to these databases will be automatically batched for maximum efficiency.&lt;/p&gt;

&lt;p&gt;The topology definition is straightforward – it’s just a simple batch processing job. First, the urlToTweeters database is queried to get the list of people who tweeted the URL for this request. That returns a list, so the ExpandList function is invoked to create a tuple for each tweeter.&lt;/p&gt;

&lt;p&gt;Next, the followers for each tweeter must be fetched. It’s important that this step be parallelized, so shuffle is invoked to evenly distribute the tweeters among all workers for the topology. Then, the followers database is queried to get the list of followers for each tweeter. You can see that this portion of the topology is given a large parallelism since this is the most intense portion of the computation.&lt;/p&gt;

&lt;p&gt;Next, the set of followers is uniqued and counted. This is done in two steps. First a “group by” is done on the batch by “follower”, running the “One” aggregator on each group. The “One” aggregator simply emits a single tuple containing the number one for each group. Then, the ones are summed together to get the unique count of the followers set. Here’s the definition of the “One” aggregator:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class One implements CombinerAggregator { 

	public Integer init(TridentTuple tuple) { 
		return 1; 
	}

	public Integer combine(Integer val1, Integer val2) { 
		return 1; 
	}

	public Integer zero() { return 1; } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a “combiner aggregator”, which knows how to do partial aggregations before transferring tuples over the network to maximize efficiency. Sum is also defined as a combiner aggregator, so the global sum done at the end of the topology will be very efficient.&lt;/p&gt;

&lt;p&gt;Let’s now look at Trident in more detail.&lt;/p&gt;

&lt;h2 id=&quot;fields-and-tuples&quot;&gt;Fields and tuples&lt;/h2&gt;

&lt;p&gt;The Trident data model is the TridentTuple which is a named list of values. During a topology, tuples are incrementally built up through a sequence of operations. Operations generally take in a set of input fields and emit a set of “function fields”. The input fields are used to select a subset of the tuple as input to the operation, while the “function fields” name the fields the operation emits.&lt;/p&gt;

&lt;p&gt;Consider this example. Suppose you have a stream called “stream” that contains the fields “x”, “y”, and “z”. To run a filter MyFilter that takes in “y” as input, you would say:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.each(new Fields(&quot;y&quot;), new MyFilter())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Suppose the implementation of MyFilter is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyFilter extends BaseFilter { 

	public boolean isKeep(TridentTuple tuple){ 
		return tuple.getInteger(0) &amp;lt; 10; 
	} 

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will keep all tuples whose “y” field is less than 10. The TridentTuple given as input to MyFilter will only contain the “y” field. Note that Trident is able to project a subset of a tuple extremely efficiently when selecting the input fields: the projection is essentially free.&lt;/p&gt;

&lt;p&gt;Let’s now look at how “function fields” work. Suppose you had this function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class AddAndMultiply extends BaseFunction { 
	public void execute(TridentTuple tuple, TridentCollector collector) {
		int i1 = tuple.getInteger(0); 
		int i2 = tuple.getInteger(1); 
		collector.emit(new Values(i1 + i2, i1 * i2)); 
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function takes two numbers as input and emits two new values: the addition of the numbers and the multiplication of the numbers. Suppose you had a stream with the fields “x”, “y”, and “z”. You would use this function like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.each(new Fields(&quot;x&quot;, &quot;y&quot;), new AddAndMultiply(), new Fields(&quot;added&quot;, &quot;multiplied&quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of functions is additive: the fields are added to the input tuple. So the output of this each call would contain tuples with the five fields “x”, “y”, “z”, “added”, and “multiplied”. “added” corresponds to the first value emitted by AddAndMultiply, while “multiplied” corresponds to the second value.&lt;/p&gt;

&lt;p&gt;With aggregators, on the other hand, the function fields replace the input tuples. So if you had a stream containing the fields “val1” and “val2”, and you did this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.aggregate(new Fields(&quot;val2&quot;), new Sum(), new Fields(&quot;sum&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output stream would only contain a single tuple with a single field called “sum”, representing the sum of all “val2” fields in that batch.&lt;/p&gt;

&lt;p&gt;With grouped streams, the output will contain the grouping fields followed by the fields emitted by the aggregator. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream.groupBy(new Fields(&quot;val1&quot;)) .aggregate(new Fields(&quot;val2&quot;), new Sum(), new Fields(&quot;sum&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the output will contain the fields “val1” and “sum”.&lt;/p&gt;

&lt;h2 id=&quot;state&quot;&gt;State&lt;/h2&gt;

&lt;p&gt;A key problem to solve with realtime computation is how to manage state so that updates are idempotent in the face of failures and retries. It’s impossible to eliminate failures, so when a node dies or something else goes wrong, batches need to be retried. The question is – how do you do state updates (whether external databases or state internal to the topology) so that it’s like each message was only processed only once?&lt;/p&gt;

&lt;p&gt;This is a tricky problem, and can be illustrated with the following example. Suppose that you’re doing a count aggregation of your stream and want to store the running count in a database. If you store only the count in the database and it’s time to apply a state update for a batch, there’s no way to know if you applied that state update before. The batch could have been attempted before, succeeded in updating the database, and then failed at a later step. Or the batch could have been attempted before and failed to update the database. You just don’t know.&lt;/p&gt;

&lt;p&gt;Trident solves this problem by doing two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each batch is given a unique id called the “transaction id”. If a batch is retried it will have the exact same transaction id.&lt;/li&gt;
  &lt;li&gt;State updates are ordered among batches. That is, the state updates for batch 3 won’t be applied until the state updates for batch 2 have succeeded.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With these two primitives, you can achieve exactly-once semantics with your state updates. Rather than store just the count in the database, what you can do instead is store the transaction id with the count in the database as an atomic value. Then, when updating the count, you can just compare the transaction id in the database with the transaction id for the current batch. If they’re the same, you skip the update – because of the strong ordering, you know for sure that the value in the database incorporates the current batch. If they’re different, you increment the count.&lt;/p&gt;

&lt;p&gt;Of course, you don’t have to do this logic manually in your topologies. This logic is wrapped by the State abstraction and done automatically. Nor is your State object required to implement the transaction id trick: if you don’t want to pay the cost of storing the transaction id in the database, you don’t have to. In that case the State will have at-least-once-processing semantics in the case of failures (which may be fine for your application). You can read more about how to implement a State and the various fault-tolerance tradeoffs possible &lt;a href=&quot;http://storm.apache.org/documentation/Trident-state&quot;&gt;in this doc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A State is allowed to use whatever strategy it wants to store state. So it could store state in an external database or it could keep the state in-memory but backed by HDFS (like how HBase works). State’s are not required to hold onto state forever. For example, you could have an in-memory State implementation that only keeps the last X hours of data available and drops anything older. Take a look at the implementation of the &lt;a href=&quot;https://github.com/nathanmarz/trident-memcached/blob/master/src/jvm/trident/memcached/MemcachedState.java&quot;&gt;Memcached integration&lt;/a&gt; for an example State implementation.&lt;/p&gt;

&lt;h2 id=&quot;execution-of-trident-topologies&quot;&gt;Execution of Trident topologies&lt;/h2&gt;

&lt;p&gt;Trident topologies compile down into as efficient of a Storm topology as possible. Tuples are only sent over the network when a repartitioning of the data is required, such as if you do a groupBy or a shuffle. So if you had this Trident topology:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-trident/trident-to-storm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It would compile into Storm spouts/bolts like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-trident/trident-to-storm2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Trident makes realtime computation elegant. You’ve seen how high throughput stream processing, state manipulation, and low-latency querying can be seamlessly intermixed via Trident’s API. Trident lets you express your realtime computations in a natural way while still getting maximal performance.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;h2 id=&quot;section-1&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;今天几个人讨论某事，说来是鸡毛蒜皮的小事，不过也大小是个活动，通过及时通讯软件进行讨论，参与人员基本上都装死，很冷清，大部分人都是在看消息，不过内心里，大家都是愿意参加这么个活动的，这样只有一个人积极发言，没有交互、讨论，内心还是挺凄凉的，这种情况怎么办？时势造英雄，需要一个人花点时间整理一下活动的方方面面细则：时间、地点、人员、路线、内容、费用、事项（每项的负责人），然后出一个基本稿，发到群中，针对基本稿进行讨论，就有目标了；讨论结束再来个总结，算是定稿，妥妥的。&lt;/p&gt;

&lt;p&gt;说起这个，又想起今天早上一起去装机的事情了，&lt;code&gt;**&lt;/code&gt;去重装服务器，回来后说：已经装完了，并且测试外网能够访问。OK，他是个有良好习惯的人，不仅把事情做了，而且做了验证(测试)，确保事情做好了。想想之前我去重装系统，也是这么操作的，并且旁边站了2位同事，其中就有&lt;code&gt;**&lt;/code&gt;，当时安装完系统后，我要测试一下是否完成配置，他们两位没有说话，我想他们应该是支持花费时间进行测试的，因为他们并没有反对测试，我想在我要花时间测试的时候，肯定有一种人会说：都已经安装、配置好了，还测试什么，浪费时间，走吧。说了这么多，其实是想说一件事：人是有差异的，有优秀的人，有的就平庸，有的就是需要剔除的坏因素；识别优秀的人，亲近他们，疏离除此之外的任何人，年轻成长的时候，更是要如此。&lt;/p&gt;

&lt;p&gt;把事情做成，并测试事情是否已经做成，以此来确保已经把事情做成，这只是起步，往上还有空间：把事做成，确保做成，把事做好，确保做好。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>Eclipse下搭建Kafka的开发环境</title>
     <link href="http://ningg.github.com/kafka-dev-env-with-eclipse"/>
     <updated>2014-11-01T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-dev-env-with-eclipse</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近要进行Kafka开发，在&lt;a href=&quot;http://kafka.apache.org/code.html&quot;&gt;官网&lt;/a&gt;看到可以在IDE下开发，赶紧点进去看了看，并且在本地Eclipse下搭建了个Kafka的开发环境，主要参考来源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Developer+Setup&quot;&gt;Kafka Developer Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;编译环境(非代理)&lt;/h2&gt;

&lt;p&gt;查看自己机器的环境：我用笔记本来编译的，是win 7（x64）操作系统；更详细的编译环境信息通过如下方式查看：&lt;code&gt;CMD&lt;/code&gt;–&amp;gt;&lt;code&gt;systeminfo&lt;/code&gt;，这个命令收集系统信息，需要花费40s，稍等一会儿，得到如下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\Administrator&amp;gt;systeminfo

OS 名称:          Microsoft Windows 7 旗舰版
OS 版本:          6.1.7601 Service Pack 1 Build 7601

系统类型:         x64-based PC
处理器:           安装了 1 个处理器。
	 [01]: Intel64 Family 6 Model 23 Stepping 6 GenuineIntel ~785 Mhz

物理内存总量:     2,968 MB
可用的物理内存:   819 MB
虚拟内存: 最大值: 5,934 MB
虚拟内存: 可用:   2,196 MB
虚拟内存: 使用中: 3,738 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-2&quot;&gt;开始编译&lt;/h3&gt;

&lt;p&gt;需要提前下载几个东西：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka源码包：&lt;a href=&quot;http://kafka.apache.org/downloads.html&quot;&gt;kafka-0.8.1.1-src.tgz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse下的Scala 2.10.x IDE plugin：&lt;a href=&quot;http://scala-ide.org/download/current.html&quot;&gt;For Scala 2.10.4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse下的IvyIDE plugin：&lt;a href=&quot;http://ant.apache.org/ivy/ivyde/download.cgi&quot;&gt; apache-ivyde-2.2.0.final-201311091524-RELEASE.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eclipse&quot;&gt;Eclipse下安装插件&lt;/h3&gt;

&lt;p&gt;基本步骤：打开Eclipse–Help–Install new Software，具体见下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/install-new-software.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/install-plugins.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于IvyDE，如果上述办法添加插件出错，则，进行如下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IvyDE features &lt;code&gt;features/org.apache.ivyde.*.jar&lt;/code&gt; to put in your &lt;code&gt;$ECLIPSE_HOME/features&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;IvyDE plugins &lt;code&gt;plugins/org.apache.ivyde.*.jar&lt;/code&gt; to put in your &lt;code&gt;$ECLIPSE_HOME/plugins&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eclipse-project-file&quot;&gt;生成Eclipse project file&lt;/h3&gt;

&lt;p&gt;由于我的电脑是Windowns 7，因此安装了Cygwin，下面的操作都是在Cygwin下进行的，具体是，到Kafka源码包的路径下，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd $KAFKA_SRC_HOME
./gradlew eclipse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;kafkaeclipse&quot;&gt;kafka工程导入Eclipse&lt;/h3&gt;

&lt;p&gt;将上一步生成的project导入到Eclipse中，具体：&lt;code&gt;File&lt;/code&gt; -&amp;gt; &lt;code&gt;Import&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Existing Projects into Workspace&lt;/code&gt;，结果如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/kafka-src.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;问题及解决办法&lt;/h3&gt;

&lt;p&gt;上述kafka工程导入Eclipse后，实质是几个工程：perf、examples、core、contrib、clients；其中perf、core工程是scala工程，其余为java工程；但是examples工程中提示多个问题，列出一个看一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# The import kafka.consumer cannot be resolved
import kafka.consumer.ConsumerConfig;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述问题，在网上搜了一圈，最终StackOverFlow上找到了：&lt;a href=&quot;http://stackoverflow.com/questions/22102257/eclipse-scala-object-cannot-be-resolved&quot;&gt;Eclipse scala.object cannot be resolved&lt;/a&gt;，不过，上面的提示对于当前的问题，好像没有用；因为，examples工程以调用的是core工程的核心代码，而不是scala-library中的代码；并且，examples工程在&lt;code&gt;Java Build Path&lt;/code&gt;–&lt;code&gt;Projects&lt;/code&gt;(Required projects on the build path)中已经添加了core工程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-dev-env-with-eclipse/examples-core-build-path.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;怎么还是有错？奥，core工程是scala工程，而examples工程是java工程，并且在examples中引用的core中代码也都是*.scala代码，OK，将examples工程也转换为scala工程吧：点击examples工程，&lt;code&gt;右键&lt;/code&gt;–&lt;code&gt;Configure&lt;/code&gt;–&lt;code&gt;add Scala Nature&lt;/code&gt;，clean一下examples工程，OK，examples工程的错误没啦。不过core工程仍然还有错误，大意如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka.utils.nonthreadsafe  required: scala.annotation.Annotation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：打开core工程下&lt;code&gt;Annotations_2.8.scala&lt;/code&gt;文件，添加一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import scala.annotation.StaticAnnotation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;clean一下core工程，OK，这次总算搞定了，开始开发吧。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;编译环境(代理)&lt;/h2&gt;

&lt;p&gt;利用公司内网进行编译，开始之前，先查询一下机器配置，命令&lt;code&gt;CMD&lt;/code&gt;–&lt;code&gt;systeminfo&lt;/code&gt;，显示结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Documents and Settings\ningg&amp;gt;systeminfo

OS 名称:      Microsoft Windows XP Professional
OS 版本:      5.1.2600 Service Pack 3 Build 2600
OS 制造商:    Microsoft Corporation
OS 构件类型:  Multiprocessor Free
系统制造商:   LENOVO
系统型号:     ThinkCentre M6400t-N000
系统类型:     X86-based PC
处理器:       安装了 1 个处理器。
       [01]: x86 Family 6 Model 58 Stepping 9 GenuineIntel ~3392 Mhz
BIOS 版本:    LENOVO - 14f0
物理内存总量: 3,546 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;编译环境(代理)&lt;/strong&gt;与&lt;strong&gt;编译环境(非代理)&lt;/strong&gt;基本一致，不过需要配置一下代理，仅此而已，下面只列出两者有差异的地方。&lt;/p&gt;

&lt;h3 id=&quot;eclipse-project-file-1&quot;&gt;生成Eclipse project file&lt;/h3&gt;

&lt;p&gt;目标：通过代理方式要Cygwin能够联网，并且在Cygwin下编译Kafka对应的eclipse环境。&lt;/p&gt;

&lt;h4 id=&quot;cygwinhttp&quot;&gt;设置Cygwin的http代理&lt;/h4&gt;

&lt;p&gt;在Cygwin下无法上网，并且没有&lt;code&gt;wget&lt;/code&gt;命令，那好，重装一下Cygwin，并且安装过程中增加&lt;code&gt;wget&lt;/code&gt;工具；然后，配置Cygwin的代理：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# set http proxy
export http_proxy=http://username:passwd@ip:port

# test whether http_proxy is useful.
wget www.baidu.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照当前的设置，Cygwin环境就联网了。另，仍有个问题，每次重启Cygwin的终端窗口，都需要重新设置http_proxy，如何解决？&lt;/p&gt;

&lt;h4 id=&quot;gradle&quot;&gt;gradle代理设置&lt;/h4&gt;

&lt;p&gt;上述设置，并无法执行&lt;code&gt;./gradlew eclipse&lt;/code&gt;，具体出错信息是：无法下载一些maven的中央仓库中的依赖。gradle是第一次接触，说是与Maven功能类似的自动构建工具，细节不多说，既然是工具，应该跟Maven类似，可以配置代理，查询一下，OK，果真能设置，具体操作：在工程的根目录下，创建文件&lt;code&gt;gradle.properties&lt;/code&gt;，其中添加proxy设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemProp.http.proxyHost=IP
systemProp.http.proxyPort=port
systemProp.http.proxyUser=username
systemProp.http.proxyPassword=password
systemProp.http.nonProxyHosts=*.nonproxyrepos.com|localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置好代理之后，在Cgywin下执行命令&lt;code&gt;./gradlew eclipse&lt;/code&gt;即可；另外，也可在&lt;code&gt;gradle.properties&lt;/code&gt;中修改编译源码时，对应的scala版本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：gradle是什么，要有个基本的了解，当前&lt;a href=&quot;http://stevex.blog.51cto.com/4300375/1339735&quot;&gt;参考&lt;/a&gt;，其中提到两个地方：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gradle入门：&lt;a href=&quot;http://spring.io/guides/gs/gradle/&quot;&gt;http://spring.io/guides/gs/gradle/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;gradle官网有免费电子书：《Building and Testing with Gradle》&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;今天在公司，折腾一下午，网络问题，脑袋都大了，回来后，不到30mins就搞定了，顺便还整理了下，形成了此文；没有稳定的网络，对于有追求的工程师，就如同拿着锄头的特战队员，能力再牛，照样被拿微冲的小白恐吓。&lt;/p&gt;

&lt;p&gt;补充：今天（2014-11-03）到公司，还是不死心，重新尝试了一下在代理环境中编译Kafka源码；具体参考上面&lt;strong&gt;编译环境(代理)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;今天无意间看到某句话，贴出来，算是勉励自己：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;大部分事情，多用心、多用时间，都是可以超过大部分人的。人做事其实是习惯，只有这样的人，才是优秀的。人生里，要识别出这样的人，和优秀者接近，远离平庸。不要急，多练多体会，水平会逐步提高。我每次都是搜索菜谱，然后用心比较，选择正确的步骤整合。&lt;/p&gt;
&lt;/blockquote&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm 0.9.2：如何从Kafka读取数据</title>
     <link href="http://ningg.github.com/storm-with-kafka"/>
     <updated>2014-10-31T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-with-kafka</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;之前研读了&lt;a href=&quot;/in-stream-big-data-processing&quot;&gt;In-Stream Big Data Processing&lt;/a&gt;，组里将基于此实现一个实时的数据分析系统，基本选定三个组件：Flume、Kafka、Storm，其中，Flume负责数据采集，Kafka是一个MQ:负责数据采集与数据分析之间解耦，Storm负责进行流式处理。&lt;/p&gt;

&lt;p&gt;把这3个东西串起来，可以吗？可以的，之前整理了&lt;a href=&quot;/flume-with-kafka&quot;&gt;Flume与Kafka整合&lt;/a&gt;的文章，那Storm能够与Kafka整合吗？Storm官网有介绍：
&lt;a href=&quot;http://storm.apache.org/about/integrates.html&quot;&gt;Storm Integrates&lt;/a&gt;，其中给出了Storm与Kafka集成的&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/external/storm-kafk&quot;&gt;方案&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;storm&quot;&gt;回顾Storm&lt;/h2&gt;

&lt;p&gt;之前都是以原文+注释方式，来阅读Storm的官方文档，现在集中整理一下。Storm集群的构成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;包含两种节点：master和worker；&lt;/li&gt;
  &lt;li&gt;master上运行&lt;code&gt;Nimbus&lt;/code&gt;，负责：distribute code、assign task、monitor failutes；&lt;/li&gt;
  &lt;li&gt;worker上运行&lt;code&gt;Supervisor&lt;/code&gt;，负责：监听&lt;code&gt;Nimbus&lt;/code&gt;分配的任务，并启停worker precess；&lt;/li&gt;
  &lt;li&gt;zookeeper负责协调&lt;code&gt;Nimbus&lt;/code&gt;和&lt;code&gt;Supervisor&lt;/code&gt;之间的关系，所有状态信息都存储在zookeeper or local host；因此，重启Nimbus or Supervisor进程，对用户来说无影响；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/storm-cluster.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于 spout 和 bolt ，说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;spout（龙卷风、气旋）： source of stream，向topology中拉入数据的原点；&lt;/li&gt;
  &lt;li&gt;bolt（闪电）：处理 stream，通过run functions、filter tuples、do streaming aggregations、do streaming join、talk to database… 来做任何事情；&lt;/li&gt;
  &lt;li&gt;topology：由spout、bolt以及他们之间的关系构成，是client提交给Storm cluster执行的基本单元；&lt;/li&gt;
  &lt;li&gt;topology中所有node都是并发运行的，可以配置每个node的并发数；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：topology中node是什么概念？spout、bolt？master、worker？jvm process？thread？
&lt;strong&gt;RE&lt;/strong&gt;：master、worker对应Storm的node，master负责控制，worker负责具体执行；spout、bolt是逻辑上的，并且分布在不同的worker上；每个spout、bolt可配置并发数，这个并发数对应启动的thread；不同的spout、bolt对应不同的thread，thread间不能共用；这些所有的thread由所有的worker process来执行，举例，累计300个thread，启动了30个worker，则平均每个worker process对应执行10个thread（前面的说法对吗？哈哈）&lt;/p&gt;

&lt;p&gt;关于数据模型，即数据的结构，说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storm中，使用tuple结构来存储数据，tuple由fields构成，field可以为任意类型；&lt;/li&gt;
  &lt;li&gt;topology中spout、bolt必须声明其emit的tuple格式：&lt;code&gt;declareOutputFields()&lt;/code&gt;；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setSpout&lt;/code&gt;/&lt;code&gt;setBolt&lt;/code&gt;用于定义spout和bolt，输入参数：node id、processing logic、amount of parallelism；&lt;/li&gt;
  &lt;li&gt;processing logic对应类spout/bolt，需要implement &lt;code&gt;IRichSpout&lt;/code&gt;/&lt;code&gt;IRichBolt&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Storm有两种执行模式，&lt;code&gt;local mode&lt;/code&gt;和&lt;code&gt;distributed mode&lt;/code&gt;，补充几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;local mode，在本地的process中通过thread模拟worker，多用于testing和development topology；更多参考&lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;资料&lt;/a&gt;；&lt;/li&gt;
  &lt;li&gt;distributed mode，用户向master提交topology以及运行topology所需的所有code；master向worker分发topology；更多参考&lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;资料&lt;/a&gt;；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于Stream groupings，几点：&lt;/p&gt;

&lt;p&gt;*　stream grouping解决的问题：多个执行spout逻辑的thread都输出tuple，这些tuple要发送给bolt对应的多个thread，问题来了，tuple发给bolt的哪个thread？即，stream grouping解决：tuple在不同task之间传递关系；
*　shuffle grouping，随机分发；field grouping，根据给定的field进行分发；更多&lt;a href=&quot;http://storm.apache.org/documentation/Concepts.html&quot;&gt;参考&lt;/a&gt;；&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology-tasks.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;stromkafka&quot;&gt;Strom整合Kafka&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;版本信息&lt;/h3&gt;

&lt;p&gt;Storm与Kafka的版本信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storm：apache-storm-0.9.2-incubating&lt;/li&gt;
  &lt;li&gt;Kafka：kafka_2.9.2-0.8.1.1.tgz&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;基础知识&lt;/h3&gt;

&lt;p&gt;实现Storm读取Kafka中的数据，参考&lt;a href=&quot;http://storm.apache.org/about/integrates.html&quot;&gt;官网介绍&lt;/a&gt;， 本部分主要参考自&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/external/storm-kafk&quot;&gt;storm-kafka&lt;/a&gt;的README。&lt;/p&gt;

&lt;p&gt;Strom从Kafka中读取数据，本质：实现一个Storm中的Spout，来读取Kafka中的数据；这个Spout，可以称为Kafka Spout。实现一个Kafka Spout有两条路：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;core storm spout；&lt;/li&gt;
  &lt;li&gt;Trident spout；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;无论用哪种方式实现Kafka Spout，都分为两步走：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;实现BrokerHost接口：用于记录Kafka broker host与partition之间的映射关系；具体两种实现方式：
    &lt;ul&gt;
      &lt;li&gt;ZkHosts类：从zookeeper中动态的获取kafka broker与partition之间的映射关系；初始化时，需要配置zookeeper的&lt;code&gt;ip:port&lt;/code&gt;；默认，每60s从zookeeper中请求一次映射关系；&lt;/li&gt;
      &lt;li&gt;StaticHosts类：当broker–partition之间的映射关系是静态时，常使用此方法；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;继承KafkaConfig类：用于存储Kafka相关的参数；将上面实例的BrokerHost对象，作为参数传入KafkaConfig，例，Kafka的一个构造方法为&lt;code&gt;KafkaConfig(BrokerHosts hosts, String topic)&lt;/code&gt;；当前其实现方式有两个：
    &lt;ul&gt;
      &lt;li&gt;SpoutConfig：Core KafkaSpout只接受此配置方式；&lt;/li&gt;
      &lt;li&gt;TridentKafkaConfig：TridentKafkaEmitter只接受此配置方式；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KafkaConfig类中涉及到的配置参数默认值如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public int fetchSizeBytes = 1024 * 1024;
public int socketTimeoutMs = 10000;
public int fetchMaxWait = 10000;
public int bufferSizeBytes = 1024 * 1024;
public MultiScheme scheme = new RawMultiScheme();
public boolean forceFromStart = false;
public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
public long maxOffsetBehind = Long.MAX_VALUE;
public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
public int metricsTimeBucketSizeInSecs = 60;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的MultiScheme类型的参数shceme，其负责：将Kafka中取出的byte[]转换为storm所需的tuple，这是一个扩展点，默认是原文输出。两种实现：&lt;code&gt;SchemeAsMultiScheme&lt;/code&gt;和&lt;code&gt;KeyValueSchemeAsMultiScheme&lt;/code&gt;可将读取的byte[]转换为String。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：几个疑问，列在下面了&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;ZkHosts&lt;/code&gt;类的一个构造方法&lt;code&gt;ZkHosts(String brokerZkStr, String brokerZkPath)&lt;/code&gt;，其中&lt;code&gt;brokerZkPath&lt;/code&gt;的含义，原始给出的说法是：”rokerZkPath is the root directory under which all the topics and partition information is stored. by Default this is &lt;code&gt;/brokers&lt;/code&gt; which is what default kafka implementation uses.”&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SpoutConfig(BrokerHosts hosts, String topic, String zkRoot, String id)&lt;/code&gt;，其中，&lt;code&gt;zkRoot&lt;/code&gt;是一个root目录，用于存储consumer的offset；那这个&lt;code&gt;zkRoot&lt;/code&gt;对应的目录物理上在哪台机器？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;配置实例&lt;/h3&gt;

&lt;h4 id=&quot;core-kafka-spout&quot;&gt;Core Kafka Spout&lt;/h4&gt;

&lt;p&gt;本质是设置一个读取Kafka中数据的Kafka Spout，然后，将从替换原始local mode下，topology中的Spout即可。下面是一个已经验证过的实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TopologyBuilder builder = new TopologyBuilder();

BrokerHosts hosts = new ZkHosts(&quot;121.7.2.12:2181&quot;);
SpoutConfig spoutConfig = new SpoutConfig(hosts, &quot;ningg&quot;, &quot;/&quot; + &quot;ningg&quot;, UUID.randomUUID().toString());
spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

// set Spout.
builder.setSpout(&quot;word&quot;, kafkaSpout, 3);
builder.setBolt(&quot;result&quot;, new ExclamationBolt(), 3).shuffleGrouping(&quot;word&quot;);

Config conf = new Config();
conf.setDebug(true);

// submit topology in local mode
LocalCluster cluster = new LocalCluster();
cluster.submitTopology(&quot;test&quot;, conf, builder.createTopology());
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;trident-kafka-spouttodo&quot;&gt;Trident Kafka Spout（todo）&lt;/h3&gt;

&lt;p&gt;todo&lt;/p&gt;

&lt;p&gt;下面的样例并还没验证：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TridentTopology topology = new TridentTopology();
BrokerHosts zk = new ZkHosts(&quot;localhost&quot;);
TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, &quot;test-topic&quot;);
spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());
OpaqueTridentKafkaSpout spout = new OpaqueTridentKafkaSpout(spoutConf);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/about/integrates.html&quot;&gt;Storm Integrates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/external/storm-kafk&quot;&gt;storm-kafka&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume下插件方式实现Advanced Logger Sink</title>
     <link href="http://ningg.github.com/flume-advance-logger-sink"/>
     <updated>2014-10-31T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-advance-logger-sink</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;Flume自带的Logger sink常用于直接在console上输出event的header和body，这对test和debug很重要，但body默认只truncate 16B，无法全部展示，这对test造成很大影响，怎么办？自己实现一个Adavanced Logger sink：完全输出整个event，这样就便利多了。&lt;/p&gt;

&lt;h2 id=&quot;flumelogger-sink&quot;&gt;Flume中Logger Sink&lt;/h2&gt;

&lt;p&gt;在&lt;a href=&quot;/build-flume&quot;&gt;编译flume：使用eclipse查看flume源码&lt;/a&gt;中，已经介绍了如何在Eclipse下查看Flume的源代码，通过查看&lt;code&gt;LoggerSink&lt;/code&gt;源码发现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// LoggerSink.java
logger.info(&quot;Event: &quot; + EventHelper.dumpEvent(event));
...

// EventHelper.java
private static final int DEFAULT_MAX_BYTES = 16;

public static String dumpEvent(Event event) {
	return dumpEvent(event, DEFAULT_MAX_BYTES);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过上面的Flume源码片段可知，Logger Sink默认限制了event的大小为16字节，这样，只需要实现一个与Logger Sink基本一致，但不对event设限制的sink就好了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：编译flume时，直接将源码当作existing maven project导入，行不行？Flume的源码全是java写的吗？还有个问题：如果使用eclipse来进行源代码的开发，最终通过git方式向repository中提交代码时，会夹带.class文件吗？&lt;/p&gt;

&lt;h2 id=&quot;sink&quot;&gt;自定义Sink&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-advance-logger-sink/advanced-logger-sink.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;flume-ng-core&lt;/code&gt;工程的&lt;code&gt;src/main/java&lt;/code&gt;目录下，新建package：&lt;code&gt;com.github.ningg&lt;/code&gt;，然后新建class：&lt;code&gt;AdvancedLoggerSink&lt;/code&gt;，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package com.github.ningg;

import org.apache.flume.Channel;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.EventDeliveryException;
import org.apache.flume.Transaction;
import org.apache.flume.conf.Configurable;
import org.apache.flume.event.EventHelper;
import org.apache.flume.sink.AbstractSink;
import org.apache.flume.sink.LoggerSink;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class AdvancedLoggerSink extends AbstractSink implements Configurable {

	private static final Logger logger = LoggerFactory
			.getLogger(LoggerSink.class);

	private static final int DEFAULT_MAX_BYTES = 16;
	private int maxBytes = DEFAULT_MAX_BYTES;
	
	@Override
	public void configure(Context context) {
		maxBytes = context.getInteger(&quot;maxBytes&quot;, DEFAULT_MAX_BYTES);
		logger.debug(this.getName() + &quot; maximum bytes set to &quot; + String.valueOf(maxBytes));
	}
	
	@Override
	public Status process() throws EventDeliveryException {
		Status result = Status.READY;
		Channel channel = getChannel();
		Transaction transaction = channel.getTransaction();
		Event event = null;

		try {
			transaction.begin();
			event = channel.take();

			if (event != null) {
				if (logger.isInfoEnabled()) {
					// edit this line, so you can change the output formater.
					logger.info(&quot;Event: &quot; + EventHelper.dumpEvent(event, maxBytes));
				}
			} else {
				// No event found, request back-off semantics from the sink
				// runner
				result = Status.BACKOFF;
			}
			transaction.commit();
		} catch (Exception ex) {
			transaction.rollback();
			throw new EventDeliveryException(&quot;Failed to log event: &quot; + event,
					ex);
		} finally {
			transaction.close();
		}

		return result;
	}

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来，将整个&lt;code&gt;com.github.ningg&lt;/code&gt;package导出为jar包：&lt;code&gt;advancedLoggerSink.jar&lt;/code&gt;；根据Flume官网的建议，将此jar包上传到&lt;code&gt;$FLUME_HOME/plugins.d&lt;/code&gt;目录下，具体：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugins.d/advanced-logger-sink/lib/advancedLoggerSink.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了测试效果，在&lt;code&gt;$FLUME_HOME/conf&lt;/code&gt;下新建&lt;code&gt;advancedLoggerSink.conf&lt;/code&gt;文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;agent.sources = net
agent.sinks = loggerSink
agent.channels = memoryChannel

# For each one of the sources, the type is defined
agent.sources.net.type = netcat
agent.sources.net.bind = localhost
agent.sources.net.port = 44444

# Each sink&#39;s type must be defined
agent.sinks.loggerSink.type = com.github.ningg.AdvancedLoggerSink
agent.sinks.loggerSink.maxBytes = 100

# Each channel&#39;s type is defined.
agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 100


agent.sources.net.channels = memoryChannel
agent.sinks.loggerSink.channel = memoryChannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;回到&lt;code&gt;$FLUME_HOME&lt;/code&gt;目录下，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/flume-ng agent --conf conf --conf-file conf/advancedLoggerSink.conf --name agent -Dflume.root.logger=INFO,console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当页面显示如下字样，表示flume启动成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另开一个窗口，在当前服务器上，执行命令：&lt;code&gt;telnet localhost 44444&lt;/code&gt;，并且输入如下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Now I&#39;m testing the Advanced Logger Sink
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则，AdavancedSinkLogger的输出内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[INFO - com.github.ningg.AdvancedLoggerSink.process(AdvancedLoggerSink.java:44)] Event: { headers:{} 
   body: 4E 6F 77 20 49 27 6D 20 74 65 73 74 69 6E 67 20 Now I&#39;m testing
00000010 74 68 65 20 41 64 76 61 6E 63 65 64 20 4C 6F 67 the Advanced Log
00000020 67 65 72 20 53 69 6E 6B 0D                      ger Sink. }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AdvancedLoggerSink的输出格式：每行输出16个byte，左侧是字母对应的ASCII码，右侧是字母本身。备注：如果希望定制上述的输出格式，可以直接新建类来替代&lt;code&gt;EventHelper.dumpEvent(event, maxBytes)&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/20189437/flume-is-truncating-characters-when-i-use-the-source-type-as-logger-it-just-s&quot;&gt;Logger Sink truncate Event body&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2246&quot;&gt;FLUME-2246&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;本文写完之后，我发现了：&lt;a href=&quot;https://issues.apache.org/jira/browse/FLUME-2246&quot;&gt;FLUME-2246&lt;/a&gt;，Ou，已经有人在Flume官网上讨论并解决了这个问题，看来不会使用Flume官网不行呀，之前自己阅读标记过&lt;a href=&quot;/how-to-contribute-open-source-project&quot;&gt;如何参与开源项目&lt;/a&gt;， 但是没有实际尝试参与。个人心里一个想法：玩开源的东西，要参与到开源社区中，你的问题开源社区早已涉及，只不过有些扩展功能重要程度低，虽然已解决，但并没有并入发行版本中。&lt;/p&gt;

&lt;p&gt;另，说明一点啊：遇到问题，自己先想思路，再去社区找答案，也是个好方法。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>CentOS 6.4下LVM的使用</title>
     <link href="http://ningg.github.com/use-lvm"/>
     <updated>2014-10-29T00:00:00+08:00</updated>
     <id>http://ningg.github.com/use-lvm</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;系统有9块盘，每个320GB，之前服务器（CentOS 6.3）上磁盘分配情况：选取一块盘作为系统盘，划分&lt;code&gt;/boot&lt;/code&gt;、&lt;code&gt;swap&lt;/code&gt;、&lt;code&gt;/&lt;/code&gt;、&lt;code&gt;/home&lt;/code&gt;；然后，将剩余的8块盘，逐个格式化，并mount到系统盘的某个目录下。现在的问题是：某系统要上线，需要一个足够大的空间来存储数据，如果使用上述的方案，每个目录最大的存储空间都只有320GB，还是不够大，今后可能面临分区扩容的问题，使用静态分区，扩容有些麻烦。而LVM（Logical Volume Management，逻辑卷管理）能够将多个磁盘/分区组合在一起，抽象为一个逻辑上的分区，即，利用LVM技术，8块盘可以组成一个2.5T大小的分区，这样问题就解决了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：上面利用LVM来管理8块盘的方案，只是初步想法，不是最终方案。&lt;/p&gt;

&lt;h2 id=&quot;lvm&quot;&gt;LVM是什么&lt;/h2&gt;

&lt;p&gt;LVM(Logical Volume Management，逻辑卷管理)，一大核心功能是：对磁盘分区进行动态管理。当前无论在Linux、类Unix以及其他Windowns操作系统上，都存在LVM管理软件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/use-lvm/lvm-arch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lvm-1&quot;&gt;LVM要解决的问题&lt;/h3&gt;

&lt;p&gt;LVM要解决的典型问题：一块磁盘的空间160GB，其存满数据后，需要扩容，怎么办？传统静态分区时，需要将磁盘中近160GB的数据复制到1TB的磁盘上，然后，使用1TB的磁盘替换掉原来160GB的磁盘即可（替换：只要是挂载点替换一下就可以了）。LVM有更好的思路，来解决这个问题吗？&lt;/p&gt;

&lt;h3 id=&quot;lvm-2&quot;&gt;LVM原理简介&lt;/h3&gt;

&lt;p&gt;要解决上面磁盘空间不足时，磁盘的扩容问题，LVM提供了一个基本思路：LVM将底层的磁盘封装抽象为逻辑卷（logical volume），上层应用不直接从物理磁盘分区中读数据，而是从逻辑卷中读数据；LVM负责底层磁盘到逻辑卷的映射和管理；增加底层磁盘时，通过LVM可以为逻辑卷动态扩充容量，而这对上层应用是无影响的（透明的）。说这么多，总结一点：LVM提高了磁盘管理的灵活性。&lt;/p&gt;

&lt;h2 id=&quot;lvm-3&quot;&gt;LVM原理详解&lt;/h2&gt;

&lt;p&gt;上面简要说了一点点LVM的基本原理，吃饭要吃饱、做事要做好，OK，把LVM的原理好好理解一下。有几个概念要好好说一说。&lt;/p&gt;

&lt;h3 id=&quot;pv-physical-volume&quot;&gt;PV: Physical Volume&lt;/h3&gt;

&lt;p&gt;PV（Physical Volume），物理卷&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;物理卷在LVM系统中处于最底层。&lt;/li&gt;
  &lt;li&gt;物理卷可以是整个硬盘、硬盘上的分区或从逻辑上与磁盘分区具有同样功能的设备（如：RAID）。&lt;/li&gt;
  &lt;li&gt;物理卷是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：为什么要有PV？直接使用物理分区不行吗？&lt;/p&gt;

&lt;h3 id=&quot;vg-volume-group&quot;&gt;VG: Volume Group&lt;/h3&gt;

&lt;p&gt;VG（Volume Group），卷组&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;卷组建立在物理卷之上，它由一个或多个物理卷组成。&lt;/li&gt;
  &lt;li&gt;卷组创建之后，可以动态地添加物理卷到卷组中，在卷组上可以创建一个或多个“LVM分区”（逻辑卷）。&lt;/li&gt;
  &lt;li&gt;一个LVM系统中可以只有一个卷组，也可以包含多个卷组。&lt;/li&gt;
  &lt;li&gt;LVM的卷组类似于非LVM系统中的物理硬盘。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lg-logical-volume&quot;&gt;LG: Logical Volume&lt;/h3&gt;

&lt;p&gt;LG（Logical Volume），逻辑卷&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;逻辑卷建立在卷组之上，它是从卷组中“切出”的一块空间。&lt;/li&gt;
  &lt;li&gt;逻辑卷创建之后，其大小可以伸缩。&lt;/li&gt;
  &lt;li&gt;LVM的逻辑卷类似于非LVM系统中的硬盘分区，在逻辑卷之上可以建立文件系统（比如，/home或者/usr等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pe-physical-extent&quot;&gt;PE: Physical Extent&lt;/h3&gt;

&lt;p&gt;PE（Physical Extent），物理区域，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每一个物理卷被划分为基本单元（称为PE），具有唯一编号的PE是可以被LVM寻址的最小存储单元。&lt;/li&gt;
  &lt;li&gt;PE的大小可根据实际情况在创建物理卷时指定，默认为4 MB。&lt;/li&gt;
  &lt;li&gt;PE的大小一旦确定将不能改变，同一个卷组中的所有物理卷的PE的大小需要一致。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;le-logical-extent&quot;&gt;LE: Logical Extent&lt;/h3&gt;

&lt;p&gt;LE（Logical Extent），逻辑区域&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;逻辑区域也被划分为可被寻址的基本单位（称为LE）。&lt;/li&gt;
  &lt;li&gt;在同一个卷组中，LE的大小和PE是相同的，并且一一对应。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：PE、LE，是否与文件系统中block（逻辑块）类似？而block大，能够提升磁盘IO速度；但block过大，造成磁盘空间的浪费？&lt;/p&gt;

&lt;h3 id=&quot;vgda&quot;&gt;VGDA&lt;/h3&gt;

&lt;p&gt;和非LVM系统将包含分区信息的元数据保存在位于分区的起始位置的分区表中一样，逻辑卷以及卷组相关的元数据也是保存在位于物理卷起始处的卷组描述符区域（Volume Group Descriptor Area, VGDA）中。VGDA包括以下内容：PV描述符、VG描述符、LV描述符、和一些PE描述符。&lt;/p&gt;

&lt;p&gt;注意：/boot分区不能位于卷组中，因为引导装载程序无法从逻辑卷中读取。如果你想把/分区放在逻辑卷上，必须创建一个与卷组分离的/boot分区。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;小结&lt;/h3&gt;

&lt;p&gt;我们在创建好LV以后，我们会在 /dev 目录下看到我们的LV信息，例如 /dev/vgname/lvname， 我们每创建一个VG，其会在/dev目录下创建一个以该VG名字命名的文件夹，在该VG的基础上创建好LV以后，我们会在这个VG目录下多出一个以LV名字命名的逻辑卷。&lt;/p&gt;

&lt;p&gt;下面我们来对整个LVM的工作原理进行一个总结：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;物理磁盘/物理分区，被格式化为PV，本质：空间被划分为一个个的PE&lt;/li&gt;
  &lt;li&gt;不同的PV加入到同一个VG中，不同PV的PE全部进入到了VG的PE池内&lt;/li&gt;
  &lt;li&gt;LV基于PE创建，大小为PE的整数倍，组成LV的PE可能来自不同的物理磁盘&lt;/li&gt;
  &lt;li&gt;LV现在就直接可以格式化后挂载使用了&lt;/li&gt;
  &lt;li&gt;LV的扩充缩减实际上就是增加或减少组成该LV的PE数量，其过程不会丢失原始数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：在创建PV时，需要物理磁盘/物理分区提前进行格式化吗？&lt;strong&gt;RE&lt;/strong&gt;：不需要格式化的，直接创建PV就行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/use-lvm/pv-vg-lv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;centosdoing&quot;&gt;CentOS的推荐配置doing..&lt;/h2&gt;

&lt;p&gt;使用LVM来管理磁盘时，CentOS有没有推荐的配置？&lt;/p&gt;

&lt;p&gt;LVM只有优点吗？
LVM有没有副作用？降低磁盘IO？耗费一定的磁盘空间？LVM管理时，有没有CPU、磁盘资源的浪费？&lt;/p&gt;

&lt;p&gt;（下面还没有修改，参考来源：&lt;a href=&quot;http://hily.me/blog/2008/10/understanding-lvm/&quot;&gt;理解 LVM (Logical Volume Manager)&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;是否使用 LVM？&lt;/p&gt;

&lt;p&gt;在决定是否使用 LVM 前请先了解下 LVM 的优缺点。&lt;/p&gt;

&lt;p&gt;使用 LVM 的优势：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;文件系统可以跨多个磁盘，因此大小不会受物理磁盘的限制。&lt;/li&gt;
  &lt;li&gt;可以在系统运行状态下动态地扩展文件系统大小。&lt;/li&gt;
  &lt;li&gt;可以增加新磁盘到 LVM 的存储池中。&lt;/li&gt;
  &lt;li&gt;可以以镜像的方式冗余重要数据到多个物理磁盘上。&lt;/li&gt;
  &lt;li&gt;可以很方便地导出整个卷组，并导入到另外一台机器上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用 LVM 的限制：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;系统、文件系统出现问题时，LVM处理起来麻烦，关键要看处理者的能力&lt;/li&gt;
  &lt;li&gt;在从卷组中移除一个磁盘时必须使用 reducevg，否则会出问题。&lt;/li&gt;
  &lt;li&gt;当卷组中的一个磁盘损坏时，整个卷组都会受影响。&lt;/li&gt;
  &lt;li&gt;不能减小文件系统大小（受文件系统类型限制）。&lt;/li&gt;
  &lt;li&gt;因为加入了额外的操作，存储性能会受影响（使用 Stripe 的情况另当别论）。&lt;/li&gt;
  &lt;li&gt;使用 LVM 将获得更好的可扩展性和可操作性，但却损失了可靠性和存储性能，总的说来就是在这两者间选择。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用要点&lt;/p&gt;

&lt;p&gt;按需分配文件系统大小，不要一次性分配太大的空间给文件系统，剩余的空间可以放在存储池中，在需要时再扩充到文件系统中。
把不同的数据放在不同的卷组中，这样在做系统升级或数据迁移操作时会比较方便。&lt;/p&gt;

&lt;h2 id=&quot;linuxlvm&quot;&gt;Linux下LVM命令&lt;/h2&gt;

&lt;p&gt;LVM要实现的如下几个功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建PV、VG、LV&lt;/li&gt;
  &lt;li&gt;向VG中增加新的PV&lt;/li&gt;
  &lt;li&gt;从VG中移除PV&lt;/li&gt;
  &lt;li&gt;动态调整LV容量&lt;/li&gt;
  &lt;li&gt;删除PV、VG、LV&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：LV本质由PE组成的，那一个LV对应的所有PE是均匀分布在PV上吗？有什么策略？&lt;/p&gt;

&lt;h3 id=&quot;pvvglv&quot;&gt;PV\VG\LV的创建、删除&lt;/h3&gt;

&lt;p&gt;来个表格吧：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;命令&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;pvcreate&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;创建PV&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;pvdisplay&lt;/code&gt;/&lt;code&gt;pvs&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;查询PV详情&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;pvremove&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;删除PV&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;vgcreate&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;创建VG&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;vgdisplay&lt;/code&gt;/&lt;code&gt;vgs&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;查询VG详情&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;vgremove&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;删除VG&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;lvcreate&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;基于VG创建LV&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;lvdisplay&lt;/code&gt;/&lt;code&gt;lvs&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;查询LV详情&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;lvremove&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;删除LV&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;mkfs&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;格式化LV&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;mount&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;挂载LV&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;umount&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;卸载LV&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：先删除LV，再删除VG，最后删除PV。&lt;/p&gt;

&lt;h3 id=&quot;lvdoing&quot;&gt;LV的动态调整doing…&lt;/h3&gt;

&lt;p&gt;LV的动态调整：参考&lt;a href=&quot;http://www.cnblogs.com/xiaoluo501395377/archive/2013/05/24/3097785.html&quot;&gt;LVM逻辑卷的拉伸及缩减&lt;/a&gt;，&lt;a href=&quot;http://labs.chinamobile.com/mblog/854855_181800&quot;&gt;简单理解LVM(Logical Volume Manager)的基本原理&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/jimmy_zjw/article/details/8598219&quot;&gt;什么是LVM?（CentOS 5）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/xiaoluo501395377/archive/2013/05/22/3093405.html&quot;&gt;LVM逻辑卷基本概念及LVM的工作原理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://labs.chinamobile.com/mblog/854855_181800&quot;&gt;简单理解LVM(Logical Volume Manager)的基本原理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;我一直坚持读第一手的资料，因此，这次我直接就想到了CentOS官网的文章；不过读起来还是有些羞涩的，第一次接触LVM，很多东西云里雾里，相反查到的几篇blog还是不错的，有配图、有简洁的表述；因此，个人感觉，对于很生疏的东西，查看网上的blog反倒是入门的好方法；有了简单的入门知识，又希望深入理解的，那再去查第一手的资料就好了。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>Understanding LVM</title>
     <link href="http://ningg.github.com/understanding-lvm"/>
     <updated>2014-10-28T00:00:00+08:00</updated>
     <id>http://ningg.github.com/understanding-lvm</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/sn-partitioning-lvm.html&quot;&gt;Understanding LVM&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;LVM (Logical Volume Management) partitions provide a number of advantages over standard partitions. （LVM，Logical Volume Management，逻辑卷管理）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One or more physical volumes are combined to form a volume group. （n个physical volume，组成一个volume group）&lt;/li&gt;
  &lt;li&gt;Each volume group’s total storage is then divided into one or more logical volumes.（volume group被分割为n个logical volume）&lt;/li&gt;
  &lt;li&gt;The logical volumes function much like standard partitions. （在user看来，logical volume跟standard partition一样）&lt;/li&gt;
  &lt;li&gt;LVM partitions are formatted as physical volumes. They have a file system type, such as &lt;code&gt;ext4&lt;/code&gt;, and a mount point.（logical volume有自己的file system type，以及mount point）&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The /boot Partition and LVM&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;On most architectures, the boot loader cannot read LVM volumes. You must make a standard, non-LVM disk partition for your &lt;code&gt;/boot&lt;/code&gt; partition.（绝大多数architecture下，boot loader不能读取LVM volume；因此，需要为&lt;code&gt;/boot&lt;/code&gt;单独分区，并指定一个non-LVM的分区）&lt;/p&gt;

  &lt;p&gt;However, on System z, the &lt;code&gt;zipl&lt;/code&gt; boot loader supports &lt;code&gt;/boot&lt;/code&gt; on LVM logical volumes with linear mapping.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To understand LVM better, imagine the physical volume as a pile of &lt;code&gt;blocks&lt;/code&gt;. A block is simply a storage unit used to store data. Several piles of blocks can be combined to make a much larger pile, just as physical volumes are combined to make a volume group. The resulting pile can be subdivided into several smaller piles of arbitrary size, just as a volume group is allocated to several logical volumes.&lt;/p&gt;

&lt;p&gt;An administrator may grow or shrink logical volumes without destroying data, unlike standard disk partitions. If the physical volumes in a volume group are on separate drives or RAID arrays then administrators may also spread a logical volume across the storage devices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：LVM有两个优点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;动态调整logical volume&lt;/strong&gt;：动态的grow or shrink Logical volume，数据不会损坏；&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;并发写drives&lt;/strong&gt;：如果physical volume是多个drives 或者 RAID arrays，则 a logical volume能够横跨这些storage devices，带来一个好处，向某一目录写数据时，能够向多个磁盘并发写，加快写数据的速度；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may lose data if you shrink a logical volume to a smaller capacity than the data on the volume requires. To ensure maximum flexibility, create logical volumes to meet your current needs, and leave excess storage capacity unallocated. You may safely grow logical volumes to use unallocated space, as your needs dictate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：当将logical volume大小调整为小于其所存储数据的大小时，会丢失数据；通常，按照当前需求分配logical volume大小，其余的存储空间不分配，今后根据需要动态的增加logical volume的大小。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;LVM and the Default Partition Layout&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;By default, the installation process creates &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;swap&lt;/code&gt; partitions within LVM volumes, with a separate &lt;code&gt;/boot&lt;/code&gt; partition.&lt;/p&gt;
&lt;/blockquote&gt;

</content>
   </entry>
   
   <entry>
     <title>安装CentOS 6.4</title>
     <link href="http://ningg.github.com/centos-installation"/>
     <updated>2014-10-26T00:00:00+08:00</updated>
     <id>http://ningg.github.com/centos-installation</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;1. 简介&lt;/h2&gt;

&lt;p&gt;CentOS（Community Enterprise Operating System，企业社区操作系统）是Linux发行版本之一。Red Hat Enterpris Linux（RHEL，红帽企业级Linux）依照开放源码规定，开源了每个RHEL版本的源代码，CentOS正是基于RHEL的源代码重新编译而成的[1]，并且在RHEL基础上修复了一些已知的bug，相对与其他Linux发行版本，CentOS稳定性值得信赖。当前，很多企业都在服务器上安装CentOS系统，来支撑线上应用。
CentOS与RHEL的最大区别在于：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;RHEL中包含了部分封闭源码的工具，而CentOS包含的所有工具都是开源的；&lt;/li&gt;
  &lt;li&gt;RHEL提供技术服务，以此来收费；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;值得注意的是，2014年初，CentOS宣布加入Red Hat[2]。&lt;/p&gt;

&lt;p&gt;备注：CentOS的版本与RHEL版本基本一一对应，举例，CentOS 6.4对应RHEL 6.4的源代码。&lt;/p&gt;

&lt;h2 id=&quot;centos-6&quot;&gt;2. 安装CentOS 6&lt;/h2&gt;

&lt;p&gt;说到安装Linux系统，不要着急，官网肯定有操作手册来说明这个事，嗯，CentOS应用这么广泛，帮助手册总该有吧，要不然与其身份也不相符合。很不幸，&lt;a href=&quot;http://www.centos.org/docs/&quot;&gt;CentOS的官网&lt;/a&gt;中，并没有CentOS 6的操作手册，欧，赶快查查什么原因：CentOS完全基于RHEL源码编译而来，并且版本基本一一对应，因此，直接使用RHEL的官网文档即可[3]。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：本文所有安装CentOS 6的步骤、配置，都参考自RHEL 6官方文档[5]。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2.1.	基本设置&lt;/h3&gt;

&lt;p&gt;这一部分，主要演示几点：如何通过CD/DVD光驱来重装系统。&lt;/p&gt;

&lt;p&gt;步骤 1. 	重启系统，出现图1界面时，点击”F11”按钮，目的：设置Boot Menu。
说明：当点击完”F11”按钮之后，如图1界面最下端所示，”F11”按钮背景由黑色变为白色。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/001.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;步骤 2. 	当出现图2所示界面时，选择”1”，目标：从光驱中加载系统。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/002.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;稍等一会儿，有可能出现图3所示界面，不要管他，等一段时间即可
备注：如果长时间停留在图3界面，则敲击Enter。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/003.png&quot; alt=&quot;图 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;步骤 3. 	出现如图4时，选择“Install sytem with basic video driver”（第二项），目标：重装系统。
备注：也可选择“Install or upgrade an existing system”（第一项），但，有可能显示器画面出现倾斜异常（显卡驱动问题），因此推荐 “Install sytem with basic video driver”（第二项）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/004.png&quot; alt=&quot;&quot; /&gt;
图 4&lt;/p&gt;

&lt;p&gt;步骤 4. 	出现如图5所示界面后，通过”Tab”键，选择“Skip”选项，并使用“Space”键来确认即可。目标：在安装之前，不进行磁盘、网卡、内存等硬件设备的测试。（因为太浪费时间了）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/005.png&quot; alt=&quot;&quot; /&gt;
图 5&lt;/p&gt;

&lt;p&gt;选择”Skip”之后，可能会出现图6所示界面，稍等一会儿，会自动跳入下个页面（如图7）。等待时间：几十秒~几分钟。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/006.png&quot; alt=&quot;&quot; /&gt; 
图 6&lt;/p&gt;

&lt;p&gt;步骤 5. 	出现如图7所示页面后，点击”Next”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/007.png&quot; alt=&quot;&quot; /&gt;
图 7&lt;/p&gt;

&lt;p&gt;步骤 6. 	在如图8所示界面，选择安装CentOS过程中，页面的显示语言，当安装服务器时，建议选择“English（English）”。
备注：这一步选定哪种语言，貌似对安装系统没有影响，而实际测试发现，有些细微差异，例如，安装完系统后，系统环境变量LANG会有细微差异。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/008.png&quot; alt=&quot;&quot; /&gt;
图 8&lt;/p&gt;

&lt;p&gt;步骤 7. 	参照下图9~15，一步步安装下去即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/009.png&quot; alt=&quot;&quot; /&gt;
图 9&lt;/p&gt;

&lt;p&gt;图9：选择系统键盘语言，选“U.S. English”即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/010.png&quot; alt=&quot;&quot; /&gt;
图 10&lt;/p&gt;

&lt;p&gt;图10：选择系统安装的磁盘，选“Basic Storage Devices”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/011.png&quot; alt=&quot;&quot; /&gt; 
图 11&lt;/p&gt;

&lt;p&gt;特别说明：有可能会出现图11界面，如果没有出现，则忽略图11。&lt;/p&gt;

&lt;p&gt;图11：是否覆盖掉所有系统数据，如果是重装系统，数据已经做过备份，则直接选“Fresh Installation”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/012.png&quot; alt=&quot;&quot; /&gt;
图 12&lt;/p&gt;

&lt;p&gt;图12：设定主机名（hostname），按照要求进行设置即可。&lt;/p&gt;

&lt;p&gt;备注：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在图11页面的左下角，也可以通过“配置网络”按钮来设定网络，但不建议在此通过页面来配置网络（因为可能碰到乱七八糟的问题），而建议安装完系统后，通过简单命令来配置网络。&lt;/li&gt;
  &lt;li&gt;也可以安装完系统后，打开文件”/etc/sysconfig/network”，修改其中HOSTNAME字段。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/013.png&quot; alt=&quot;&quot; /&gt;
图 13&lt;/p&gt;

&lt;p&gt;图13：选定时区，选定“Asia/Shanghai”即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/014.png&quot; alt=&quot;&quot; /&gt; 
图 14&lt;/p&gt;

&lt;p&gt;图14：设定root密码&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/015.png&quot; alt=&quot;&quot; /&gt; 
图 15：选“Use Anyway”&lt;/p&gt;

&lt;p&gt;图15：提示密码不够安全，直接点击“Use Anyway”（无论如何都使用）即可。&lt;/p&gt;

&lt;p&gt;特别说明：至此，安装并没有结束，下面“2.2磁盘分区”部分才是重点。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.2.	磁盘分区&lt;/h3&gt;

&lt;p&gt;从图16开始，我们将进行磁盘分区，这一部分有些配置的东西，需要认真看了。&lt;/p&gt;

&lt;p&gt;备注：在此之前，需要补充一点理论知识：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.	为什么要进行磁盘分区？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;磁盘分区两点考虑，也就是说两个好处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据安全：不同磁盘分区之间相互独立，某个分区损坏，不会影响其他分区内的数据；&lt;/li&gt;
  &lt;li&gt;读写性能：读写数据时，磁盘分区对应一段连续的磁柱，由于磁柱集中，提升数据的读写效率；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.	磁盘分区要分为几个区？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;磁盘分区方案，官网建议[7]，应该包含如下几个分区：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;分区&lt;/th&gt;
      &lt;th&gt;作用&lt;/th&gt;
      &lt;th&gt;官方建议大小&lt;/th&gt;
      &lt;th&gt;此次安装使用&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;/boot&lt;/td&gt;
      &lt;td&gt;存放OS kernel，以及系统bootstrap过程要使用的其他文件&lt;/td&gt;
      &lt;td&gt;&amp;gt;250MB&lt;/td&gt;
      &lt;td&gt;500MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;swap&lt;/td&gt;
      &lt;td&gt;虚拟内存：当内存空间不足时使用此空间&lt;/td&gt;
      &lt;td&gt;至少4GB，推荐为内存的1~2倍&lt;/td&gt;
      &lt;td&gt;128GB （系统内存64GB）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;/&lt;/td&gt;
      &lt;td&gt;存放：系统安装文件&lt;/td&gt;
      &lt;td&gt;3~5GB&lt;/td&gt;
      &lt;td&gt;60GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;/home&lt;/td&gt;
      &lt;td&gt;存放：user data \n单独分区的目标：将user data与系统文件隔离&lt;/td&gt;
      &lt;td&gt;没有&lt;/td&gt;
      &lt;td&gt;100GB（实际是sda磁盘的剩余空间）&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;步骤 8. 	在图16界面，选择“Use All Space”，同时，勾选左下的“Review and modify partitioning layout”，目标：进入磁盘分区设置页面，调整磁盘分区。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/016.png&quot; alt=&quot;&quot; /&gt;
图 16：选“Use All Space”和勾选“Review and modify partitioning layout”&lt;/p&gt;

&lt;p&gt;中间可能要等待一段时间&lt;/p&gt;

&lt;p&gt;步骤 9. 	在图17所示页面，选择要进行分区的的磁盘，通常将“Data Storage Devicess”中所有磁盘都添加到“Install Target Devices”中，添加结果如图18所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/017.png&quot; alt=&quot;&quot; /&gt; 
图 17&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/018.png&quot; alt=&quot;&quot; /&gt; 
图 18&lt;/p&gt;

&lt;p&gt;图18：将“Data Storage Devicess”中所有磁盘都添加到“Install Target Devices”后的结果。 &lt;/p&gt;

&lt;p&gt;步骤 10. 	在图19所示页面，删除磁盘sda默认的分区：LVM Volume groups下的vg_cib61、sda下sda1和sda2；删除结果如图20所示。&lt;/p&gt;

&lt;p&gt;特别说明：要删除sda2分区，需要先删除LVM Volume groups下的vg_cib61。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/019.png&quot; alt=&quot;&quot; /&gt; 
图 19&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/020.png&quot; alt=&quot;&quot; /&gt;
图 20&lt;/p&gt;

&lt;p&gt;图20：删除sda上所有分区之后的结果。 &lt;/p&gt;

&lt;p&gt;步骤 11. 	在图20页面，按照提前规划的分区方案，在sda磁盘的Free空间上，依次划分/boot、swap、/、/home共计4个分区。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/021.png&quot; alt=&quot;&quot; /&gt; 
图 21&lt;/p&gt;

&lt;p&gt;图21：选择sda下Free空间，” Create” “Standard Partition”，即可进行创建分区，具体“/boot、swap、/、/home”的分区操作，依次参考图22、图23、图24、图25。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/022.png&quot; alt=&quot;&quot; /&gt; 
图 22&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/023.png&quot; alt=&quot;&quot; /&gt;
图 23&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/024.png&quot; alt=&quot;&quot; /&gt;
图 24&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/025.png&quot; alt=&quot;&quot; /&gt;
图 25&lt;/p&gt;

&lt;p&gt;步骤 12. 	这一步是进行LVM设置，如果没有LVM创建LV的需要，请直接跳过这一步，直接参考“步骤13”。 &lt;/p&gt;

&lt;p&gt;在此之前，补充一点LVM相关的理论知识：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.	为什么要用LVM？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LVM要解决的典型问题&lt;/strong&gt;：一块磁盘的空间160GB，其存满数据后，需要扩容，怎么办？传统静态分区时，需要将磁盘中近160GB的数据复制到1TB的磁盘上，然后，使用1TB的磁盘替换掉原来160GB的磁盘。（这个是传统扩容的基本原理，还有其他的原理吗？）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LVM基本原理&lt;/strong&gt;：要解决上面磁盘空间不足时，磁盘的扩容问题，LVM提供了一个基本思路：LVM将底层的磁盘封装抽象为逻辑卷（logical volume），上层应用不直接从物理磁盘分区中读数据，而是从逻辑卷中读数据；LVM负责底层磁盘到逻辑卷的映射和管理；增加底层磁盘时，通过LVM可以为逻辑卷动态扩充容量，而这对上层应用是无影响的（透明的）。&lt;/p&gt;

&lt;p&gt;说这么多，总结一点：LVM能够将多个小磁盘抽象为一个大逻辑卷，并且支持磁盘的动态扩容，提高了磁盘管理的灵活性。&lt;/p&gt;

&lt;p&gt;图26、图27、图28、图29：展示了在sdb1、sdc1、sdd1上创建一个大小约为850GB大小的VG（命名为vg_cib61），并且在这一VG上创建一个500GB大小的LV（lv_00）的基本过程。脑袋疼，不想多说，请自行查找其他资料。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/026.png&quot; alt=&quot;&quot; /&gt; 
图 26&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/027.png&quot; alt=&quot;&quot; /&gt;
图 27&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/028.png&quot; alt=&quot;&quot; /&gt;
图 28&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/029.png&quot; alt=&quot;&quot; /&gt;
图 29&lt;/p&gt;

&lt;p&gt;图29：创建Logical Volume时，并没有设置Mount Point，因为当前并不能确定挂载目录，装完系统之后，可以通过命令进行挂载。&lt;/p&gt;

&lt;p&gt;步骤 13. 	设置完磁盘分区后，到达图30所示界面，直接点击“Next”。&lt;/p&gt;

&lt;p&gt;特别说明：如果没有在步骤12中设置LVM，则没有图30中的“LVM Volume Groups”部分。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/030.png&quot; alt=&quot;&quot; /&gt;
图 30&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/031.png&quot; alt=&quot;&quot; /&gt;
图 31：选“Write changes to disk”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/032.png&quot; alt=&quot;&quot; /&gt;
图 32&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/033.png&quot; alt=&quot;&quot; /&gt;
图 33&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/034.png&quot; alt=&quot;&quot; /&gt;
图 34：选“Basic Server”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/035.png&quot; alt=&quot;&quot; /&gt;
图 35&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/centos-installation/036.png&quot; alt=&quot;&quot; /&gt;
图 36&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3.	配置网络&lt;/h2&gt;

&lt;p&gt;安装完系统之后，需要进行网络配置，目标：保证机器能够入网。&lt;/p&gt;

&lt;p&gt;通常直接修改/etc/sysconfig/network-scripts/ifcfg-eth0文件即可，此次使用的是静态配置IP方式，因此需要进行如下修改（保持ifcfg-eth0文件中其他字段不变）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ONBOOT=yes
BOOTPROTO=static
IPADDR=168.7.2.111
NETMASK=255.255.255.0
GATEWAY=168.7.2.126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;特别说明：服务器上有eth0–eth5，共计6个网口，需要根据具体情况，修改配置文件，上例中修改的是ifcfg-eth0文件，而在其他服务器上，如果网线插在eth3口，则需要修改ifcfg-eth3文件。&lt;/p&gt;

&lt;p&gt;有个小问题，值得说一下：服务器通常带有eth0–eth5多个网口，如何将eth0~5与实际的物理网口对应起来？&lt;/p&gt;

&lt;p&gt;RE：需要借助工具：ethtool，执行命令&lt;code&gt;ethtool -p eth0&lt;/code&gt;，再去看看那排网口，会有发现的~执行Ctrl + C，即可终止此命令。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;4.	格式化磁盘并挂载&lt;/h2&gt;

&lt;p&gt;场景 1.	格式化单个磁盘，并进行挂载，命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 格式化磁盘
mkfs -t ext3 /dev/sdb1
# 新建挂载点
mkdir -p /srv/hadoop/data1
# 挂载磁盘
mount /dev/sdb1 /srv/hadoop/data1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景 2.	批量格式化多个磁盘，并进行挂载，本质上就是重复“场景1”，只不过使用shell脚本来实现，脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in {b..k}; do mkfs -t ext3 /dev/sd${i}1; done

for i in {1..10}; do mkdir -p /srv/hadoop/data${i}; done

array=(b c d e f g h i j k)
for((i=0;i&amp;lt;${#array[@]};i++)); do mount /dev/sd${array[i]}1 /srv/hadoop/data$(($i+1)); done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景 3.	设置开机自动挂载磁盘&lt;/p&gt;

&lt;p&gt;上面两个场景中，都涉及到mount磁盘到某个目录，但如果系统一不小心重启了，这些磁盘就需要重新挂载。解决办法：在fstab文件中设置开机自动挂载磁盘。
通过命令：man  fstab就可以查看fstab文件每列的含义：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;th&gt;6&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;&amp;lt;special device&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code&gt;&amp;lt;mount point&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code&gt;&amp;lt;fs type&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code&gt;&amp;lt;mount options&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code&gt;&amp;lt;dump&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code&gt;&amp;lt;fsck&amp;gt;&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;上述/etc/fstab文件每行数据，都有6个字段，如上图所示，简要说明几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;间隔符号：不同字段之间使用 ”空格” 或者 “Tab” 键来间隔&lt;/li&gt;
  &lt;li&gt;special device：要挂载的设备，例如：/dev/sdb1;&lt;/li&gt;
  &lt;li&gt;mount point：设备挂载的目标目录；&lt;/li&gt;
  &lt;li&gt;fs type：要挂载的设备上文件系统的类型；&lt;/li&gt;
  &lt;li&gt;options：mount命令进行挂载时，输入的参数；&lt;/li&gt;
  &lt;li&gt;dump：是否要对此文件系统进行备份，0代表不做dump备份，1代表需要dump备份，2代表也需要dump备份，但2的重要程度低于1；&lt;/li&gt;
  &lt;li&gt;fsck：系统启动时，是否检测文件系统的完整性，0代表不检测，根目录/需要设置为1，其他需要开机扫描的文件系统设置为2；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;来个fstab文件的样例，朝着这个格式来做就可以：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/dev/sdb1  /srv/hadoop/data1  ext3  defaults  0  0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置完fstab文件，一定要来一条命令：&lt;code&gt;mount -a&lt;/code&gt;
含义：&lt;code&gt;Mount all filesystems (of the given types) mentioned in fstab.&lt;/code&gt;
这一命令可用于检查fstab文件中的配置是否正确。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;5.	参考来源&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.centos.org/about/&quot;&gt;CentOS简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.redhat.com/en/about/press-releases/red-hat-and-centos-join-forces&quot;&gt;Red Hat and CentOS Project Join Forces to Speed Open Source Innovation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lists.centos.org/pipermail/centos/2012-November/130123.html&quot;&gt;CentOS 6 docs参考RHEL 6即可&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/&quot;&gt;RHEL官方文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/index.html&quot;&gt;RHEL 6官文文档“Installation Guide”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/s1-x86-bootloader.html&quot;&gt;设定bootloader&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/s2-diskpartrecommend-x86.html&quot;&gt;Recommended Partitioning Schema&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-6&quot;&gt;6.	附录&lt;/h2&gt;

&lt;p&gt;几个有用的命令：&lt;/p&gt;

&lt;p&gt;命令 1. 	&lt;code&gt;dmidecode -t 1&lt;/code&gt;，查看当前服务器的序列号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# dmidecode -t 1
# dmidecode 2.11
SMBIOS 2.7 present.

Handle 0x0100, DMI type 1, 27 bytes
System Information
		Manufacturer: HP
		Product Name: ProLiant ********
		Version: Not Specified
		Serial Number: ********
		UUID: ****-****-****-****-****
		Wake-up Type: Power Switch
		SKU Number: ****-****
		Family: ProLiant
&lt;/code&gt;&lt;/pre&gt;

</content>
   </entry>
   
 
</feed>
