<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>NingG.github.com</title>
   <link href="http://ningg.github.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://ningg.github.com" rel="alternate" type="text/html" />
   <updated>2014-10-20T23:18:13+08:00</updated>
   <id>http://ningg.github.com</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>Storm：setting up a development environment</title>
     <link href="http://ningg.github.com/storm-setting-up-dev-env"/>
     <updated>2014-10-21T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-setting-up-dev-env</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Setting up a development environment&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This page outlines what you need to do to get a Storm development environment set up. In summary, the steps are:
（本文重点：set up a Storm dev env。概括一下，基本步骤如下）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download a &lt;a href=&quot;http://storm.apache.org//downloads.html&quot;&gt;Storm release&lt;/a&gt; , unpack it, and put the unpacked &lt;code&gt;bin/&lt;/code&gt; directory on your &lt;code&gt;PATH&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;To be able to start and stop topologies on a remote cluster, put the cluster information in &lt;code&gt;~/.storm/storm.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More detail on each of these steps is below.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-development-environment&quot;&gt;What is a development environment?&lt;/h2&gt;

&lt;p&gt;Storm has two modes of operation: &lt;code&gt;local mode&lt;/code&gt; and &lt;code&gt;remote mode&lt;/code&gt;. In local mode, you can develop and test topologies completely in process on your local machine. In remote mode, you submit topologies for execution on a cluster of machines.
（两种mode：local mode，develop和test topologies；remote mode，真正执行时，submit topologies到cluster）&lt;/p&gt;

&lt;p&gt;A Storm development environment has everything installed so that you can develop and test Storm topologies in local mode, package topologies for execution on a remote cluster, and submit/kill topologies on a remote cluster.&lt;/p&gt;

&lt;p&gt;Let’s quickly go over the relationship between your machine and a remote cluster. A Storm cluster is managed by a master node called “Nimbus”. Your machine communicates with Nimbus to submit code (packaged as a jar) and topologies for execution on the cluster, and Nimbus will take care of distributing that code around the cluster and assigning workers to run your topology. Your machine uses a command line client called storm to communicate with Nimbus. The storm client is only used for remote mode; it is not used for developing and testing topologies in local mode.
（master node，called &lt;code&gt;Nimbus&lt;/code&gt;，管理整个cluster，提交的code（jar包）、topologies都是交给&lt;code&gt;Nimbus&lt;/code&gt;负责接收的；之后，&lt;code&gt;Nimbus&lt;/code&gt;负责distribute code、assign worker to run topologies；本地提交topologies的机器，通过调用 storm client来通知storm与&lt;code&gt;Nimbus&lt;/code&gt;通信。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：distribute code？与assign worker to run topologies有区别吗？&lt;/p&gt;

&lt;h2 id=&quot;installing-a-storm-release-locally&quot;&gt;Installing a Storm release locally&lt;/h2&gt;

&lt;p&gt;If you want to be able to submit topologies to a remote cluster from your machine, you should install a Storm release locally. Installing a Storm release will give you the &lt;code&gt;storm&lt;/code&gt; client that you can use to interact with remote clusters. To install Storm locally, download a release &lt;a href=&quot;https://github.com/apache/incubator-storm/downloads&quot;&gt;from here&lt;/a&gt; and unzip it somewhere on your computer. Then add the unpacked &lt;code&gt;bin/&lt;/code&gt; directory onto your &lt;code&gt;PATH&lt;/code&gt; and make sure the &lt;code&gt;bin/storm&lt;/code&gt; script is executable.
（本地安装的Storm，也可以作为与remote cluster交互的client；安装办法：下载、解压、添加bin到PATH）&lt;/p&gt;

&lt;p&gt;Installing a Storm release locally is only for interacting with remote clusters. For developing and testing topologies in local mode, it is recommended that you use Maven to include Storm as a dev dependency for your project. You can read more about using Maven for this purpose on &lt;a href=&quot;http://storm.apache.org/documentation/Maven.html&quot;&gt;Maven&lt;/a&gt;.
（本地安装Storm唯一目标：interact with remote cluster；如果想利用local mode来进行develop、test，建议使用Maven将Storm作为依赖导入。）&lt;/p&gt;

&lt;h2 id=&quot;starting-and-stopping-topologies-on-a-remote-cluster&quot;&gt;Starting and stopping topologies on a remote cluster&lt;/h2&gt;

&lt;p&gt;The previous step installed the &lt;code&gt;storm&lt;/code&gt; client on your machine which is used to communicate with remote Storm clusters. Now all you have to do is tell the client which Storm cluster to talk to. To do this, all you have to do is put the host address of the master in the &lt;code&gt;~/.storm/storm.yaml&lt;/code&gt; file. It should look something like this:
（在本地安装storm实质是为了interact with remote cluster，这就需要告诉本地storm：remote cluster的位置。）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nimbus.host: &quot;123.45.678.890&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, if you use the &lt;a href=&quot;https://github.com/nathanmarz/storm-deploy&quot;&gt;storm-deploy&lt;/a&gt; project to provision Storm clusters on AWS, it will automatically set up your ~/.storm/storm.yaml file. You can manually attach to a Storm cluster (or switch between multiple clusters) using the “attach” command, like so:
（还有一种方法：如果Storm部署在AWS上，可直接使用 “attach” command）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lein run :deploy --attach --name mystormcluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More information is on the storm-deploy &lt;a href=&quot;https://github.com/nathanmarz/storm-deploy/wiki&quot;&gt;wiki&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;Apache Storm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Apache Storm: Documentation Rationale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm：Tutorial</title>
     <link href="http://ningg.github.com/storm-tutorial"/>
     <updated>2014-10-20T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-tutorial</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;Storm Tutorial&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this tutorial, you’ll learn how to create Storm topologies and deploy them to a Storm cluster. Java will be the main language used, but a few examples will use Python to illustrate Storm’s multi-language capabilities.
（这个tutorial中，着重说明：如何创建Storm topologies，如何将Storm topologies部署到Storm cluster中。）&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;p&gt;This tutorial uses examples from the &lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt; project. It’s recommended that you clone the project and follow along with the examples. Read &lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Setting up a development environment&lt;/a&gt; and &lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt; to get your machine set up.
（example都是来自&lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt; project，建议clone一份，跟着例子操作一把。在此之前，需要参考&lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Setting up a development environment&lt;/a&gt; 和 &lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt;来配置一下环境）&lt;/p&gt;

&lt;h2 id=&quot;components-of-a-storm-cluster&quot;&gt;Components of a Storm cluster&lt;/h2&gt;

&lt;p&gt;A Storm cluster is superficially similar to a Hadoop cluster. Whereas on Hadoop you run “MapReduce jobs”, on Storm you run “topologies”. “Jobs” and “topologies” themselves are very different – one key difference is that a MapReduce job eventually finishes, whereas a topology processes messages forever (or until you kill it).
（Storm cluster与Hadoop cluster类似，Hadoop中运行&lt;code&gt;MapReduce jobs&lt;/code&gt;，Storm中运行&lt;code&gt;topologies&lt;/code&gt;。）&lt;/p&gt;

&lt;p&gt;There are two kinds of nodes on a Storm cluster: the master node and the worker nodes. The master node runs a daemon called “Nimbus” that is similar to Hadoop’s “JobTracker”. Nimbus is responsible for distributing code around the cluster, assigning tasks to machines, and monitoring for failures.
（Storm cluster中两类node：master node、worker node；master上运行一个daemon——&lt;code&gt;Nimbus&lt;/code&gt;，其负责：distribute code，assign task，monitor failures）&lt;/p&gt;

&lt;p&gt;Each worker node runs a daemon called the “Supervisor”. The supervisor listens for work assigned to its machine and starts and stops worker processes as necessary based on what Nimbus has assigned to it. Each worker process executes a subset of a topology; a running topology consists of many worker processes spread across many machines.
（worker上运行daemon——&lt;code&gt;Supervisor&lt;/code&gt;，监听来自&lt;code&gt;Nimbus&lt;/code&gt;的任务，并starts或stop worker process。每个worker上执行topology的一部分，也就是说，a running topology 包含很多分布在不同机器上的worker process。）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/storm-cluster.png&quot; alt=&quot;Storm cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All coordination between Nimbus and the Supervisors is done through a &lt;a href=&quot;http://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt; cluster. Additionally, the Nimbus daemon and Supervisor daemons are fail-fast and stateless; all state is kept in Zookeeper or on local disk. This means you can kill -9 Nimbus or the Supervisors and they’ll start back up like nothing happened. This design leads to Storm clusters being incredibly stable.
（Zookeeper cluster负责&lt;code&gt;Nimbus&lt;/code&gt;和&lt;code&gt;Supervisor&lt;/code&gt;之间的协调；Nimbus和Supervisor deamon是fail-fast和stateless的，所有的状态都存储在Zookeeper or local disk；即，通过&lt;code&gt;kill -9 Nimbus/Supervisor&lt;/code&gt;，然后重启进程，对于用户基本是透明的）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;a href=&quot;http://en.wikipedia.org/wiki/Fail-fast&quot;&gt;fail-fast&lt;/a&gt;，表示如果输入有误，或者其他异常，则系统立刻停止运行。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;举例说明一下&lt;code&gt;fail-fast&lt;/code&gt;：在并发的时候，如果线程A正遍历一个collection(List, Map, Set etc.)，这时另外一个线程B却修改了该collection的size，线程A就会抛出一个错：ConcurrentModificationException，表明：我正读取的内容被修改掉了，你是否需要重新遍历？或是做其它处理？这就是fail-fast的含义。 &lt;/p&gt;

  &lt;p&gt;Fail-fast是并发中乐观(optimistic)策略的具体应用，它允许线程自由竞争，但在出现冲突的情况下假设你能应对，即你能判断出问题何在，并且给出解决办法。悲观(pessimistic)策略就正好相反，它总是预先设置足够的限制，通常是采用锁(lock)，来保证程序进行过程中的无错，付出的代价是其它线程的等待开销&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;topologies&quot;&gt;Topologies&lt;/h2&gt;

&lt;p&gt;To do realtime computation on Storm, you create what are called “topologies”. A topology is a graph of computation. Each node in a topology contains processing logic, and links between nodes indicate how data should be passed around between nodes.
（topology中每个node都包含了相应的processing logic）&lt;/p&gt;

&lt;p&gt;Running a topology is straightforward. First, you package all your code and dependencies into a single jar. Then, you run a command like the following:（执行 a topology：将your code和dependencies都放到一个jar中）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;storm jar all-my-code.jar backtype.storm.MyTopology arg1 arg2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This runs the class &lt;code&gt;backtype.storm.MyTopology&lt;/code&gt; with the arguments &lt;code&gt;arg1&lt;/code&gt; and &lt;code&gt;arg2&lt;/code&gt;. The main function of the class defines the topology and submits it to Nimbus. The &lt;code&gt;storm jar&lt;/code&gt; part takes care of connecting to Nimbus and uploading the jar.
（&lt;code&gt;storm jar&lt;/code&gt;负责connecting to Nimbus，以及uploading the jar）&lt;/p&gt;

&lt;p&gt;Since topology definitions are just Thrift structs, and Nimbus is a Thrift service, you can create and submit topologies using any programming language. The above example is the easiest way to do it from a JVM-based language. See &lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;Running topologies on a production cluster&lt;/a&gt; for more information on starting and stopping topologies.
（利用Thrift structs，来定义topology，并且，Nimbus是Thrift service，因此，可以使用任何编程语言。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Thrift structs? Thrift service?&lt;/p&gt;

&lt;h2 id=&quot;streams&quot;&gt;Streams&lt;/h2&gt;

&lt;p&gt;The core abstraction in Storm is the “stream”. A stream is an unbounded sequence of tuples. Storm provides the primitives for transforming a stream into a new stream in a distributed and reliable way. For example, you may transform a stream of tweets into a stream of trending topics.
（Storm的核心是&lt;code&gt;Stream&lt;/code&gt;，Storm提供一种分布式的、可靠的数据流转换方式，将原始数据流处理后，转换为其他的数据流）&lt;/p&gt;

&lt;p&gt;The basic primitives Storm provides for doing stream transformations are “spouts” and “bolts”. Spouts and bolts have interfaces that you implement to run your application-specific logic.
（Storm通过&lt;code&gt;spouts&lt;/code&gt;和&lt;code&gt;bolts&lt;/code&gt;来实现stream transformation）&lt;/p&gt;

&lt;p&gt;A spout is a source of streams. For example, a spout may read tuples off of a Kestrel queue and emit them as a stream. Or a spout may connect to the Twitter API and emit a stream of tweets.
（spout是a source of streams）&lt;/p&gt;

&lt;p&gt;A bolt consumes any number of input streams, does some processing, and possibly emits new streams. Complex stream transformations, like computing a stream of trending topics from a stream of tweets, require multiple steps and thus multiple bolts. Bolts can do anything from run functions, filter tuples, do streaming aggregations, do streaming joins, talk to databases, and more.
（bolt负责处理input stream，也可能产生new stream；bolt通过run functions，filter tuples，do streaming aggregations，do streaming joins，talk to database，etc…来做任何事情）&lt;/p&gt;

&lt;p&gt;Networks of spouts and bolts are packaged into a “topology” which is the top-level abstraction that you submit to Storm clusters for execution. A topology is a graph of stream transformations where each node is a spout or bolt. Edges in the graph indicate which bolts are subscribing to which streams. When a spout or bolt emits a tuple to a stream, it sends the tuple to every bolt that subscribed to that stream.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology.png&quot; alt=&quot;A Storm topology&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Links between nodes in your topology indicate how tuples should be passed around. For example, if there is a link between Spout A and Bolt B, a link from Spout A to Bolt C, and a link from Bolt B to Bolt C, then everytime Spout A emits a tuple, it will send the tuple to both Bolt B and Bolt C. All of Bolt B’s output tuples will go to Bolt C as well.&lt;/p&gt;

&lt;p&gt;Each node in a Storm topology executes in parallel. In your topology, you can specify how much parallelism you want for each node, and then Storm will spawn that number of threads across the cluster to do the execution.
（topology中所有node都是并发运行的，可以配置每个node的并发数。）&lt;/p&gt;

&lt;p&gt;A topology runs forever, or until you kill it. Storm will automatically reassign any failed tasks. Additionally, Storm guarantees that there will be no data loss, even if machines go down and messages are dropped.
（topology会自动重启失败的任务，并且run forever——因为是stream processing；即使node机器崩溃并且message丢失，Storm也能保证no data loss）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Storm如何保证node崩溃后，no data loss的？&lt;/p&gt;

&lt;h2 id=&quot;data-model&quot;&gt;Data model&lt;/h2&gt;

&lt;p&gt;Storm uses tuples as its data model. A tuple is a named list of values, and a field in a tuple can be an object of any type. Out of the box, Storm supports all the primitive types, strings, and byte arrays as tuple field values. To use an object of another type, you just need to implement &lt;a href=&quot;http://storm.apache.org/documentation/Serialization.html&quot;&gt;a serializer&lt;/a&gt; for the type.
（Storm中用&lt;code&gt;tuple&lt;/code&gt;结构来存储数据，tuple中的field可以是任何类型的object，自定义的类型，需要实现&lt;a href=&quot;http://storm.apache.org/documentation/Serialization.html&quot;&gt;a serializer&lt;/a&gt;接口）&lt;/p&gt;

&lt;p&gt;Every node in a topology must declare the output fields for the tuples it emits. For example, this bolt declares that it emits 2-tuples with the fields “double” and “triple”:
（每个node都要定义输出的tuple的结构）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java public class DoubleAndTripleBolt extends BaseRichBolt { 

   private OutputCollectorBase _collector;
   
   @Override
   public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
	   _collector = collector;
   }
   
   @Override
   public void execute(Tuple input) {
	   int val = input.getInteger(0);        
	   _collector.emit(input, new Values(val*2, val*3));
	   _collector.ack(input);
   }
   
   @Override
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;double&quot;, &quot;triple&quot;));
   }     

} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The declareOutputFields function declares the output fields &lt;code&gt;[&quot;double&quot;, &quot;triple&quot;]&lt;/code&gt; for the component. The rest of the bolt will be explained in the upcoming sections.&lt;/p&gt;

&lt;h2 id=&quot;a-simple-topology&quot;&gt;A simple topology&lt;/h2&gt;

&lt;p&gt;Let’s take a look at a simple topology to explore the concepts more and see how the code shapes up. Let’s look at the &lt;code&gt;ExclamationTopology&lt;/code&gt; definition from storm-starter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
TopologyBuilder builder = new TopologyBuilder(); 
builder.setSpout(&quot;words&quot;, new TestWordSpout(), 10); 
builder.setBolt(&quot;exclaim1&quot;, new ExclamationBolt(), 3).shuffleGrouping(&quot;words&quot;); 
builder.setBolt(&quot;exclaim2&quot;, new ExclamationBolt(), 2).shuffleGrouping(&quot;exclaim1&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This topology contains a spout and two bolts. The spout emits words, and each bolt appends the string “!!!” to its input. The nodes are arranged in a line: the spout emits to the first bolt which then emits to the second bolt. If the spout emits the tuples “bob” and “john”, then the second bolt will emit the words “bob!!!!!!” and “john!!!!!!”.&lt;/p&gt;

&lt;p&gt;This code defines the nodes using the &lt;code&gt;setSpout&lt;/code&gt; and &lt;code&gt;setBolt&lt;/code&gt; methods. These methods take as input a user-specified id, an object containing the processing logic, and the amount of parallelism you want for the node. In this example, the spout is given id “words” and the bolts are given ids “exclaim1” and “exclaim2”.
（两个方法&lt;code&gt;setSpout&lt;/code&gt;`setBolt`，参数的含义：node id、processing logic、amount of parallelism。）&lt;/p&gt;

&lt;p&gt;The object containing the processing logic implements the &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/topology/IRichSpout.html&quot;&gt;IRichSpout&lt;/a&gt; interface for spouts and the &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/topology/IRichBolt.html&quot;&gt;IRichBolt&lt;/a&gt; interface for bolts.
（包含processing logic的object需要实现IRichSpout\IRichBolt）&lt;/p&gt;

&lt;p&gt;The last parameter, how much parallelism you want for the node, is optional. It indicates how many threads should execute that component across the cluster. If you omit it, Storm will only allocate one thread for that node.
（如果不设置 amount of parallelism的，系统默认自启动一个thread）&lt;/p&gt;

&lt;p&gt;&lt;code&gt;setBolt&lt;/code&gt; returns an &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/topology/InputDeclarer.html&quot;&gt;InputDeclarer&lt;/a&gt; object that is used to define the inputs to the Bolt. Here, component “exclaim1” declares that it wants to read all the tuples emitted by component “words” using a shuffle grouping, and component “exclaim2” declares that it wants to read all the tuples emitted by component “exclaim1” using a shuffle grouping. “shuffle grouping” means that tuples should be randomly distributed from the input tasks to the bolt’s tasks. There are many ways to group data between components. These will be explained in a few sections.
（&lt;code&gt;shuffle grouping&lt;/code&gt;，随机分发tuple）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Storm是流式处理，需要保证tuple的顺序执行吗？还有tuple需要顺序执行吗？&lt;/p&gt;

&lt;p&gt;If you wanted component “exclaim2” to read all the tuples emitted by both component “words” and component “exclaim1”, you would write component “exclaim2”’s definition like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
builder.setBolt(&quot;exclaim2&quot;, new ExclamationBolt(), 5) .shuffleGrouping(&quot;words&quot;) .shuffleGrouping(&quot;exclaim1&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, input declarations can be chained to specify multiple sources for the Bolt.&lt;/p&gt;

&lt;p&gt;Let’s dig into the implementations of the spouts and bolts in this topology. Spouts are responsible for emitting new messages into the topology. &lt;code&gt;TestWordSpout&lt;/code&gt; in this topology emits a random word from the list &lt;code&gt;[“nathan”, “mike”, “jackson”, “golda”, “bertels”]&lt;/code&gt; as a 1-tuple every 100ms. The implementation of &lt;code&gt;nextTuple()&lt;/code&gt; in TestWordSpout looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
public void nextTuple() { 
    Utils.sleep(100); 
    final String[] words = new String[] {&quot;nathan&quot;, &quot;mike&quot;, &quot;jackson&quot;, &quot;golda&quot;, &quot;bertels&quot;}; 
    final Random rand = new Random(); 
    final String word = words[rand.nextInt(words.length)]; 
    _collector.emit(new Values(word)); 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the implementation is very straightforward.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ExclamationBolt&lt;/code&gt; appends the string “!!!” to its input. Let’s take a look at the full implementation for &lt;code&gt;ExclamationBolt&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
public static class ExclamationBolt implements IRichBolt { OutputCollector _collector;
   
   public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	   _collector = collector;
   }
   
   public void execute(Tuple tuple) {
	   _collector.emit(tuple, new Values(tuple.getString(0) + &quot;!!!&quot;));
	   _collector.ack(tuple);
   }
   
   public void cleanup() {
   }
   
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;word&quot;));
   }
   
   public Map getComponentConfiguration() {
	   return null;
   } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;prepare&lt;/code&gt; method provides the bolt with an &lt;code&gt;OutputCollector&lt;/code&gt; that is used for emitting tuples from this bolt. Tuples can be emitted at anytime from the bolt – in the &lt;code&gt;prepare&lt;/code&gt;, &lt;code&gt;execute&lt;/code&gt;, or &lt;code&gt;cleanup&lt;/code&gt; methods, or even asynchronously in another thread. This &lt;code&gt;prepare&lt;/code&gt; implementation simply saves the &lt;code&gt;OutputCollector&lt;/code&gt; as an instance variable to be used later on in the &lt;code&gt;execute&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;execute&lt;/code&gt; method receives a tuple from one of the bolt’s inputs. The &lt;code&gt;ExclamationBolt&lt;/code&gt; grabs the first field from the tuple and emits a new tuple with the string “!!!” appended to it. If you implement a bolt that subscribes to multiple input sources, you can find out which component the Tuple came from by using the &lt;code&gt;Tuple#getSourceComponent&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;There’s a few other things going in in the &lt;code&gt;execute&lt;/code&gt; method, namely that the input tuple is passed as the first argument to &lt;code&gt;emit&lt;/code&gt; and the input tuple is acked on the final line. These are part of Storm’s reliability API for guaranteeing no data loss and will be explained later in this tutorial.
（在&lt;code&gt;execute&lt;/code&gt;方法中，最后一行进行&lt;code&gt;ack&lt;/code&gt;）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;ack&lt;/code&gt;方法的作用？如何实现的？&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;cleanup&lt;/code&gt; method is called when a Bolt is being shutdown and should cleanup any resources that were opened. There’s no guarantee that this method will be called on the cluster: for example, if the machine the task is running on blows up, there’s no way to invoke the method. The &lt;code&gt;cleanup&lt;/code&gt; method is intended for when you run topologies in &lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;local mode&lt;/a&gt; (where a Storm cluster is simulated in process), and you want to be able to run and kill many topologies without suffering any resource leaks.
（&lt;code&gt;cleanup&lt;/code&gt;的一个用途：local mode中运行Storm时，可以释放资源，防止资源泄漏。）&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;declareOutputFields&lt;/code&gt; method declares that the &lt;code&gt;ExclamationBolt&lt;/code&gt; emits 1-tuples with one field called “word”.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;getComponentConfiguration&lt;/code&gt; method allows you to configure various aspects of how this component runs. This is a more advanced topic that is explained further on &lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;Configuration&lt;/a&gt;.
（component中很多方面的属性都可进行设置）&lt;/p&gt;

&lt;p&gt;Methods like cleanup and getComponentConfiguration are often not needed in a bolt implementation. You can define bolts more succinctly by using a base class that provides default implementations where appropriate. ExclamationBolt can be written more succinctly by extending BaseRichBolt, like so:
（通常，bolt并不需要实现cleanup和getComponentConfiguration；建议：为多个功能相似的bolt创建一个base类，然后所有的实现都继承base类）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
public static class ExclamationBolt extends BaseRichBolt { OutputCollector _collector;
   
   public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	   _collector = collector;
   }
   
   public void execute(Tuple tuple) {
	   _collector.emit(tuple, new Values(tuple.getString(0) + &quot;!!!&quot;));
	   _collector.ack(tuple);
   }
   
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;word&quot;));
   }     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;running-exclamationtopology-in-local-mode&quot;&gt;Running ExclamationTopology in local mode&lt;/h2&gt;

&lt;p&gt;Let’s see how to run the ExclamationTopology in local mode and see that it’s working.&lt;/p&gt;

&lt;p&gt;Storm has two modes of operation: local mode and distributed mode. In local mode, Storm executes completely in process by simulating worker nodes with threads. Local mode is useful for testing and development of topologies. When you run the topologies in storm-starter, they’ll run in local mode and you’ll be able to see what messages each component is emitting. You can read more about running topologies in local mode on &lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;Local mode&lt;/a&gt;.
（local mode，用于testing和development，因为，其可以查看到component发出的所有message）&lt;/p&gt;

&lt;p&gt;In distributed mode, Storm operates as a cluster of machines. When you submit a topology to the master, you also submit all the code necessary to run the topology. The master will take care of distributing your code and allocating workers to run your topology. If workers go down, the master will reassign them somewhere else. You can read more about running topologies on a cluster on &lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;Running topologies on a production cluster&lt;/a&gt;.
（distributed mode，用户需要向master提交topology，以及运行这个topology所需要的所有code，详细信息推荐&lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;阅读&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;Here’s the code that runs &lt;code&gt;ExclamationTopology&lt;/code&gt; in local mode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
Config conf = new Config(); 
conf.setDebug(true); 
conf.setNumWorkers(2);

LocalCluster cluster = new LocalCluster(); 
cluster.submitTopology(“test”, conf, builder.createTopology()); 

Utils.sleep(10000); 

cluster.killTopology(“test”); 
cluster.shutdown();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, the code defines an in-process cluster by creating a &lt;code&gt;LocalCluster&lt;/code&gt; object. Submitting topologies to this virtual cluster is identical to submitting topologies to distributed clusters. It submits a topology to the &lt;code&gt;LocalCluster&lt;/code&gt; by calling &lt;code&gt;submitTopology&lt;/code&gt;, which takes as arguments a name for the running topology, a configuration for the topology, and then the topology itself.
（为topology指定一个唯一标识的名字）&lt;/p&gt;

&lt;p&gt;The name is used to identify the topology so that you can kill it later on. A topology will run indefinitely until you kill it.
（除非kill掉topology，否则，其永久的运行下去，因为是stream processing嘛，自然是endless）&lt;/p&gt;

&lt;p&gt;The configuration is used to tune various aspects of the running topology. The two configurations specified here are very common:
（有两个参数，重点说一下）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;TOPOLOGY_WORKERS&lt;/strong&gt; (set with &lt;code&gt;setNumWorkers&lt;/code&gt;) specifies how many &lt;code&gt;processes&lt;/code&gt; you want allocated around the cluster to execute the topology. Each component in the topology will execute as many &lt;code&gt;threads&lt;/code&gt;. The number of threads allocated to a given component is configured through the &lt;code&gt;setBolt&lt;/code&gt; and &lt;code&gt;setSpout&lt;/code&gt; methods. Those threads exist within worker processes. Each &lt;code&gt;worker process&lt;/code&gt; contains within it some number of threads for some number of components. For instance, you may have 300 threads specified across all your components and 50 worker processes specified in your config. Each worker process will execute 6 threads, each of which of could belong to a different component. You tune the performance of Storm topologies by tweaking the parallelism for each component and the number of worker processes those threads should run within.（每个component都会启动几个thread，这些Thread都是包含在worker process中的，每个worker process都会包含来自一些components的一些thread。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TOPOLOGY_DEBUG&lt;/strong&gt; (set with &lt;code&gt;setDebug&lt;/code&gt;), when set to true, tells Storm to log every message every emitted by a component. This is useful in local mode when testing topologies, but you probably want to keep this turned off when running topologies on the cluster.（设置Debug模式启动后，component emit的所有message都会被记录下来）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There’s many other configurations you can set for the topology. The various configurations are detailed on &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/Config.html&quot;&gt;the Javadoc for Config&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：worker process、thread、component之间什么关系？worker process只是作为支撑component与thread间的映射关系？&lt;/p&gt;

&lt;p&gt;To learn about how to set up your development environment so that you can run topologies in local mode (such as in Eclipse), see &lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt;.
（设置本地的开发环境，这样就可以在Eclipse等中进行调试开发）	&lt;/p&gt;

&lt;h2 id=&quot;stream-groupings&quot;&gt;Stream groupings&lt;/h2&gt;

&lt;p&gt;A stream grouping tells a topology how to send tuples between two components. Remember, spouts and bolts execute in parallel as many tasks across the cluster. If you look at how a topology is executing at the task level, it looks something like this:
（stream grouping负责一个topology中tuples在components间的传递；）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology-tasks.png&quot; alt=&quot;Tasks in a topology&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When a task for Bolt A emits a tuple to Bolt B, which task should it send the tuple to?
（从Bolt A发出的tuple，发送给Bolt B的那一个task？）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：tuple与task之间什么关系？&lt;/p&gt;

&lt;p&gt;A “stream grouping” answers this question by telling Storm how to send tuples between sets of tasks. Before we dig into the different kinds of stream groupings, let’s take a look at another topology from &lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt;. This &lt;a href=&quot;https://github.com/nathanmarz/storm-starter/blob/master/src/jvm/storm/starter/WordCountTopology.java&quot;&gt;WordCountTopology&lt;/a&gt; reads sentences off of a spout and streams out of &lt;code&gt;WordCountBolt&lt;/code&gt; the total number of times it has seen that word before:
（stream grouping，负责分配tuple到task中去；）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
TopologyBuilder builder = new TopologyBuilder();

builder.setSpout(“sentences”, new RandomSentenceSpout(), 5); 
builder.setBolt(“split”, new SplitSentence(), 8) .shuffleGrouping(“sentences”); 
builder.setBolt(“count”, new WordCount(), 12) .fieldsGrouping(“split”, new Fields(“word”));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;SplitSentence&lt;/code&gt; emits a tuple for each word in each sentence it receives, and WordCount keeps a map in memory from word to count. Each time WordCount receives a word, it updates its state and emits the new word count.&lt;/p&gt;

&lt;p&gt;There’s a few different kinds of stream groupings.&lt;/p&gt;

&lt;p&gt;The simplest kind of grouping is called a “shuffle grouping” which sends the tuple to a random task. A shuffle grouping is used in the &lt;code&gt;WordCountTopology&lt;/code&gt; to send tuples from RandomSentenceSpout to the SplitSentence bolt. It has the effect of evenly distributing the work of processing the tuples across all of SplitSentence bolt’s tasks.
（shuffle grouping，随机分发tuple到所有的bolt）&lt;/p&gt;

&lt;p&gt;A more interesting kind of grouping is the “fields grouping”. A fields grouping is used between the SplitSentence bolt and the WordCount bolt. It is critical for the functioning of the WordCount bolt that the same word always go to the same task. Otherwise, more than one task will see the same word, and they’ll each emit incorrect values for the count since each has incomplete information. A fields grouping lets you group a stream by a subset of its fields. This causes equal values for that subset of fields to go to the same task. Since &lt;code&gt;WordCount&lt;/code&gt; subscribes to SplitSentence’s output stream using a fields grouping on the “word” field, the same word always goes to the same task and the bolt produces the correct output.
（fields grouping，根据给定的fields进行group操作）&lt;/p&gt;

&lt;p&gt;Fields groupings are the basis of implementing streaming joins and streaming aggregations as well as a plethora of other use cases. Underneath the hood, fields groupings are implemented using mod hashing.
（fields grouping，应用很广泛，本质上，其使用mod hashing方式实现）&lt;/p&gt;

&lt;p&gt;There’s a few other kinds of stream groupings. You can read more about them on &lt;a href=&quot;http://storm.apache.org/documentation/Concepts.html&quot;&gt;Concepts&lt;/a&gt;.
（关于stream grouping的更多内容，参考&lt;a href=&quot;http://storm.apache.org/documentation/Concepts.html&quot;&gt;Concepts&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;defining-bolts-in-other-languages&quot;&gt;Defining Bolts in other languages&lt;/h2&gt;

&lt;p&gt;Bolts can be defined in any language. Bolts written in another language are executed as subprocesses, and Storm communicates with those subprocesses with JSON messages over stdin/stdout. The communication protocol just requires an ~100 line adapter library, and Storm ships with adapter libraries for Ruby, Python, and Fancy.
（bolt可以使用其他语言实现，本质上其他语言编写的bolt都是subprocess，Storm通过stdin/stdout上的JSON串来与其进行通讯）&lt;/p&gt;

&lt;p&gt;Here’s the definition of the SplitSentence bolt from WordCountTopology:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
public static class SplitSentence extends ShellBolt implements IRichBolt { 
   public SplitSentence() { 
       super(“python”, “splitsentence.py”); 
   }
   
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;word&quot;));
   } 
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SplitSentence overrides ShellBolt and declares it as running using python with the arguments splitsentence.py. Here’s the implementation of splitsentence.py:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// python 
import storm

class SplitSentenceBolt(storm.BasicBolt): 
	def process(self, tup): 
		words = tup.values[0].split(“ “) 
		for word in words: 
			storm.emit([word])

SplitSentenceBolt().run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For more information on writing spouts and bolts in other languages, and to learn about how to create topologies in other languages (and avoid the JVM completely), see &lt;a href=&quot;http://storm.apache.org/documentation/Using-non-JVM-languages-with-Storm.html&quot;&gt;Using non-JVM languages with Storm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;guaranteeing-message-processing&quot;&gt;Guaranteeing message processing&lt;/h2&gt;

&lt;p&gt;Earlier on in this tutorial, we skipped over a few aspects of how tuples are emitted. Those aspects were part of Storm’s reliability API: how Storm guarantees that every message coming off a spout will be fully processed. See &lt;a href=&quot;http://storm.apache.org/documentation/Guaranteeing-message-processing.html&quot;&gt;Guaranteeing message processing&lt;/a&gt; for information on how this works and what you have to do as a user to take advantage of Storm’s reliability capabilities.
（本文中，并没有深入介绍how tuples are emitted，其中涉及一个关键问题：如何保证spout提供的message一定会被成功处理，详细信息可参考&lt;a href=&quot;http://storm.apache.org/documentation/Guaranteeing-message-processing.html&quot;&gt;Guaranteeing message processing&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;transactional-topologies&quot;&gt;Transactional topologies&lt;/h2&gt;

&lt;p&gt;Storm guarantees that every message will be played through the topology at least once. A common question asked is “how do you do things like counting on top of Storm? Won’t you overcount?” Storm has a feature called transactional topologies that let you achieve exactly-once messaging semantics for most computations. Read more about transactional topologies &lt;a href=&quot;http://storm.apache.org/documentation/Transactional-topologies.html&quot;&gt;here&lt;/a&gt;.
（如何保证message只被处理一次，而不被重复处理？Storm提供了transactional topologies来保证只执行一次）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message到底是什么？tuple？task？&lt;/p&gt;

&lt;h2 id=&quot;distributed-rpc&quot;&gt;Distributed RPC&lt;/h2&gt;

&lt;p&gt;This tutorial showed how to do basic stream processing on top of Storm. There’s lots more things you can do with Storm’s primitives. One of the most interesting applications of Storm is Distributed RPC, where you parallelize the computation of intense functions on the fly. Read more about Distributed RPC &lt;a href=&quot;http://storm.apache.org/documentation/Distributed-RPC.html&quot;&gt;here&lt;/a&gt;.
（Distribute RPC很有意思的，能够parallelize the computation of intense functions on the fly。）&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This tutorial gave a broad overview of developing, testing, and deploying Storm topologies. The rest of the documentation dives deeper into all the aspects of using Storm.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;Storm Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm：Rationale</title>
     <link href="http://ningg.github.com/storm-documentation-rationale"/>
     <updated>2014-10-20T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-documentation-rationale</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Storm Rationale&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The past decade has seen a revolution in data processing. MapReduce, Hadoop, and related technologies have made it possible to store and process data at scales previously unthinkable. Unfortunately, these data processing technologies are not realtime systems, nor are they meant to be. There’s no hack that will turn Hadoop into a realtime system; realtime data processing has a fundamentally different set of requirements than batch processing.
（Hadoop的相关技术已经很多，大规模数据处理方面很强，但realtime processing却不行）&lt;/p&gt;

&lt;p&gt;However, realtime data processing at massive scale is becoming more and more of a requirement for businesses. The lack of a “Hadoop of realtime” has become the biggest hole in the data processing ecosystem.&lt;/p&gt;

&lt;p&gt;Storm fills that hole.
（Storm将解决大规模数据的实时处理问题。）&lt;/p&gt;

&lt;p&gt;Before Storm, you would typically have to manually build a network of queues and workers to do realtime processing. Workers would process messages off a queue, update databases, and send new messages to other queues for further processing. Unfortunately, this approach has serious limitations:
（在Storm之前，需要手动创建queues和workers，其中，worker从queue中取出message，并进行处理；不幸呐，这种方式有很严重的限制）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Tedious&lt;/strong&gt;: You spend most of your development time configuring where to send messages, deploying workers, and deploying intermediate queues. The realtime processing logic that you care about corresponds to a relatively small percentage of your codebase.（重复劳动：花费大量时间来配置和部署）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Brittle&lt;/strong&gt;: There’s little fault-tolerance. You’re responsible for keeping each worker and queue up.（系统很脆弱：需要不停地检测并保证worker、queue都是存活的）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Painful to scale&lt;/strong&gt;: When the message throughput get too high for a single worker or queue, you need to partition how the data is spread around. You need to reconfigure the other workers to know the new locations to send messages. This introduces moving parts and new pieces that can fail.（扩展困难：流量上升后，添加worker、queue，需要重新进行处理逻辑配置，并且由于结构变得复杂，系统可靠性会降低）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although the queues and workers paradigm breaks down for large numbers of messages, message processing is clearly the fundamental paradigm for realtime computation. The question is: how do you do it in a way that doesn’t lose data, scales to huge volumes of messages, and is dead-simple to use and operate?
（&lt;strong&gt;问题是：能否实现一个方案，满足：不丢数据、易于扩展、使用和操作极其简单？&lt;/strong&gt;）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：paradigm? break down? fundamental paradigm?什么含义？&lt;/p&gt;

&lt;p&gt;Storm satisfies these goals.
（Storm就是这么一个方案）&lt;/p&gt;

&lt;h2 id=&quot;why-storm-is-important&quot;&gt;Why Storm is important&lt;/h2&gt;

&lt;p&gt;Storm exposes a set of primitives for doing realtime computation. Like how MapReduce greatly eases the writing of parallel batch processing, Storm’s primitives greatly ease the writing of parallel realtime computation.
（Storm暴漏了a set of primitives/原语，来进行realtime computation。就像MapReduce极大改善了parallel batch processing一样，Storm改善了parallel realtime computation）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：primitives？对于一个software，primitives什么含义？&lt;/p&gt;

&lt;p&gt;The key properties of Storm are:
（Storm的关键特性如下）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Extremely broad set of use cases&lt;/strong&gt;: Storm can be used for processing messages and updating databases (stream processing), doing a continuous query on data streams and streaming the results into clients (continuous computation), parallelizing an intense query like a search query on the fly (distributed RPC), and more. Storm’s small set of primitives satisfy a stunning number of use cases.（广泛的应用场景）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Storm scales to massive numbers of messages per second. To scale a topology, all you have to do is add machines and increase the parallelism settings of the topology. As an example of Storm’s scale, one of Storm’s initial applications processed 1,000,000 messages per second on a 10 node cluster, including hundreds of database calls per second as part of the topology. Storm’s usage of Zookeeper for cluster coordination makes it scale to much larger cluster sizes.（易于扩展：增加机器，配置并发数；备注：这跟使用zookeeper进行cluster管理相关。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Guarantees no data loss&lt;/strong&gt;: A realtime system must have strong guarantees about data being successfully processed. A system that drops data has a very limited set of use cases. Storm guarantees that every message will be processed, and this is in direct contrast with other systems like S4.（数据不丢失：Storm保证every message will be prcessed，而S4不行）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extremely robust&lt;/strong&gt;: Unlike systems like Hadoop, which are notorious for being difficult to manage, Storm clusters just work. It is an explicit goal of the Storm project to make the user experience of managing Storm clusters as painless as possible.（极其健壮：Storm Cluster非常易于管理，这也是其设计的初衷。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault-tolerant&lt;/strong&gt;: If there are faults during execution of your computation, Storm will reassign tasks as necessary. Storm makes sure that a computation can run forever (or until you kill the computation).（容错性：出错后，storm会reassign task。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Programming language agnostic&lt;/strong&gt;: Robust and scalable realtime processing shouldn’t be limited to a single platform. Storm topologies and processing components can be defined in any language, making Storm accessible to nearly anyone.（语言无关性：Storm topologies、processing components可使用多种语言实现）&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;Apache Storm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Apache Storm: Documentation Rationale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1 Documentation：Design</title>
     <link href="http://ningg.github.com/kafka-documentation-design"/>
     <updated>2014-10-18T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-documentation-design</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1 Documentation：Getting Started</title>
     <link href="http://ningg.github.com/kafka-documentation"/>
     <updated>2014-10-18T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-documentation</id>
     <content type="html">&lt;h2 id=&quot;getting-started&quot;&gt;1. Getting Started&lt;/h2&gt;

&lt;h3 id=&quot;introduction&quot;&gt;1.1 Introduction&lt;/h3&gt;

&lt;p&gt;Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：distributed, partitioned, replicated commit log service?&lt;/p&gt;

&lt;p&gt;First let’s review some basic messaging terminology:（几个messaging概念）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka maintains feeds of messages in categories called topics.（按topic来分类message？）&lt;/li&gt;
  &lt;li&gt;We’ll call processes that publish messages to a Kafka topic producers.（调用process，向topic producer中写message）&lt;/li&gt;
  &lt;li&gt;We’ll call processes that subscribe to topics and process the feed of published messages consumers..&lt;/li&gt;
  &lt;li&gt;Kafka is run as a cluster comprised of one or more servers each of which is called a broker.（kafka集群由borker构成）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:（producer向kafka集群写入message，consumer从kafka集群中读取message）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/producer_consumer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Communication between the clients and the servers is done with a simple, high-performance, language agnostic &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol&quot;&gt;TCP protocol&lt;/a&gt;. We provide a Java client for Kafka, but clients are available in &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Clients&quot;&gt;many languages&lt;/a&gt;.（client与server之间通过TCP协议通信，默认为kafka提供了java client，当然也可以用其他语言实现client）&lt;/p&gt;

&lt;h4 id=&quot;topics-and-logs&quot;&gt;Topics and Logs&lt;/h4&gt;

&lt;p&gt;A topic is a category or feed name to which messages are published. For each topic, the Kafka cluster maintains a partitioned log that looks like this:（topic，就是category、feed name，message按此分开存放；每个topic，对应一个partitioned log）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/log_anatomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each partition is an ordered, immutable sequence of messages that is continually appended to—a commit log. The messages in the partitions are each assigned a sequential id number called the offset that uniquely identifies each message within the partition.（partition是ordered、immutable sequence of message，其中的message被唯一标识，partition对应 a commit log）&lt;/p&gt;

&lt;p&gt;The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. For example if the log retention is set to two days, then for the two days after a message is published it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so retaining lots of data is not a problem.（在一段可配置的时间内，kafka始终保存所有的published messages，即使message已经被consume；Kafka对data size不敏感，lots of data对performance造成太大影响）&lt;/p&gt;

&lt;p&gt;In fact the only metadata retained on a per-consumer basis is the position of the consumer in the log, called the “offset”. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages, but in fact the position is controlled by the consumer and it can consume messages in any order it likes. For example a consumer can reset to an older offset to reprocess.
（on a per-consumer basis，只需保存元数据：consumer在log中的position，即，offset；这个offset完全由consumer自己决定，offset默认是顺序递增，但实际上consumer可以任意调整。）&lt;/p&gt;

&lt;p&gt;This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers.（总之，consumer在kafka中非常cheap：随意的come and go，对系统影响很小，consumer相互之间的影响也很小）&lt;/p&gt;

&lt;p&gt;The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit.（对log分partition，有几点目的：1.single server支撑较大的log，单个partition受到server的限制，但partition的数量不受限；2.多partition可以支撑并发处理，每个partition作为一个unit。）&lt;/p&gt;

&lt;h4 id=&quot;distribution&quot;&gt;Distribution&lt;/h4&gt;

&lt;p&gt;The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.
（partition分布式存储，方便共享；同时可配置每个patition的复制份数，以提升系统可靠性）&lt;/p&gt;

&lt;p&gt;Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.
（每个partition都对应一个server担当”leader”角色，也可能有其他server担当”follower”角色；leader负责所有的Read、write；follower只replicate the leader；如果leader崩溃，则自动推选一个follower升级为leader；server只对其上的部分partition充当leader角色，方便cluster的均衡。）&lt;/p&gt;

&lt;h4 id=&quot;producers&quot;&gt;Producers&lt;/h4&gt;

&lt;p&gt;Producers publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the message). More on the use of partitioning in a second.
（producer复制将message分发到相应的topic，具体：1.将message分发到哪个topic的哪个partition，常用方式，轮询、函数；）&lt;/p&gt;

&lt;h4 id=&quot;consumers&quot;&gt;Consumers&lt;/h4&gt;

&lt;p&gt;Messaging traditionally has two models: &lt;a href=&quot;http://en.wikipedia.org/wiki/Message_queue&quot;&gt;queuing&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern&quot;&gt;publish-subscribe&lt;/a&gt;. In a queue, a pool of consumers may read from a server and each message goes to one of them; in publish-subscribe the message is broadcast to all consumers. Kafka offers a single consumer abstraction that generalizes both of these—the consumer group.
（messaging，消息发送，由两种方式：queuing、publish-subscribe。Queuing，message发送到某一个consumer；publish-subscribe，message广播到所有的consumers。Kafka，通过将consumer泛化为consumer group，来支持这两种方式）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：publish-subscribe，发布-订阅模式的含义？&lt;/p&gt;

&lt;p&gt;Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.
（每个consumer都标记有consumer group name，每个message都被分发给subscribing consumer group中的一个consumer instance，consumer instances可以是不同的进程，也可以分布在不同的物理机器上。）&lt;/p&gt;

&lt;p&gt;If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.（若所有的consumer都属于同一个consumer group，则，情况变为：queue的负载均衡？）&lt;/p&gt;

&lt;p&gt;If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.
（若所有的consumer都属不同的consumer group，则，情况变为：publish-subscribe，message广播发送到所有consumer）&lt;/p&gt;

&lt;p&gt;More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is cluster of consumers instead of a single process.
（topics只对应少数的consumer group，即，consumer group类似&lt;code&gt;logical subscriber&lt;/code&gt;；每个group中有多个consumer，目的是提升可扩展性、容错能力）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：consumer group下有多个consumer？这些consumer怎么调用的？相互之间有什么差异？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/consumer-groups.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kafka has stronger ordering guarantees than a traditional messaging system, too.（kafka有strong ordering guarantees）&lt;/p&gt;

&lt;p&gt;A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored. However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the messages is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.
（message queue中存放的message，按照顺序发送到不同的consumers，但是这些发送是异步的，因此，后发送的message可能先到达consumer，即，并行处理时，有可能message乱序。现有的Messaging system，常用&lt;code&gt;exclusive consumer&lt;/code&gt;，独占消费，仅仅启动一个process来读取一个queue中的数据，此时，就无法实现并行处理。）&lt;/p&gt;

&lt;p&gt;Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances than partitions.
（Kafka，采用&lt;code&gt;partition&lt;/code&gt;的方式解决上述问题：每个partition被指定给topic对应的consumer group中的特定的consumer，这样能保证一点：一个partition中的message被顺序处理。由于有多个partition，并且对应多个consumer instance来处理，从而实现负载均衡；特别注意：consumer instance个数不能多于partitions个数）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message是怎么分配到topic对应的partition中的？consumer instance为什么不能多于partition个数？&lt;/p&gt;

&lt;p&gt;Kafka only provides a total order over messages within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process.
（Kafka只保证partition内mesaage的顺序处理，不保证partition之间的处理顺序。per-partition ordering和partition data by key，满足了大部分需求。如果要保证所有message顺序处理，则，将topic设置为only one partition，此时，变为串行处理。）&lt;/p&gt;

&lt;h4 id=&quot;guarantees&quot;&gt;Guarantees&lt;/h4&gt;

&lt;p&gt;At a high-level Kafka gives the following guarantees:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.（同一个producer发送到a particular topic partition的message，保证在partition中是有序的）&lt;/li&gt;
  &lt;li&gt;A consumer instance sees messages in the order they are stored in the log.（partition对应的commit log中message是有序的）&lt;/li&gt;
  &lt;li&gt;For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.（复制N份的topic，保证N-1份都丢失的情况下能够恢复。）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details on these guarantees are given in the design section of the documentation.&lt;/p&gt;

&lt;h3 id=&quot;use-cases&quot;&gt;1.2 Use Cases&lt;/h3&gt;

&lt;p&gt;Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see &lt;a href=&quot;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;this blog post&lt;/a&gt;.
（使用Kafka的典型场景，详细应用参考&lt;a href=&quot;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;this blog post&lt;/a&gt;）&lt;/p&gt;

&lt;h4 id=&quot;messaging&quot;&gt;Messaging&lt;/h4&gt;

&lt;p&gt;Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.
（替换传统的message broker/消息代理，其基本用途：解耦processing和data producer，缓存message，etc。）&lt;/p&gt;

&lt;p&gt;In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.
（实验发现messaging过程中，对broker的吞吐量要求不高，但要求低延迟、高可靠，这些kafka都满足。）&lt;/p&gt;

&lt;p&gt;In this domain Kafka is comparable to traditional messaging systems such as &lt;a href=&quot;http://activemq.apache.org/&quot;&gt;ActiveMQ&lt;/a&gt; or &lt;a href=&quot;https://www.rabbitmq.com/&quot;&gt;RabbitMQ&lt;/a&gt;.
（在messaging方面，Kafka的性能可与ActiveMQ、RabbitMQ相匹敌。）&lt;/p&gt;

&lt;h4 id=&quot;website-activity-tracking&quot;&gt;Website Activity Tracking&lt;/h4&gt;

&lt;p&gt;The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.&lt;/p&gt;

&lt;p&gt;Activity tracking is often very high volume as many activity messages are generated for each user page view.
（活动追踪，数据流量很大）&lt;/p&gt;

&lt;h4 id=&quot;metrics&quot;&gt;Metrics&lt;/h4&gt;

&lt;p&gt;Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.
（运行状态监控系统，从分布式应用中，汇总统计数据，形成集中的运行监控数据）&lt;/p&gt;

&lt;h4 id=&quot;log-aggregation&quot;&gt;Log Aggregation&lt;/h4&gt;

&lt;p&gt;Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.
（收集不同物理机器上的log，汇总到a central place：a file server or HDFS。与 Scribe or Flume相比，Kafka提供相当的performance、可靠性、低延迟。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：日志收集方面，Kafka的性能与Flume相当？Kafka能取代掉Flume吗？&lt;/p&gt;

&lt;h4 id=&quot;stream-processing&quot;&gt;Stream Processing&lt;/h4&gt;

&lt;p&gt;Many users end up doing stage-wise processing of data where data is consumed from topics of raw data and then aggregated, enriched, or otherwise transformed into new Kafka topics for further consumption. For example a processing flow for article recommendation might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might help normalize or deduplicate this content to a topic of cleaned article content; a final stage might attempt to match this content to users. This creates a graph of real-time data flow out of the individual topics. &lt;a href=&quot;https://github.com/nathanmarz/storm&quot;&gt;Storm&lt;/a&gt; and &lt;a href=&quot;http://samza.incubator.apache.org/&quot;&gt;Samza&lt;/a&gt; are popular frameworks for implementing these kinds of transformations.
（在Stream Processing中，Kafka担当data存储功能，即，raw data存储到Kafka中，consumer处理后的结果存储到new kafka topics中）&lt;/p&gt;

&lt;h4 id=&quot;event-sourcing&quot;&gt;Event Sourcing&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/eaaDev/EventSourcing.html&quot;&gt;Event sourcing&lt;/a&gt; is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka’s support for very large stored log data makes it an excellent backend for an application built in this style.
（Event sourcing，事件溯源，记录不同时间点的应用状态变化，通常log数据很大，Kafka满足此需求）&lt;/p&gt;

&lt;h4 id=&quot;commit-log&quot;&gt;Commit Log&lt;/h4&gt;

&lt;p&gt;Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The &lt;a href=&quot;http://kafka.apache.org/documentation.html#compaction&quot;&gt;log compaction&lt;/a&gt; feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.&lt;/p&gt;

&lt;h3 id=&quot;quick-start&quot;&gt;1.3 Quick Start&lt;/h3&gt;

&lt;p&gt;This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
（新手入门，对Kafka、Zookeeper一知半解的人，看这儿就对了）&lt;/p&gt;

&lt;h4 id=&quot;step-1-download-the-code&quot;&gt;Step 1: Download the code&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz&quot;&gt;Download&lt;/a&gt; the 0.8.1.1 release and un-tar it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; tar -xzf kafka_2.9.2-0.8.1.1.tgz
&amp;gt; cd kafka_2.9.2-0.8.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-2-start-the-server&quot;&gt;Step 2: Start the server&lt;/h4&gt;

&lt;p&gt;Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.
（kafka自带了ZooKeeper，不推荐使用）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/zookeeper-server-start.sh config/zookeeper.properties
[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now start the Kafka server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-server-start.sh config/server.properties
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-3-create-a-topic&quot;&gt;Step 3: Create a topic&lt;/h4&gt;

&lt;p&gt;Let’s create a topic named “test” with a single partition and only one replica:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see that topic if we run the list topic command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --list --zookeeper localhost:2181
test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.
（可通过配置文件，让broker自动创建topic）&lt;/p&gt;

&lt;h4 id=&quot;step-4-send-some-messages&quot;&gt;Step 4: Send some messages&lt;/h4&gt;

&lt;p&gt;Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.
（kafka自带了一个工具，能够将file或者standard input作为输入，按行传送到kafka cluster中。）&lt;/p&gt;

&lt;p&gt;Run the producer and then type a few messages into the console to send to the server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
This is a message
This is another message
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-5-start-a-consumer&quot;&gt;Step 5: Start a consumer&lt;/h4&gt;

&lt;p&gt;Kafka also has a command line consumer that will dump out messages to standard output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
This is a message
This is another message
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.&lt;/p&gt;

&lt;p&gt;All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.
（所有命令行，不夹带参数启动时，会自动弹出usage info）&lt;/p&gt;

&lt;h4 id=&quot;step-6-setting-up-a-multi-broker-cluster&quot;&gt;Step 6: Setting up a multi-broker cluster&lt;/h4&gt;

&lt;p&gt;So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).
（上述例子中，只启动了一个broker，其最多能够启动几个broker instances。下面说一下如何启动多个broker，构造cluster）&lt;/p&gt;

&lt;p&gt;First we make a config file for each of the brokers:（为每个broker，设定属性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; cp config/server.properties config/server-1.properties 
&amp;gt; cp config/server.properties config/server-2.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now edit these new files and set the following properties:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config/server-1.properties:
	broker.id=1
	port=9093
	log.dir=/tmp/kafka-logs-1
 
config/server-2.properties:
	broker.id=2
	port=9094
	log.dir=/tmp/kafka-logs-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;broker.id&lt;/code&gt; property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.
（不同的broker，应该设置不同的&lt;code&gt;port&lt;/code&gt;和&lt;code&gt;log.dir&lt;/code&gt;，否则，broker的数据会相互覆盖。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：同一台物理主机上，可以启动多个node，每个node通过&lt;code&gt;broker.id&lt;/code&gt;唯一标识。&lt;/p&gt;

&lt;p&gt;We already have Zookeeper and our single node started, so we just need to start the two new nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-server-start.sh config/server-1.properties &amp;amp;
...
&amp;gt; bin/kafka-server-start.sh config/server-2.properties &amp;amp;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a new topic with a replication factor of three:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay but now that we have a cluster how can we know which broker is doing what? To see that run the &lt;code&gt;describe topics&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.
（&lt;code&gt;describe topics&lt;/code&gt;命令的输出结果说明：first line，partition的汇总信息；remaining lines 分别说明每个partition的详细信息）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;leader&lt;/strong&gt; is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;replicas&lt;/strong&gt; is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive. （备份当前partition的node列表，包含当前已经不再存活的node）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;isr&lt;/strong&gt; is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.（&lt;code&gt;replicas&lt;/code&gt;内的node中，存活的node列表）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;leader&lt;/code&gt;后的数字&lt;code&gt;1&lt;/code&gt;，对应的含义？leader是怎么标识的？node怎么标识的？&lt;/p&gt;

&lt;p&gt;Note that in my example node 1 is the leader for the only partition of the topic.
We can run the same command on the original topic we created to see where it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.&lt;/p&gt;

&lt;p&gt;Let’s publish a few messages to our new topic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
...
my test message 1
my test message 2
^C 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s consume these messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:（验证kafka的容错性：kill leader）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ps | grep server-1.properties
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java...
&amp;gt; kill -9 7564
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:（leader终止后，slave自动升级为leader）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But the messages are still be available for consumption even though the leader that took the writes originally is down:（新选出的leader，对用户是透明的，consumer感觉不到异常）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ecosystem&quot;&gt;1.4 Ecosystem&lt;/h3&gt;

&lt;p&gt;There are a plethora of tools that integrate with Kafka outside the main distribution. The &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;ecosystem page&lt;/a&gt; lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.
（有很多工具与Kafka集成，参考&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;页面&lt;/a&gt;）&lt;/p&gt;

&lt;h3 id=&quot;upgrading-from-previous-versions&quot;&gt;1.5 Upgrading From Previous Versions&lt;/h3&gt;

&lt;h4 id=&quot;upgrading-from-080-to-081&quot;&gt;Upgrading from 0.8.0 to 0.8.1&lt;/h4&gt;

&lt;p&gt;0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.&lt;/p&gt;

&lt;h4 id=&quot;upgrading-from-07&quot;&gt;Upgrading from 0.7&lt;/h4&gt;

&lt;p&gt;0.8, the release in which added replication, was our first backwards-incompatible release: major changes were made to the API, ZooKeeper data structures, and protocol, and configuration. The upgrade from 0.7 to 0.8.x requires a &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8&quot;&gt;special tool&lt;/a&gt; for migration. This migration can be done without downtime.&lt;/p&gt;

&lt;h2 id=&quot;api&quot;&gt;2. API&lt;/h2&gt;

&lt;h3 id=&quot;producer-api&quot;&gt;2.1 Producer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;high-level-consumer-api&quot;&gt;2.2 High Level Consumer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;simple-consumer-api&quot;&gt;2.3 Simple Consumer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;kafka-hadoop-consumer-api&quot;&gt;2.4 Kafka Hadoop Consumer API&lt;/h3&gt;

&lt;p&gt;Providing a horizontally scalable solution for aggregating and loading data into Hadoop was one of our basic use cases. To support this use case, we provide a Hadoop-based consumer which spawns off many map tasks to pull data from the Kafka cluster in parallel. This provides extremely fast pull-based Hadoop data load capabilities (we were able to fully saturate the network with only a handful of Kafka servers).
（Hadoop-based consumer，并行的从Kafka cluster中pull data，速度很快）&lt;/p&gt;

&lt;p&gt;Usage information on the hadoop consumer can be found &lt;a href=&quot;https://github.com/linkedin/camus/tree/camus-kafka-0.8/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;3. Configuration&lt;/h2&gt;

&lt;p&gt;Kafka uses key-value pairs in the &lt;a href=&quot;http://en.wikipedia.org/wiki/.properties&quot;&gt;property file format&lt;/a&gt; for configuration. These values can be supplied either from a file or programmatically.&lt;/p&gt;

&lt;h3 id=&quot;broker-configs&quot;&gt;3.1 Broker Configs&lt;/h3&gt;

&lt;p&gt;The essential configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;broker.id&lt;/li&gt;
  &lt;li&gt;log.dirs&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#brokerconfigs&quot;&gt;Broker Configs&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;consumer-configs&quot;&gt;3.2 Consumer Configs&lt;/h3&gt;

&lt;p&gt;The essential consumer configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;group.id&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#consumerconfigs&quot;&gt;Consumer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;producer-configs&quot;&gt;3.3 Producer Configs&lt;/h3&gt;

&lt;p&gt;Essential configuration properties for the producer include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list&lt;/li&gt;
  &lt;li&gt;request.required.acks&lt;/li&gt;
  &lt;li&gt;producer.type&lt;/li&gt;
  &lt;li&gt;serializer.class&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#producerconfigs&quot;&gt;Producer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;new-producer-configs&quot;&gt;3.4 New Producer Configs&lt;/h3&gt;

&lt;p&gt;We are working on a replacement for our existing producer. The code is available in trunk now and can be considered beta quality. Below is the configuration for the new producer.&lt;/p&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#newproducerconfigs&quot;&gt;New Producer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume 1.5.0.1 User Guide：Flume Sources</title>
     <link href="http://ningg.github.com/flume-user-guide-source"/>
     <updated>2014-10-17T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-user-guide-source</id>
     <content type="html">&lt;h2 id=&quot;avro-source&quot;&gt;Avro Source&lt;/h2&gt;

&lt;p&gt;Listens on Avro port and receives events from external Avro client streams. When paired with the built-in Avro Sink on another (previous hop) Flume agent, it can create tiered collection topologies. Required properties are in bold.（必须属性加黑了）&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be &lt;code&gt;avro&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;bind&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;hostname or IP address to listen on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;port&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Port # to bind to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;threads&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Maximum number of worker threads to spawn&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space-separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;compression-type&lt;/td&gt;
      &lt;td&gt;none&lt;/td&gt;
      &lt;td&gt;This can be “none” or “deflate”. The compression-type must match the compression-type of matching AvroSource&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ssl&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Set this to true to enable SSL encryption. You must also specify a “keystore” and a “keystore-password”.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;This is the path to a Java keystore file. Required for SSL.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore-password&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The password for the Java keystore. Required for SSL.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore-type&lt;/td&gt;
      &lt;td&gt;JKS	The type of the Java keystore. This can be “JKS” or “PKCS12”.&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ipFilter&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Set this to true to enable ipFiltering for netty&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ipFilter.rules&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Define N netty ipFilter pattern rules with this config.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example of ipFilter.rules&lt;/p&gt;

&lt;p&gt;ipFilter.rules defines N netty ipFilters separated by a comma(&lt;code&gt;,&lt;/code&gt;) a pattern rule must be in this format.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;`allow` or `deny`&amp;gt;:&amp;lt;`ip` or `name` for computer name&amp;gt;:&amp;lt;`pattern`&amp;gt; 
allow/deny:ip/name:pattern

# example
ipFilter.rules=allow:ip:127.*,allow:name:localhost,deny:ip:*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the first rule to match will apply as the example below shows from a client on the localhost（从左向右，第一个匹配出的rules生效）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# This will Allow the client on localhost be deny clients from any other ip 
ipFilter.rules = allow:name:localhost,deny:ip:

# This will deny the client on localhost be allow clients from any other ip 
ipFilter.rules = deny:name:localhost,allow:ip:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;thrift-source&quot;&gt;Thrift Source&lt;/h2&gt;

&lt;p&gt;Listens on Thrift port and receives events from external Thrift client streams. When paired with the built-in ThriftSink on another (previous hop) Flume agent, it can create tiered collection topologies. Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be thrift&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;bind&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;hostname or IP address to listen on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;port&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Port # to bind to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;threads&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Maximum number of worker threads to spawn&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = thrift
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;exec-source&quot;&gt;Exec Source&lt;/h2&gt;

&lt;p&gt;Exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as &lt;code&gt;cat [named pipe]&lt;/code&gt; or &lt;code&gt;tail -F [file]&lt;/code&gt; are going to produce the desired results where as &lt;code&gt;date&lt;/code&gt; will probably not - the former two commands produce streams of data where as the latter produces a single event and exits.（捕获命令的输出，并按行处理，当&lt;code&gt;logStdErr&lt;/code&gt;设为true时，也将捕获stderr的输出；）&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be exec&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;command&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The command to execute&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shell&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;A shell invocation used to run the command. e.g. /bin/sh -c. Required only for commands relying on shell features like wildcards, back ticks, pipes etc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restartThrottle&lt;/td&gt;
      &lt;td&gt;10000&lt;/td&gt;
      &lt;td&gt;Amount of time (in millis) to wait before attempting a restart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restart&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Whether the executed cmd should be restarted if it dies&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;logStdErr&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Whether the command’s stderr should be logged&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batchSize&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;The max number of lines to read and send to the channel at a time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt;replicating&lt;/td&gt;
      &lt;td&gt;replicating or multiplexing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Depends on the selector.type value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space-separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;： The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost. As a for instance, one of the most commonly requested features is the &lt;code&gt;tail -F [file]&lt;/code&gt;-like use case where an application writes to a log file on disk and Flume tails the file, sending each line as an event. While this is possible, there’s an obvious problem; what happens if the channel fills up and Flume can’t send an event? Flume has no way of indicating to the application writing the log file that it needs to retain the log or that the event hasn’t been sent, for some reason. If this doesn’t make sense, you need only know this: Your application can never guarantee data has been received when using a unidirectional asynchronous interface such as ExecSource! As an extension of this warning - and to be completely clear - there is absolutely zero guarantee of event delivery when using this source. For stronger reliability guarantees, consider the Spooling Directory Source or direct integration with Flume via the SDK.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：ExecSource方式，当command异常退出后，会丢失数据。解决办法：考虑Spooling Directory Source或者通过SDK直接与Flume集成。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;：You can use ExecSource to emulate TailSource from Flume 0.9x (flume og). Just use unix command tail -F /full/path/to/your/file. Parameter -F is better in this case than -f as it will also follow file rotation.（Flume 0.9x版本中，可以使用 &lt;code&gt;tail -F path&lt;/code&gt;命令模仿 TailSource）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = exec
# follow file rotation
a1.sources.r1.command = tail -F /var/log/secure
a1.sources.r1.channels = c1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ‘shell’ config is used to invoke the ‘command’ through a command shell (such as Bash or Powershell). The ‘command’ is passed as an argument to ‘shell’ for execution. This allows the ‘command’ to use features from the shell such as wildcards, back ticks, pipes, loops, conditionals etc. In the absence of the ‘shell’ config, the ‘command’ will be invoked directly. Common values for ‘shell’ : ‘/bin/sh -c’, ‘/bin/ksh -c’, ‘cmd /c’, ‘powershell -Command’, etc.（启用shell选项时，系统会将command当作参数，传送给shell执行，此时，能利用不同shell的特性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;agent_foo.sources.tailsource-1.type = exec
agent_foo.sources.tailsource-1.shell = /bin/bash -c
agent_foo.sources.tailsource-1.command = for i in /path/*.txt; do cat $i; done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jms-source&quot;&gt;JMS Source&lt;/h2&gt;

&lt;p&gt;JMS Source reads messages from a JMS destination such as a queue or topic. Being a JMS application it should work with any JMS provider but has only been tested with ActiveMQ. The JMS source provides configurable batch size, message selector, user/pass, and message to flume event converter. Note that the vendor provided JMS jars should be included in the Flume classpath using plugins.d directory (preferred), –classpath on command line, or via FLUME_CLASSPATH variable in flume-env.sh.&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be jms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;initialContextFactory&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Inital Context Factory, e.g: org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;connectionFactory&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The JNDI name the connection factory shoulld appear as&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;providerURL&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The JMS provider URL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;destinationName&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Destination name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;destinationType&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Destination type (queue or topic)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;messageSelector&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Message selector to use when creating the consumer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;userName&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Username for the destination/provider&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;passwordFile&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;File containing the password for the destination/provider&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batchSize&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;Number of messages to consume in one batch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.type&lt;/td&gt;
      &lt;td&gt;DEFAULT&lt;/td&gt;
      &lt;td&gt;Class to use to convert messages to flume events. See below.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.*&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Converter properties.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.charset&lt;/td&gt;
      &lt;td&gt;UTF-8&lt;/td&gt;
      &lt;td&gt;Default converter only. Charset to use when converting JMS TextMessages to byte arrays.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：JMS是java方面的消息队列？做几年java了，对这个我还不清楚~~&lt;/p&gt;

&lt;h3 id=&quot;converter&quot;&gt;Converter&lt;/h3&gt;

&lt;p&gt;The JMS source allows pluggable converters, though it’s likely the default converter will work for most purposes. The default converter is able to convert Bytes, Text, and Object messages to FlumeEvents. In all cases, the properties in the message are added as headers to the FlumeEvent.（默认，message的properties会转换为FlumeEvent的headers）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BytesMessage: Bytes of message are copied to body of the FlumeEvent. Cannot convert more than 2GB of data per message.&lt;/li&gt;
  &lt;li&gt;TextMessage: Text of message is converted to a byte array and copied to the body of the FlumeEvent. The default converter uses UTF-8 by default but this is configurable.&lt;/li&gt;
  &lt;li&gt;ObjectMessage: Object is written out to a ByteArrayOutputStream wrapped in an ObjectOutputStream and the resulting array is copied to the body of the FlumeEvent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = jms
a1.sources.r1.channels = c1
a1.sources.r1.initialContextFactory = org.apache.activemq.jndi.ActiveMQInitialContextFactory
a1.sources.r1.connectionFactory = GenericConnectionFactory
a1.sources.r1.providerURL = tcp://mqserver:61616
a1.sources.r1.destinationName = BUSINESS_DATA
a1.sources.r1.destinationType = QUEUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spooling-directory-source&quot;&gt;Spooling Directory Source&lt;/h2&gt;

&lt;p&gt;This source lets you ingest data by placing files to be ingested into a “spooling” directory on disk. This source will watch the specified directory for new files, and will parse events out of new files as they appear. The event parsing logic is pluggable. After a given file has been fully read into the channel, it is renamed to indicate completion (or optionally deleted).&lt;/p&gt;

&lt;p&gt;Unlike the Exec source, this source is reliable and will not miss data, even if Flume is restarted or killed. In exchange for this reliability, only immutable, uniquely-named files must be dropped into the spooling directory. Flume tries to detect these problem conditions and will fail loudly if they are violated:&lt;/p&gt;

&lt;p&gt;If a file is written to after being placed into the spooling directory, Flume will print an error to its log file and stop processing.
If a file name is reused at a later time, Flume will print an error to its log file and stop processing.
To avoid the above issues, it may be useful to add a unique identifier (such as a timestamp) to log file names when they are moved into the spooling directory.&lt;/p&gt;

&lt;p&gt;Despite the reliability guarantees of this source, there are still cases in which events may be duplicated if certain downstream failures occur. This is consistent with the guarantees offered by other Flume components.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be spooldir.
spoolDir	–	The directory from which to read files from.
fileSuffix	.COMPLETED	Suffix to append to completely ingested files
deletePolicy	never	When to delete completed files: never or immediate
fileHeader	false	Whether to add a header storing the absolute path filename.
fileHeaderKey	file	Header key to use when appending absolute path filename to event header.
basenameHeader	false	Whether to add a header storing the basename of the file.
basenameHeaderKey	basename	Header Key to use when appending basename of file to event header.
ignorePattern	^$	Regular expression specifying which files to ignore (skip)
trackerDir	.flumespool	Directory to store metadata related to processing of files. If this path is not an absolute path, then it is interpreted as relative to the spoolDir.
consumeOrder	oldest	In which order files in the spooling directory will be consumed oldest, youngest and random. In case of oldest and youngest, the last modified time of the files will be used to compare the files. In case of a tie, the file with smallest laxicographical order will be consumed first. In case of random any file will be picked randomly. When using oldest and youngest the whole directory will be scanned to pick the oldest/youngest file, which might be slow if there are a large number of files, while using random may cause old files to be consumed very late if new files keep coming in the spooling directory.
maxBackoff	4000	The maximum time (in millis) to wait between consecutive attempts to write to the channel(s) if the channel is full. The source will start at a low backoff and increase it exponentially each time the channel throws a ChannelException, upto the value specified by this parameter.
batchSize	100	Granularity at which to batch transfer to the channel
inputCharset	UTF-8	Character set used by deserializers that treat the input file as text.
decodeErrorPolicy	FAIL	What to do when we see a non-decodable character in the input file. FAIL: Throw an exception and fail to parse the file. REPLACE: Replace the unparseable character with the “replacement character” char, typically Unicode U+FFFD. IGNORE: Drop the unparseable character sequence.
deserializer	LINE	Specify the deserializer used to parse the file into events. Defaults to parsing each line as an event. The class specified must implement EventDeserializer.Builder.
deserializer.*	 	Varies per event deserializer.
bufferMaxLines	–	(Obselete) This option is now ignored.
bufferMaxLineLength	5000	(Deprecated) Maximum length of a line in the commit buffer. Use deserializer.maxLineLength instead.
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for an agent named agent-1:&lt;/p&gt;

&lt;p&gt;agent-1.channels = ch-1
agent-1.sources = src-1&lt;/p&gt;

&lt;p&gt;agent-1.sources.src-1.type = spooldir
agent-1.sources.src-1.channels = ch-1
agent-1.sources.src-1.spoolDir = /var/log/apache/flumeSpool
agent-1.sources.src-1.fileHeader = true
Twitter 1% firehose Source (experimental)&lt;/p&gt;

&lt;p&gt;Warning This source is hightly experimental and may change between minor versions of Flume. Use at your own risk.
Experimental source that connects via Streaming API to the 1% sample twitter firehose, continously downloads tweets, converts them to Avro format and sends Avro events to a downstream Flume sink. Requires the consumer and access tokens and secrets of a Twitter developer account. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.twitter.TwitterSource
consumerKey	–	OAuth consumer key
consumerSecret	–	OAuth consumer secret
accessToken	–	OAuth access token
accessTokenSecret	–	OAuth toekn secret
maxBatchSize	1000	Maximum number of twitter messages to put in a single batch
maxBatchDurationMillis	1000	Maximum number of milliseconds to wait before closing a batch
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.twitter.TwitterSource
a1.sources.r1.channels = c1
a1.sources.r1.consumerKey = YOUR_TWITTER_CONSUMER_KEY
a1.sources.r1.consumerSecret = YOUR_TWITTER_CONSUMER_SECRET
a1.sources.r1.accessToken = YOUR_TWITTER_ACCESS_TOKEN
a1.sources.r1.accessTokenSecret = YOUR_TWITTER_ACCESS_TOKEN_SECRET
a1.sources.r1.maxBatchSize = 10
a1.sources.r1.maxBatchDurationMillis = 200
Event Deserializers&lt;/p&gt;

&lt;p&gt;The following event deserializers ship with Flume.&lt;/p&gt;

&lt;p&gt;LINE&lt;/p&gt;

&lt;p&gt;This deserializer generates one event per line of text input.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer.maxLineLength	2048	Maximum number of characters to include in a single event. If a line exceeds this length, it is truncated, and the remaining characters on the line will appear in a subsequent event.
deserializer.outputCharset	UTF-8	Charset to use for encoding events put into the channel.
AVRO&lt;/p&gt;

&lt;p&gt;This deserializer is able to read an Avro container file, and it generates one event per Avro record in the file. Each event is annotated with a header that indicates the schema used. The body of the event is the binary Avro record data, not including the schema or the rest of the container file elements.&lt;/p&gt;

&lt;p&gt;Note that if the spool directory source must retry putting one of these events onto a channel (for example, because the channel is full), then it will reset and retry from the most recent Avro container file sync point. To reduce potential event duplication in such a failure scenario, write sync markers more frequently in your Avro input files.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer.schemaType	HASH	How the schema is represented. By default, or when the value HASH is specified, the Avro schema is hashed and the hash is stored in every event in the event header “flume.avro.schema.hash”. If LITERAL is specified, the JSON-encoded schema itself is stored in every event in the event header “flume.avro.schema.literal”. Using LITERAL mode is relatively inefficient compared to HASH mode.
BlobDeserializer&lt;/p&gt;

&lt;p&gt;This deserializer reads a Binary Large Object (BLOB) per event, typically one BLOB per file. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because the entire BLOB is buffered in RAM.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder
deserializer.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request
NetCat Source&lt;/p&gt;

&lt;p&gt;A netcat-like source that listens on a given port and turns each line of text into an event. Acts like nc -k -l [host] [port]. In other words, it opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel.&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be netcat
bind	–	Host name or IP address to bind to
port	–	Port # to bind to
max-line-length	512	Max line length per event body (in bytes)
ack-every-event	true	Respond with an “OK” for every event received
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Sequence Generator Source&lt;/p&gt;

&lt;p&gt;A simple sequence generator that continuously generates events with a counter that starts from 0 and increments by 1. Useful mainly for testing. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be seq
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
batchSize	1	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = seq
a1.sources.r1.channels = c1
Syslog Sources&lt;/p&gt;

&lt;p&gt;Reads syslog data and generate Flume events. The UDP source treats an entire message as a single event. The TCP sources create a new event for each string of characters separated by a newline (‘n’).&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Syslog TCP Source&lt;/p&gt;

&lt;p&gt;The original, tried-and-true syslog TCP source.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be syslogtcp
host	–	Host name or IP address to bind to
port	–	Port # to bind to
eventSize	2500	Maximum size of a single event line, in bytes
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
For example, a syslog TCP source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = syslogtcp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
Multiport Syslog TCP Source&lt;/p&gt;

&lt;p&gt;This is a newer, faster, multi-port capable version of the Syslog TCP source. Note that the ports configuration setting has replaced port. Multi-port capability means that it can listen on many ports at once in an efficient manner. This source uses the Apache Mina library to do that. Provides support for RFC-3164 and many common RFC-5424 formatted messages. Also provides the capability to configure the character set used on a per-port basis.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be multiport_syslogtcp
host	–	Host name or IP address to bind to.
ports	–	Space-separated list (one or more) of ports to bind to.
eventSize	2500	Maximum size of a single event line, in bytes.
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
portHeader	–	If specified, the port number will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the incoming port.
charset.default	UTF-8	Default character set used while parsing syslog events into strings.
charset.port.&lt;port&gt;	–	Character set is configurable on a per-port basis.
batchSize	100	Maximum number of events to attempt to process per request loop. Using the default is usually fine.
readBufferSize	1024	Size of the internal Mina read buffer. Provided for performance tuning. Using the default is usually fine.
numProcessors	(auto-detected)	Number of processors available on the system for use while processing messages. Default is to auto-detect # of CPUs using the Java Runtime API. Mina will spawn 2 request-processing threads per detected CPU, which is often reasonable.
selector.type	replicating	replicating, multiplexing, or custom
selector.*	–	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors.
interceptors.*	 	 
For example, a multiport syslog TCP source for agent named a1:&lt;/port&gt;&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = multiport_syslogtcp
a1.sources.r1.channels = c1
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.ports = 10001 10002 10003
a1.sources.r1.portHeader = port
Syslog UDP Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be syslogudp
host	–	Host name or IP address to bind to
port	–	Port # to bind to
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
For example, a syslog UDP source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = syslogudp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
HTTP Source&lt;/p&gt;

&lt;p&gt;A source which accepts Flume Events by HTTP POST and GET. GET should be used for experimentation only. HTTP requests are converted into flume events by a pluggable “handler” which must implement the HTTPSourceHandler interface. This handler takes a HttpServletRequest and returns a list of flume events. All events handled from one Http request are committed to the channel in one transaction, thus allowing for increased efficiency on channels like the file channel. If the handler throws an exception, this source will return a HTTP status of 400. If the channel is full, or the source is unable to append events to the channel, the source will return a HTTP 503 - Temporarily unavailable status.&lt;/p&gt;

&lt;p&gt;All events sent in one post request are considered to be one batch and inserted into the channel in one transaction.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
type	 	The component type name, needs to be http
port	–	The port the source should bind to.
bind	0.0.0.0	The hostname or IP address to listen on
handler	org.apache.flume.source.http.JSONHandler	The FQCN of the handler class.
handler.*	–	Config parameters for the handler
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
enableSSL	false	Set the property true, to enable SSL
keystore	 	Location of the keystore includng keystore file name
keystorePassword Keystore password
For example, a http source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = http
a1.sources.r1.port = 5140
a1.sources.r1.channels = c1
a1.sources.r1.handler = org.example.rest.RestHandler
a1.sources.r1.handler.nickname = random props
JSONHandler&lt;/p&gt;

&lt;p&gt;A handler is provided out of the box which can handle events represented in JSON format, and supports UTF-8, UTF-16 and UTF-32 character sets. The handler accepts an array of events (even if there is only one event, the event has to be sent in an array) and converts them to a Flume event based on the encoding specified in the request. If no encoding is specified, UTF-8 is assumed. The JSON handler supports UTF-8, UTF-16 and UTF-32. Events are represented as follows.&lt;/p&gt;

&lt;p&gt;[{
  “headers” : {
             “timestamp” : “434324343”,
             “host” : “random_host.example.com”
             },
  “body” : “random_body”
  },
  {
  “headers” : {
             “namenode” : “namenode.example.com”,
             “datanode” : “random_datanode.example.com”
             },
  “body” : “really_random_body”
  }]
To set the charset, the request must have content type specified as application/json; charset=UTF-8 (replace UTF-8 with UTF-16 or UTF-32 as required).&lt;/p&gt;

&lt;p&gt;One way to create an event in the format expected by this handler is to use JSONEvent provided in the Flume SDK and use Google Gson to create the JSON string using the Gson#fromJson(Object, Type) method. The type token to pass as the 2nd argument of this method for list of events can be created by:&lt;/p&gt;

&lt;p&gt;Type type = new TypeToken&amp;lt;List&lt;jsonevent&gt;&amp;gt;() {}.getType();
BlobHandler&lt;/jsonevent&gt;&lt;/p&gt;

&lt;p&gt;By default HTTPSource splits JSON input into Flume events. As an alternative, BlobHandler is a handler for HTTPSource that returns an event that contains the request parameters as well as the Binary Large Object (BLOB) uploaded with this request. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because it buffers up the entire BLOB in RAM.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
handler	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobHandler
handler.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request
Legacy Sources&lt;/p&gt;

&lt;p&gt;The legacy sources allow a Flume 1.x agent to receive events from Flume 0.9.4 agents. It accepts events in the Flume 0.9.4 format, converts them to the Flume 1.0 format, and stores them in the connected channel. The 0.9.4 event properties like timestamp, pri, host, nanos, etc get converted to 1.x event header attributes. The legacy source supports both Avro and Thrift RPC connections. To use this bridge between two Flume versions, you need to start a Flume 1.x agent with the avroLegacy or thriftLegacy source. The 0.9.4 agent should have the agent Sink pointing to the host/port of the 1.x agent.&lt;/p&gt;

&lt;p&gt;Note The reliability semantics of Flume 1.x are different from that of Flume 0.9.x. The E2E or DFO mode of a Flume 0.9.x agent will not be supported by the legacy source. The only supported 0.9.x mode is the best effort, though the reliability setting of the 1.x flow will be applicable to the events once they are saved into the Flume 1.x channel by the legacy source.
Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Avro Legacy Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.avroLegacy.AvroLegacySource
host	–	The hostname or IP address to bind to
port	–	The port # to listen on
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.avroLegacy.AvroLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Thrift Legacy Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.thriftLegacy.ThriftLegacySource
host	–	The hostname or IP address to bind to
port	–	The port # to listen on
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.thriftLegacy.ThriftLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Custom Source&lt;/p&gt;

&lt;p&gt;A custom source is your own implementation of the Source interface. A custom source’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom source is its FQCN.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be your FQCN
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.example.MySource
a1.sources.r1.channels = c1
Scribe Source&lt;/p&gt;

&lt;p&gt;Scribe is another type of ingest system. To adopt existing Scribe ingest system, Flume should use ScribeSource based on Thrift with compatible transfering protocol. For deployment of Scribe please follow the guide from Facebook. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
type	–	The component type name, needs to be org.apache.flume.source.scribe.ScribeSource
port	1499	Port that Scribe should be connected
workerThreads	5	Handing threads number in Thrift
selector.type	 	 
selector.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.scribe.ScribeSource
a1.sources.r1.port = 1463
a1.sources.r1.workerThreads = 5
a1.sources.r1.channels = c1&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide 1.5.0.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume 1.5.0.1 User Guide：introduction</title>
     <link href="http://ningg.github.com/flume-user-guide"/>
     <updated>2014-10-17T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-user-guide</id>
     <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store.&lt;/p&gt;

&lt;p&gt;The use of Apache Flume is not only restricted to log data aggregation. Since data sources are customizable, Flume can be used to transport massive quantities of event data including but not limited to network traffic data, social-media-generated data, email messages and pretty much any data source possible.（数据源：网络流量数据、社交媒体产生的数据、email数据、其他数据，Flume都能收集）&lt;/p&gt;

&lt;p&gt;Apache Flume is a top level project at the Apache Software Foundation.&lt;/p&gt;

&lt;p&gt;There are currently two release code lines available, versions 0.9.x and 1.x.&lt;/p&gt;

&lt;p&gt;Documentation for the 0.9.x track is available at the &lt;a href=&quot;http://archive.cloudera.com/cdh/3/flume/UserGuide/&quot;&gt;Flume 0.9.x User Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This documentation applies to the 1.4.x track.&lt;/p&gt;

&lt;p&gt;New and existing users are encouraged to use the 1.x releases so as to leverage the performance improvements and configuration flexibilities available in the latest architecture.（推荐使用Flume 1.x版本：性能有改善、配置方便，使用了最新架构）&lt;/p&gt;

&lt;h3 id=&quot;system-requirements&quot;&gt;System Requirements&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Java Runtime Environment - Java 1.6 or later (Java 1.7 Recommended) （运行环境：JRE 1.6+，推荐JRE1.7）&lt;/li&gt;
  &lt;li&gt;Memory - Sufficient memory for configurations used by sources, channels or sinks&lt;/li&gt;
  &lt;li&gt;Disk Space - Sufficient disk space for configurations used by channels or sinks&lt;/li&gt;
  &lt;li&gt;Directory Permissions - Read/Write permissions for directories used by agent （agent需要R/W权限）&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;h4 id=&quot;data-flow-model&quot;&gt;Data flow model&lt;/h4&gt;

&lt;p&gt;A Flume event is defined as a unit of data flow having a byte payload and an optional set of string attributes. A Flume agent is a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop).（Flume Event是a unit of data flow having a byte payload 和几个属性集合；Flume Agent是JVM进程，将events flow从一端送到另一端）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image00.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Flume source consumes events delivered to it by an external source like a web server. The external source sends events to Flume in a format that is recognized by the target Flume source. For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink. A similar flow can be defined using a Thrift Flume Source to receive events from a Thrift Sink or a Flume Thrift Rpc Client or Thrift clients written in any language generated from the Flume thrift protocol.When a Flume source receives an event, it stores it into one or more channels. The channel is a passive store that keeps the event until it’s consumed by a Flume sink. The file channel is one example – it is backed by the local filesystem. The sink removes the event from the channel and puts it into an external repository like HDFS (via Flume HDFS sink) or forwards it to the Flume source of the next Flume agent (next hop) in the flow. The source and sink within the given agent run asynchronously with the events staged in the channel.（Source，接收外部data source的数据；Channel，被动接收Source的数据；Sink主动从Channel读取数据，并将其传递出去；利用Channel机制，Source、Sink实现异步处理）&lt;/p&gt;

&lt;h4 id=&quot;complex-flows&quot;&gt;Complex flows&lt;/h4&gt;

&lt;p&gt;Flume allows a user to build multi-hop flows where events travel through multiple agents before reaching the final destination. It also allows fan-in and fan-out flows, contextual routing and backup routes (fail-over) for failed hops.（Flume内flows支持fan-in、fan-out——多入多出，contextual touting和backup routes(fail-over)）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：contextual routing 和 backup routes的含义？&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;reliability&quot;&gt;Reliability&lt;/h4&gt;

&lt;p&gt;The events are staged in a channel on each agent. The events are then delivered to the next agent or terminal repository (like HDFS) in the flow. The events are removed from a channel only after they are stored in the channel of next agent or in the terminal repository. This is a how the single-hop message delivery semantics in Flume provide end-to-end reliability of the flow.（&lt;strong&gt;single-hop message delivery semantics&lt;/strong&gt;：Channel中的event仅在被成功处理之后，才从Channel中删掉。）&lt;/p&gt;

&lt;p&gt;Flume uses a transactional approach to guarantee the reliable delivery of the events. The sources and sinks encapsulate in a transaction the storage/retrieval, respectively, of the events placed in or provided by a transaction provided by the channel. This ensures that the set of events are reliably passed from point to point in the flow. In the case of a multi-hop flow, the sink from the previous hop and the source from the next hop both have their transactions running to ensure that the data is safely stored in the channel of the next hop.（&lt;strong&gt;multi-hop&lt;/strong&gt;：）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Flume如何保证事物操作？没看懂&lt;/p&gt;

&lt;h4 id=&quot;recoverability&quot;&gt;Recoverability&lt;/h4&gt;

&lt;p&gt;The events are staged in the channel, which manages recovery from failure. Flume supports a durable file channel which is backed by the local file system. There’s also a memory channel which simply stores the events in an in-memory queue, which is faster but any events still left in the memory channel when an agent process dies can’t be recovered.（Channel需保证崩溃后，能恢复events，具体：本地FS上保存durable file channel，另，占用一个in-memory queue，Channel进程崩溃后，能加快恢复速度；但，如果agent进程崩溃，将导致内存泄漏：无法回收这一内存）&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;h3 id=&quot;setting-up-an-agent&quot;&gt;Setting up an agent&lt;/h3&gt;

&lt;p&gt;Flume agent configuration is stored in a local configuration file. This is a text file that follows the Java properties file format. Configurations for one or more agents can be specified in the same configuration file. The configuration file includes properties of each source, sink and channel in an agent and how they are wired together to form data flows.（Agent利用config file设置：source、channel、sink的属性，以及不同Agent之间前后联系）&lt;/p&gt;

&lt;h4 id=&quot;configuring-individual-components&quot;&gt;Configuring individual components&lt;/h4&gt;

&lt;p&gt;Each component (source, sink or channel) in the flow has a name, type, and set of properties that are specific to the type and instantiation. For example, an Avro source needs a hostname (or IP address) and a port number to receive data from. A memory channel can have max queue size (“capacity”), and an HDFS sink needs to know the file system URI, path to create files, frequency of file rotation (“hdfs.rollInterval”) etc. All such attributes of a component needs to be set in the properties file of the hosting Flume agent.（设置Component的属性）&lt;/p&gt;

&lt;h4 id=&quot;wiring-the-pieces-together&quot;&gt;Wiring the pieces together&lt;/h4&gt;

&lt;p&gt;The agent needs to know what individual components to load and how they are connected in order to constitute the flow. This is done by listing the names of each of the sources, sinks and channels in the agent, and then specifying the connecting channel for each sink and source. For example, an agent flows events from an Avro source called avroWeb to HDFS sink hdfs-cluster1 via a file channel called file-channel. The configuration file will contain names of these components and file-channel as a shared channel for both avroWeb source and hdfs-cluster1 sink.（设置不同agent构成的topologies）&lt;/p&gt;

&lt;h4 id=&quot;starting-an-agent&quot;&gt;Starting an agent&lt;/h4&gt;

&lt;p&gt;An agent is started using a shell script called flume-ng which is located in the bin directory of the Flume distribution. You need to specify the agent name, the config directory, and the config file on the command line:（启动agent，需要指定参数：agent name、config dir、config file。）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the agent will start running source and sinks configured in the given properties file.&lt;/p&gt;

&lt;h4 id=&quot;a-simple-example&quot;&gt;A simple example&lt;/h4&gt;

&lt;p&gt;Here, we give an example configuration file, describing a single-node Flume deployment. This configuration lets a user generate events and subsequently logs them to the console.（场景：single-node模式，user产生events并且将其输出到控制台）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration defines a single agent named a1. a1 has a source that listens for data on port 44444, a channel that buffers event data in memory, and a sink that logs event data to the console. The configuration file names the various components, then describes their types and configuration parameters. A given configuration file might define several named agents; when a given Flume process is launched a flag is passed telling it which named agent to manifest.（一个配置文件中，可设定多个agents，Flume进程启动时，会指定agent运行）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：配置文件中，具体参数配置：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/flume-config-tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given this configuration file, we can start Flume as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that in a full deployment we would typically include one more option: –conf=&lt;conf-dir&gt;. The &lt;conf-dir&gt; directory would include a shell script flume-env.sh and potentially a log4j properties file. In this example, we pass a Java option to force Flume to log to the console and we go without a custom environment script.（实际开发场景下，通过`--conf=&lt;conf-dir&gt;`传入`&lt;conf-dir&gt;`，通常这一目录下应包含flume-env.sh文件和log4j的配置文件）&lt;/conf-dir&gt;&lt;/conf-dir&gt;&lt;/conf-dir&gt;&lt;/conf-dir&gt;&lt;/p&gt;

&lt;p&gt;From a separate terminal, we can then telnet port 44444 and send Flume an event:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ telnet localhost 44444
Trying 127.0.0.1...
Connected to localhost.localdomain (127.0.0.1).
Escape character is &#39;^]&#39;.
Hello world! &amp;lt;ENTER&amp;gt;
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The original Flume terminal will output the event in a log message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;12/06/19 15:32:19 INFO source.NetcatSource: Source starting
12/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]
12/06/19 15:32:34 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D          Hello world!. }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations - you’ve successfully configured and deployed a Flume agent! Subsequent sections cover agent configuration in much more detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：telnet通过命令行方式，能够发送字符？那么能与服务器交互吗？telnet命令方式下，典型应用场景有哪些？&lt;/p&gt;

&lt;h4 id=&quot;installing-third-party-plugins&quot;&gt;Installing third-party plugins&lt;/h4&gt;

&lt;p&gt;Flume has a fully plugin-based architecture. While Flume ships with many out-of-the-box sources, channels, sinks, serializers, and the like, many implementations exist which ship separately from Flume.&lt;/p&gt;

&lt;p&gt;While it has always been possible to include custom Flume components by adding their jars to the FLUME_CLASSPATH variable in the flume-env.sh file, Flume now supports a special directory called plugins.d which automatically picks up plugins that are packaged in a specific format. This allows for easier management of plugin packaging issues as well as simpler debugging and troubleshooting of several classes of issues, especially library dependency conflicts.（在flume-env.sh中向FLUME_CLASSPATH中添加plugin的位置；另一种方式，向&lt;code&gt;plugins.d&lt;/code&gt;目录下添加plugin，即可自动安装。）&lt;/p&gt;

&lt;h5 id=&quot;the-pluginsd-directory&quot;&gt;The plugins.d directory&lt;/h5&gt;

&lt;p&gt;The &lt;code&gt;plugins.d&lt;/code&gt; directory is located at &lt;code&gt;$FLUME_HOME/plugins.d&lt;/code&gt;. At startup time, the flume-ng start script looks in the &lt;code&gt;plugins.d&lt;/code&gt; directory for plugins that conform to the below format and includes them in proper paths when starting up &lt;code&gt;java&lt;/code&gt;.（系统其中前，自动预处理plugins.d下的plugin）&lt;/p&gt;

&lt;h5 id=&quot;directory-layout-for-plugins&quot;&gt;Directory layout for plugins&lt;/h5&gt;

&lt;p&gt;Each plugin (subdirectory) within plugins.d can have up to three sub-directories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;lib - the plugin’s jar(s)&lt;/li&gt;
  &lt;li&gt;libext - the plugin’s dependency jar(s)&lt;/li&gt;
  &lt;li&gt;native - any required native libraries, such as .so files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Example of two plugins within the plugins.d directory:（&lt;code&gt;plugins.d&lt;/code&gt;目录下，plugin的目录结构如下）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugins.d/
plugins.d/custom-source-1/
plugins.d/custom-source-1/lib/my-source.jar
plugins.d/custom-source-1/libext/spring-core-2.5.6.jar
plugins.d/custom-source-2/
plugins.d/custom-source-2/lib/custom.jar
plugins.d/custom-source-2/native/gettext.so
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;data-ingestion&quot;&gt;Data ingestion&lt;/h3&gt;

&lt;p&gt;Flume supports a number of mechanisms to ingest data from external sources.（从外部 sources 获取数据，Flume有多种方式）&lt;/p&gt;

&lt;h4 id=&quot;rpc&quot;&gt;RPC&lt;/h4&gt;

&lt;p&gt;An Avro client included in the Flume distribution can send a given file to Flume Avro source using avro RPC mechanism:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/flume-ng avro-client -H localhost -p 41414 -F /usr/logs/log.10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command will send the contents of &lt;code&gt;/usr/logs/log.10&lt;/code&gt; to to the Flume source listening on that ports.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：什么含义？以Avro方式，向localhost:41414发送文件？仅仅是Data Source？有一个单独的Flume Source在localhost:41414监听？&lt;/p&gt;

&lt;h4 id=&quot;executing-commands&quot;&gt;Executing commands&lt;/h4&gt;

&lt;p&gt;There’s an exec source that executes a given command and consumes the output. A single ‘line’ of output ie. text followed by carriage return (‘\r’) or line feed (‘\n’) or both together.（exec source，执行command并将output按行发送至Channel）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Flume does not support &lt;code&gt;tail&lt;/code&gt; as a source. One can wrap the &lt;code&gt;tail&lt;/code&gt; command in an exec source to stream the file.（无法直接使用tail，需要包装在exec source中。）&lt;/p&gt;

&lt;h4 id=&quot;network-streams&quot;&gt;Network streams&lt;/h4&gt;

&lt;p&gt;Flume supports the following mechanisms to read data from popular log stream types, such as:（下述方式支持从log system中读取stream）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Avro&lt;/li&gt;
  &lt;li&gt;Thrift&lt;/li&gt;
  &lt;li&gt;Syslog&lt;/li&gt;
  &lt;li&gt;Netcat&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：上面都是什么呐？&lt;/p&gt;

&lt;h3 id=&quot;setting-multi-agent-flow&quot;&gt;Setting multi-agent flow&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to flow the data across multiple agents or hops, the sink of the previous agent and source of the current hop need to be avro type with the sink pointing to the hostname (or IP address) and port of the source.（multiple agents时，previous agent中sink、current agent中source都应为avro类型，对应到相同的&lt;code&gt;IP:port&lt;/code&gt;）&lt;/p&gt;

&lt;h3 id=&quot;consolidation&quot;&gt;Consolidation&lt;/h3&gt;

&lt;p&gt;A very common scenario in log collection is a large number of log producing clients sending data to a few consumer agents that are attached to the storage subsystem. For example, logs collected from hundreds of web servers sent to a dozen of agents that write to HDFS cluster.（一个常见场景：大量client收集数据，写入一个集中系统）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be achieved in Flume by configuring a number of first tier agents with an avro sink, all pointing to an avro source of single agent (Again you could use the thrift sources/sinks/clients in such a scenario). This source on the second tier agent consolidates the received events into a single channel which is consumed by a sink to its final destination.（解决办法：为每个client分配一个agent，再第二层，使用一个agent进行合并，然后写入最后的集中存储系统；不适用single-agent中的multi-sources，因为multi-sources，要求sources必须与channel在同一物理机器上，即，一个agent必须在一个物理机器上）&lt;/p&gt;

&lt;h3 id=&quot;multiplexing-the-flow&quot;&gt;Multiplexing the flow&lt;/h3&gt;

&lt;p&gt;Flume supports multiplexing the event flow to one or more destinations. This is achieved by defining a flow multiplexer that can replicate or selectively route an event to one or more channels.（对agent扩展，定义多个channel，实现fan-out）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image01.png&quot; alt=&quot;A fan-out flow using a (multiplexing) channel selector&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above example shows a source from agent “foo” fanning out the flow to three different channels. This fan out can be replicating or multiplexing. In case of replicating flow, each event is sent to all three channels. For the multiplexing case, an event is delivered to a subset of available channels when an event’s attribute matches a preconfigured value. For example, if an event attribute called “txnType” is set to “customer”, then it should go to channel1 and channel3, if it’s “vendor” then it should go to channel2, otherwise channel3. The mapping can be set in the agent’s configuration file.（两种fan-out方式，replicating\multiplexing，即，复制和多路复用；replicating，输入复制到每个channel中一份；multiplexing，输入仅复制到匹配的channel中，相当于channel前加了个filter）&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;As mentioned in the earlier section, Flume agent configuration is read from a file that resembles a Java property file format with hierarchical property settings.（hierarchical property settings？）&lt;/p&gt;

&lt;h3 id=&quot;defining-the-flow&quot;&gt;Defining the flow&lt;/h3&gt;

&lt;p&gt;To define the flow within a single agent, you need to link the sources and sinks via a channel. （定义single-agent内的flow时，几点：）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;列出agent对应的sources、sinks、channels；&lt;/li&gt;
  &lt;li&gt;指定与source对应的channels，指定与sink对应的channel；&lt;/li&gt;
  &lt;li&gt;一个source可对应多个channel，一个sink只能对应一个channel； &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The format is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set channel for source
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt; ...

# set channel for sink
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink&amp;gt;.channel = &amp;lt;Channel1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：source绑定&lt;span style=&quot;color:red&quot;&gt;channels&lt;/span&gt;、sink绑定&lt;span style=&quot;color:red&quot;&gt;channel&lt;/span&gt;。疑问：单个agent中source只能有一个吗？如果single-agent中有多个source，那么是否也可以实现fan-in？&lt;/p&gt;

&lt;p&gt;For example, an agent named agent_foo is reading data from an external avro client and sending it to HDFS via a memory channel. The config file weblog.config could look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels for the agent
agent_foo.sources = avro-appserver-src-1
agent_foo.sinks = hdfs-sink-1
agent_foo.channels = mem-channel-1

# set channel for source
agent_foo.sources.avro-appserver-src-1.channels = mem-channel-1

# set channel for sink
agent_foo.sinks.hdfs-sink-1.channel = mem-channel-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will make the events flow from avro-AppSrv-source to hdfs-Cluster1-sink through the memory channel mem-channel-1. When the agent is started with the weblog.config as its config file, it will instantiate that flow.&lt;/p&gt;

&lt;h3 id=&quot;configuring-individual-components-1&quot;&gt;Configuring individual components&lt;/h3&gt;

&lt;p&gt;After defining the flow, you need to set properties of each source, sink and channel. This is done in the same hierarchical namespace fashion where you set the component type and other values for the properties specific to each component:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# properties for sources
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source&amp;gt;.&amp;lt;someProperty&amp;gt; = &amp;lt;someValue&amp;gt;

# properties for channels
&amp;lt;Agent&amp;gt;.channel.&amp;lt;Channel&amp;gt;.&amp;lt;someProperty&amp;gt; = &amp;lt;someValue&amp;gt;

# properties for sinks
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Sink&amp;gt;.&amp;lt;someProperty&amp;gt; = &amp;lt;someValue&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The property &lt;code&gt;type&lt;/code&gt; needs to be set for each component for Flume to understand what kind of object it needs to be. Each source, sink and channel type has its own set of properties required for it to function as intended. All those need to be set as needed. In the previous example, we have a flow from avro-AppSrv-source to hdfs-Cluster1-sink through the memory channel mem-channel-1. Here’s an example that shows configuration of each of those components:（不同的组件有不同的property，但都有&lt;code&gt;type&lt;/code&gt;属性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;agent_foo.sources = avro-AppSrv-source
agent_foo.sinks = hdfs-Cluster1-sink
agent_foo.channels = mem-channel-1

# set channel for sources, sinks

# properties of avro-AppSrv-source
agent_foo.sources.avro-AppSrv-source.type = avro
agent_foo.sources.avro-AppSrv-source.bind = localhost
agent_foo.sources.avro-AppSrv-source.port = 10000

# properties of mem-channel-1
agent_foo.channels.mem-channel-1.type = memory
agent_foo.channels.mem-channel-1.capacity = 1000
agent_foo.channels.mem-channel-1.transactionCapacity = 100

# properties of hdfs-Cluster1-sink
agent_foo.sinks.hdfs-Cluster1-sink.type = hdfs
agent_foo.sinks.hdfs-Cluster1-sink.hdfs.path = hdfs://namenode/flume/webdata

#...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;adding-multiple-flows-in-an-agent&quot;&gt;Adding multiple flows in an agent&lt;/h3&gt;

&lt;p&gt;A single Flume agent can contain several independent flows. You can list multiple sources, sinks and channels in a config. These components can be linked to form multiple flows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source1&amp;gt; &amp;lt;Source2&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink1&amp;gt; &amp;lt;Sink2&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：关于single-agent说几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;可包含多个sources、sinks、channels；&lt;/li&gt;
  &lt;li&gt;定义多个sources时，&lt;code&gt;source1&lt;/code&gt;和&lt;code&gt;source2&lt;/code&gt;间，空格间隔；&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:red&quot;&gt;sink与channel一一对应吗&lt;/span&gt;？&lt;/li&gt;
  &lt;li&gt;可包含多个相互独立的flow；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;疑问：source、channel、sink之间对应关系？&lt;code&gt;1：1&lt;/code&gt;？&lt;code&gt;1：n&lt;/code&gt;？&lt;code&gt;n：1&lt;/code&gt;？&lt;code&gt;n：n&lt;/code&gt;？&lt;/p&gt;

&lt;p&gt;Then you can link the sources and sinks to their corresponding channels (for sources) of channel (for sinks) to setup two different flows. For example, if you need to setup two flows in an agent, one going from an external avro client to external HDFS and another from output of a tail to avro sink, then here’s a config to do that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels in the agent
agent_foo.sources = avro-AppSrv-source1 exec-tail-source2
agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2
agent_foo.channels = mem-channel-1 file-channel-2

# flow #1 configuration
agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1
agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1

# flow #2 configuration
agent_foo.sources.exec-tail-source2.channels = file-channel-2
agent_foo.sinks.avro-forward-sink2.channel = file-channel-2	
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;configuring-a-multi-agent-flow&quot;&gt;Configuring a multi agent flow&lt;/h3&gt;

&lt;p&gt;To setup a multi-tier flow, you need to have an avro/thrift sink of first hop pointing to avro/thrift source of the next hop. This will result in the first Flume agent forwarding events to the next Flume agent. For example, if you are periodically sending files (1 file per event) using avro client to a local Flume agent, then this local agent can forward it to another agent that has the mounted for storage.（multi-agent之间通过avro、thrift方式进行连接，通过&lt;code&gt;IP:port&lt;/code&gt;来交互）&lt;/p&gt;

&lt;p&gt;Weblog agent config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list sources, sinks and channels in the agent
agent_foo.sources = avro-AppSrv-source
agent_foo.sinks = avro-forward-sink
agent_foo.channels = file-channel

# define the flow
agent_foo.sources.avro-AppSrv-source.channels = file-channel
agent_foo.sinks.avro-forward-sink.channel = file-channel

# avro sink properties
agent_foo.sources.avro-forward-sink.type = avro
agent_foo.sources.avro-forward-sink.hostname = 10.1.1.100
agent_foo.sources.avro-forward-sink.port = 10000

# configure other pieces
#...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HDFS agent config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list sources, sinks and channels in the agent
agent_foo.sources = avro-collection-source
agent_foo.sinks = hdfs-sink
agent_foo.channels = mem-channel

# define the flow
agent_foo.sources.avro-collection-source.channels = mem-channel
agent_foo.sinks.hdfs-sink.channel = mem-channel

# avro sink properties
agent_foo.sources.avro-collection-source.type = avro
agent_foo.sources.avro-collection-source.bind = 10.1.1.100
agent_foo.sources.avro-collection-source.port = 10000

# configure other pieces
#...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we link the avro-forward-sink from the weblog agent to the avro-collection-source of the hdfs agent. This will result in the events coming from the external appserver source eventually getting stored in HDFS.&lt;/p&gt;

&lt;h3 id=&quot;fan-out-flow&quot;&gt;Fan out flow&lt;/h3&gt;

&lt;p&gt;As discussed in previous section, Flume supports fanning out the flow from one source to multiple channels. There are two modes of fan out, replicating and multiplexing. In the replicating flow, the event is sent to all the configured channels. In case of multiplexing, the event is sent to only a subset of qualifying channels. To fan out the flow, one needs to specify a list of channels for a source and the policy for the fanning it out. This is done by adding a channel “selector” that can be replicating or multiplexing. Then further specify the selection rules if it’s a multiplexer. If you don’t specify a selector, then by default it’s replicating:（fan-out，两种实现方式：replicating、multiplexing；replicating，发送给所有channel；multiplexing，发送给满足条件的channel。具体，设置&lt;code&gt;selector&lt;/code&gt;，并指定规则；默认是replicating）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# List the sources, sinks and channels for the agent
&amp;lt;Agent&amp;gt;.sources = &amp;lt;Source1&amp;gt;
&amp;lt;Agent&amp;gt;.sinks = &amp;lt;Sink1&amp;gt; &amp;lt;Sink2&amp;gt;
&amp;lt;Agent&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set list of channels for source (separated by space)
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.channels = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;

# set channel for sinks
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink1&amp;gt;.channel = &amp;lt;Channel1&amp;gt;
&amp;lt;Agent&amp;gt;.sinks.&amp;lt;Sink2&amp;gt;.channel = &amp;lt;Channel2&amp;gt;

# set selector.type = replicating
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.type = replicating
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The multiplexing select has a further set of properties to bifurcate the flow. This requires specifying a mapping of an event attribute to a set for channel. The selector checks for each configured attribute in the event header. If it matches the specified value, then that event is sent to all the channels mapped to that value. If there’s no match, then the event is sent to set of channels configured as default:（multiplexing方式时，设置header属性，根据header取值不同，分发到相应的channel；都不匹配的，分发到default）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：header的值，是谁设置的？在哪设置的？难道是event中自带的？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Mapping for multiplexing selector
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.type = multiplexing
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.header = &amp;lt;someHeader&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value1&amp;gt; = &amp;lt;Channel1&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value2&amp;gt; = &amp;lt;Channel1&amp;gt; &amp;lt;Channel2&amp;gt;
&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.mapping.&amp;lt;Value3&amp;gt; = &amp;lt;Channel2&amp;gt;
#...

&amp;lt;Agent&amp;gt;.sources.&amp;lt;Source1&amp;gt;.selector.default = &amp;lt;Channel2&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The mapping allows overlapping the channels for each value.（不同header取值对应的channel，可以重复）&lt;/p&gt;

&lt;p&gt;The following example has a single flow that multiplexed to two paths. The agent named agent_foo has a single avro source and two channels linked to two sinks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# list the sources, sinks and channels in the agent
agent_foo.sources = avro-AppSrv-source1
agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2
agent_foo.channels = mem-channel-1 file-channel-2

# set channels for source
agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 file-channel-2

# set channel for sinks
agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1
agent_foo.sinks.avro-forward-sink2.channel = file-channel-2

# channel selector configuration
agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing
agent_foo.sources.avro-AppSrv-source1.selector.header = State
agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1
agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The selector checks for a header called “State”. If the value is “CA” then its sent to mem-channel-1, if its “AZ” then it goes to file-channel-2 or if its “NY” then both. If the “State” header is not set or doesn’t match any of the three, then it goes to mem-channel-1 which is designated as ‘default’.&lt;/p&gt;

&lt;p&gt;The selector also supports optional channels. To specify optional channels for a header, the config parameter ‘optional’ is used in the following way:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：optional channels，要解决什么问题？RE：仅当required channel中event运行失败，才有可能涉及optional channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# channel selector configuration
agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing
agent_foo.sources.avro-AppSrv-source1.selector.header = State
agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1
agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.optional.CA = mem-channel-1 file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The selector will attempt to write to the required channels first and will fail the transaction if even one of these channels fails to consume the events. The transaction is reattempted on all of the channels. Once all required channels have consumed the events, then the selector will attempt to write to the optional channels. A failure by any of the optional channels to consume the event is simply ignored and not retried.（运行异常的事务，会尝试在所有required channels中重新运行，如果重新运行成功，则将event写入optional channels内。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：transaction、event是怎么划分的？到底什么是transaction？&lt;/p&gt;

&lt;p&gt;If there is an overlap between the optional channels and required channels for a specific header, the channel is considered to be required, and a failure in the channel will cause the entire set of required channels to be retried. For instance, in the above example, for the header “CA” mem-channel-1 is considered to be a required channel even though it is marked both as required and optional, and a failure to write to this channel will cause that event to be retried on all channels configured for the selector.（如果一个channel既是required channel，又是optional channel，则强制认定channel为required channel）&lt;/p&gt;

&lt;p&gt;Note that if a header does not have any required channels, then the event will be written to the default channels and will be attempted to be written to the optional channels for that header. Specifying optional channels will still cause the event to be written to the default channels, if no required channels are specified. If no channels are designated as default and there are no required, the selector will attempt to write the events to the optional channels. Any failures are simply ignored in that case.（如果event，没有任何对应的required channel，则尝试写入default channel，并且尝试写入对应的optional channel；如果没有default channel，则，也会写入optional channel中。）&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide 1.5.0.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>MySQL字符编码和大小写敏感问题</title>
     <link href="http://ningg.github.com/mysql-charset-and-table-name-lower-case"/>
     <updated>2014-10-16T00:00:00+08:00</updated>
     <id>http://ningg.github.com/mysql-charset-and-table-name-lower-case</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;关于MySQL，遇到几个问题，列一下：&lt;/p&gt;

&lt;p&gt;乱码问题：字符集不统一；
无法启动：查看错误日志；
无法删除database：先在数据存储目录清理；
table找不到：table区分大小写；&lt;/p&gt;

&lt;p&gt;MySQL版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[devp@localhost ~]$ mysql -V
mysql  Ver 14.14 Distrib 5.6.20, for Linux (x86_64) using  EditLine wrapper
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;乱码问题&lt;/h2&gt;

&lt;p&gt;看官网，mysql的refman（reference manual，参考手册）中，globalization –&amp;gt; character set configuration，其中提到：
system、server、client的charset不一致时，会产生乱码。&lt;/p&gt;

&lt;p&gt;通过如下命令查看一下，当前mysql各个组件的字符集详情：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &quot;character%&quot;;
+--------------------------+----------------------------+
| Variable_name            | Value                      |
+--------------------------+----------------------------+
| character_set_client     | utf8                       |
| character_set_connection | utf8                       |
| character_set_database   | latin1                     |
| character_set_filesystem | binary                     |
| character_set_results    | utf8                       |
| character_set_server     | latin1                     |
| character_set_system     | utf8                       |
| character_sets_dir       | /usr/share/mysql/charsets/ |
+--------------------------+----------------------------+
8 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过命令：show collation，查看当前MySQL支持的字符集。从上面查询结果可知，server的字符集与system、client不同，则，在my.cnf文件中设定server的字符集即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /usr/my.cnf
character_set_server=utf8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新启动MySQL，OK（根据&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_character_set_server&quot;&gt;官网解释&lt;/a&gt;，不需要向数据库重新插入数据）。database的编码方式，不要手动调整，其始终与default database保持一致，若没有default database，则由server的编码方式决定。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;无法启动&lt;/h2&gt;

&lt;p&gt;通过service mysql start，无法启动MySQL，提示出错，略焦躁，不要着急，有错误日志，查看即可。错误日志位置：/var/lib/mysql/*.err，出错信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ERROR] /usr/sbin/mysqld: unknown variable &#39;default-character-set=utf-8&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原来是在my.cnf文件中添加了一个变量，MySQL无法识别，从my.cnf删除即可。官方文档中&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/server-administration.html&quot;&gt;MySQL Server Administration&lt;/a&gt;，有查看错误日志的详细信息，另外，错误日志位置参考&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/installing.html&quot;&gt;Installing and Upgrading MySQL&lt;/a&gt;中提到的安装目录结构。&lt;/p&gt;

&lt;h2 id=&quot;database&quot;&gt;无法删除Database&lt;/h2&gt;

&lt;p&gt;删除database时，出错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; drop database test;
ERROR 1010 (HY000): Error dropping database (can&#39;t rmdir &#39;./test&#39;, errno: 39)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决办法：到MySQL存放数据的路径下（/var/lib/mysql/），将test数据库对应目录（./test）下内容清空，再删除test数据库即可。&lt;/p&gt;

&lt;h2 id=&quot;table&quot;&gt;找不到table&lt;/h2&gt;

&lt;p&gt;MySQL无法连接，提示表格不存在，设置table名称不区分大小写：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;用ROOT登录，修改&lt;code&gt;my.cnf&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;在&lt;code&gt;[mysqld]&lt;/code&gt;下加入一行：&lt;code&gt;lower_case_table_names=1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;重新启动数据库即可;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;思考&lt;/strong&gt;：字段是否区分大小写？数据库还有其他大小写敏感的地方吗？&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/&quot;&gt;MySQL官方文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/installing.html&quot;&gt;Installing and Upgrading MySQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/server-administration.html&quot;&gt;MySQL Server Administration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>如何参与开源项目</title>
     <link href="http://ningg.github.com/how-to-contribute-open-source-project"/>
     <updated>2014-10-15T00:00:00+08:00</updated>
     <id>http://ningg.github.com/how-to-contribute-open-source-project</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;本来今天晚上想浏览一下flume官网的，不过突然看到How to Get Involved，再看看那些贡献了代码的名单，很是羡慕，我这个人爱吹牛，如果我也在名单中，那岂不又能吹牛一把？哈哈~想想都能笑出声。另一方面，用过的开源工具不少，但是如何参与到开源项目中，我还真不知道，碰巧在看flume官网，那就看看如何参与到flume这个开源项目中去吧。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;官方原文地址：&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Contribute&quot;&gt;How to Contribute&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;Welcome contributors! We strive to include everyone’s contributions. This page provides necessary guidelines on how to contribute effectively towards furthering the development and evolution of Flume. You should also read the guide on setting up &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/Development+Environment&quot;&gt;Development Environment&lt;/a&gt; where you will find details on how to checkout, build and test Flume.
（如何下载源码、编译源码、测试源码，需要先阅读&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/Development+Environment&quot;&gt;Development Environment&lt;/a&gt;。）&lt;/p&gt;

&lt;p&gt;Note: This guide applies to general contributors. If you are a committer, please read the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Commit&quot;&gt;How to Commit&lt;/a&gt; as well.
（committer还需阅读&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Commit&quot;&gt;How to Commit&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;what-can-be-contributed&quot;&gt;What can be contributed?&lt;/h2&gt;

&lt;p&gt;There are many ways you can contribute towards the project. A few of these are:（参与方式，有如下几种）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Jump in on discussions&lt;/strong&gt;: It is possible that someone initiates a thread on the mailing list describing a problem that you have dealt with in the past. You can help the project by chiming in on that thread and guiding that user to overcome or workaround that problem or limitation.（查看邮件列表，参与讨论，帮助他人解决问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;File Bugs&lt;/strong&gt;: If you notice a problem and are sure it is a bug, then go ahead and file a JIRA. If however, you are not very sure that it is a bug, you should first confirm it by discussing it on the Mailing Lists.（通过JIRA，提交bug；如果不确定，通过邮件列表提问，确认是否为bug）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Review Code&lt;/strong&gt;: If you see that a JIRA ticket has a “Patch Available” status, go ahead and review it. It cannot be stressed enough that you must be kind in your review and explain the rationale for your feedback and suggestions. Also note that not all review feedback is accepted - often times it is a compromise between the contributor and reviewer. If you are happy with the change and do not spot any major issues, then +1 it. More information on this is available in the following sections.（通过JIRA，检查代码-Patch补丁，提出反馈意见）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Provide Patches&lt;/strong&gt;: We encourage you to assign the relevant JIRA issue to yourself and supply a patch for it. The patch you provide can be code, documentation, build changes, or any combination of these. More information on this is available in the following sections.（通过JIRA，提交代码-patch补丁，可以是代码、文档、编译细节等）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：邮件列表、JIRA，我都没有关注过，也不知道具体怎么用，打算学一下；patch文件了解一点。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;providing-patches&quot;&gt;Providing Patches&lt;/h2&gt;

&lt;p&gt;In order to provide patches, follow these guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Make sure there is a JIRA&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;If you are working on fixing a problem that already has an associated JIRA, then go ahead and assign it to yourself. （JIRA上找到问题，并指派给自己）&lt;/li&gt;
      &lt;li&gt;If it is already assigned to someone else, check with the current assignee before moving it over to your queue.（跟正在解决这个问题的人商量下）&lt;/li&gt;
      &lt;li&gt;If the current assignee has already worked out some part of the fix, suggest that you can take that change over from them and complete the remaining parts.（如果有人已经修复bug的一部分，你可以接手，把余下的做完）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attach the patches as you go through development&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;While small fixes are easily done in a single patch, it is preferable that you attach patches to the JIRA as you go along. This serves as an early feedback mechanism where interested folks can look it over and suggest changes where necessary. It also ensures that if for some reason you are not able to find the time to complete the change, someone else can take up your initial patches and drive them to completion.（本地开发环境测试通过，就提交patch，即使是small fixes）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Before you submit your patch&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;Your change should be well-formatted and readable. Please use two spaces for indentation (no tabs).（保证patch的well-formatted和readable，使用2个space，避免tab）&lt;/li&gt;
      &lt;li&gt;Carefully consider whether you have handled all boundary conditions and have provided sufficiently defensive code where necessary.（代码的边界条件、异常捕获）&lt;/li&gt;
      &lt;li&gt;Add one or more unit tests, if your change is not covered by existing automated tests.（添加单元测试）&lt;/li&gt;
      &lt;li&gt;Insert javadocs and code comments where appropriate.（添加javadocs和comments）&lt;/li&gt;
      &lt;li&gt;Update the &lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide&lt;/a&gt; (&lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=flume.git;a=blob;f=flume-ng-doc/sphinx/FlumeUserGuide.rst;hb=trunk&quot;&gt;source&lt;/a&gt;) if your change affects the Flume config file or any user interface. Include those changes in your patch.（修改文档）&lt;/li&gt;
      &lt;li&gt;Make sure you update the relevant developer documentation, wiki pages, etc. if your change affects the development environment.（修改开发手册）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Test your changes before submitting a review&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;Before you make the JIRA status as “Patch Available”, please test your changes thoroughly. Try any new feature or fix out for yourself, and make sure that it works.（测试充分）&lt;/li&gt;
      &lt;li&gt;Make sure that all unit/integration tests are passing, and that the functionality you have worked on is tested through existing or new tests.（unit/integration测试）&lt;/li&gt;
      &lt;li&gt;You can run all the tests by going to the root level of the source tree and typing &lt;code&gt;mvn clean install&lt;/code&gt;.（mvn clean install，执行测试）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to create a patch file&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;The preferred naming convention for Flume patches is &lt;code&gt;FLUME-12345.patch&lt;/code&gt;, or &lt;code&gt;FLUME-12345-0.patch&lt;/code&gt; where 12345 is the JIRA number. You might want to name successive versions of the patch something like &lt;code&gt;FLUME-12345-1.patch&lt;/code&gt;, &lt;code&gt;FLUME-12345-2.patch&lt;/code&gt;, etc. as you iterate on your changes based on review feedback and re-submit them.（patch命名方式）&lt;/li&gt;
      &lt;li&gt;The command to generate the patch is &lt;code&gt;git diff&lt;/code&gt;. Example:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;$ git diff &amp;gt; /path/to/FLUME-1234-0.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;How to apply someone else’s patch file&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;You can apply someone else’s patch with the GNU patch tool. Example:&lt;/li&gt;
      &lt;li&gt;Contributors may variously submit patches in a couple of different formats. If you get some dialog from the patch tool asking which file you want to patch, try variously the “-p1” or “-p0” flags to patch. Without any additional arguments, git diff generates patches that are applied using patch &lt;code&gt;-p1&lt;/code&gt;. If you use git diff &lt;code&gt;--no-prefix&lt;/code&gt; to generate your patch, you have to apply it using patch &lt;code&gt;-p0&lt;/code&gt;. The ReviewBoard tool understands both formats and is able to apply both types automatically.（&lt;code&gt;patch&lt;/code&gt;命令的选项）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/src/flume # or wherever you keep the root of your Flume source tree
$ patch -p1 &amp;lt; FLUME-1234.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Submitting your patch for review&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;To submit a patch, attach the patch file to the JIRA and change the status of the JIRA to “Patch Available”.（JIRA上，提交patch，修改状态）&lt;/li&gt;
      &lt;li&gt;If the change is non-trivial, please also post it for review on the Review Board. Use the Repository “flume-git” on Review Board.（关键的bug，需要在Review Board上标记一下）&lt;/li&gt;
      &lt;li&gt;Link the JIRA to the Review Board review. JIRA has a feature you can use for this by going to More Actions &amp;gt; Link &amp;gt; Web Link when logged into JIRA.（Review Board上添加JIRA的链接）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identify a reviewer&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;When posting on review board (repository: “flume-git”), always add the Group “Flume” to the list of reviewers.（添加&lt;code&gt;Flume&lt;/code&gt;到reviewers列表）&lt;/li&gt;
      &lt;li&gt;Optionally, you may also add a specific reviewer to the review. You can pick any of the project committers for review. Note that identifying a reviewer does not stop others from reviewing your change. Be prepared for having your change reviewed by others at any time.（可以指定committer作为reviewer，但其他人仍可以review）&lt;/li&gt;
      &lt;li&gt;If you have posted your change for review and no one has had a chance to review it yet, you can gently remind everyone by dropping a note on the developer mailing list with a link to the review.（可在mailing list中添加一个JIRA链接，告知别人来review代码）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Work with reviewers to get your change fleshed out&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;When your change is reviewed, please engage with the reviewer via JIRA or review board to get necessary clarifications and work out other details.（及时给reviewer反馈，多交流）&lt;/li&gt;
      &lt;li&gt;The goal is to ensure that the final state of your change is acceptable to the reviewer so that they can +1 it.（经过数次交流，reviewer确认代码可用了，会点击+1的）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：代码中添加javadocs，集成测试是什么，我还不清楚。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;reviewing-code&quot;&gt;Reviewing Code&lt;/h2&gt;

&lt;p&gt;Flume uses the &lt;a href=&quot;https://reviews.apache.org/groups/Flume&quot;&gt;Apache Review Board&lt;/a&gt; for doing code reviews. In order for a change to be reviewed, it should be either posted on the review board or attached to the JIRA. If the change is a minor change affecting only few lines and does not seem to impact main logic of the affected sources, it need not be posted on the review board. However, if the code change is large or otherwise impacting the core logic of the affected sources, it should be posted on the review board. Feel free to comment on the JIRA requesting the assignee to post the patch for review on review board.（小改动的patch，贴在JIRA上就好了；涉及核心代码的patch，应同时在JIRA和review board上贴出来。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Not all patches attached to a JIRA are ready for review. Sometimes the patches are attached just to solicit early feedback regarding the implementation direction. Feel free to look it over and give your feedback in the JIRA as necessary. Patches are considered ready for review either when the patch has been posted on review board, or the JIRA status has been changed to ‘Patch Available’. Find here a &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20flume%20AND%20status%20%3D%20%22Patch%20Available%22&quot;&gt;list of Flume JIRAs marked Patch Available&lt;/a&gt;. （patch有时候are not ready for review，只是为了征求意见，看看实现的方向对不对）&lt;/p&gt;

&lt;h3 id=&quot;goals-for-code-reviews&quot;&gt;Goals for Code Reviews&lt;/h3&gt;

&lt;p&gt;The net outcome from the review should be the same - which is to ensure the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bugs/Omissions/Regressions are caught before the change is committed to the source control.（问题已解决）&lt;/li&gt;
  &lt;li&gt;The change is subjected to keeping the quality of code high so as to make the overall system sustainable. The implementation of the change should be easily readable, documented where necessary, and must favor simplicity of implementation.（高质量的代码与可正常运行的系统，同等重要。代码质量包括：可读性、文档、实现简洁）&lt;/li&gt;
  &lt;li&gt;Changes are evaluated from the perspective of a consumer (the reviewer) as opposed to the developer, which often brings out subtleties in the implementation that otherwise go unnoticed.（reviewer通常能为代码实现，提供细微改动的建议）&lt;/li&gt;
  &lt;li&gt;The change should be backward compatible and not require extensive work on existing installations in order for it to be consumed. There are exceptions to this in some cases like when work is done on a major release, but otherwise backward compatibility should be upheld at all times. If you are not clear, raise it is as a concern to be clarified during the review.（change保证，后向兼容，即，原来已有的应用代码部分，不需要大改动；如果不确定是否后向兼容，则，说明一下）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;code-review-guidelines&quot;&gt;Code review guidelines&lt;/h3&gt;

&lt;p&gt;Following are some guidelines on how to do a code review. You may use any other approach instead as long as the above stated goals are met. That said, here is an approach that works fine generally:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Understand the problem being solved&lt;/strong&gt;: This often requires going through the JIRA comments and/or mailing list threads where the discussion around the problem has happened in the past. Look for key aspects of the problem such as how it has impacted the users and what, if any, is the suggested way to solve it. You may not find enough information regarding the problem in some cases, in which case - feel free to ask for clarification from the developer contributing the change.（广泛查询，弄清问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Think about how you would solve the problem&lt;/strong&gt;: There are many ways to solve any code problem, with different ways having different merits. Before proceeding to review the change, think through how you would solve the problem if you were the one implementing the solution. Note the various aspects of the problem that your solution might have. Some such aspects to think about are - impact on backward compatibility, overall usability of the system, any impact on performance etc.（制定详细的解决方案，考虑几点：后向兼容、系统全局可用、性能影响）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluate the proposed change in contrast to your solution&lt;/strong&gt;: Unless the change is obvious, it is likely that the implementation of the change you are reviewing is very different from the solution you would go for. Evaluate this change on the various aspects that you evaluated your solution on in the previous step. See how it measures up and give feedback where you think it could be improved.（review代码时，与自己的方案多方面对比）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Look for typical pitfalls&lt;/strong&gt;: Read through the implementation to see if: it needs to be documented at places where the intention is not clear; if all the boundary conditions are being addressed; if the code is defensive enough; if any bad idioms have leaked in such as double check locking etc. In short, check for things that a developer is likely to miss in their own code which are otherwise obvious to someone trying to read and understand the code.（全面检查配套方面：文档中描述是否清晰、边界条件是否考虑、代码安全性怎么样、是否包含bad idioms；总之，站在非developer的角度，看看哪些潜在问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;See if the change is complete&lt;/strong&gt;: Check if the change is such that it affects the user interface. If it does, then the documentation should likely be updated. What about testing - does it have enough test coverage or not? What about other aspects like license headers, copyright statements etc. How about checkstyle and findbugs - did they generate new warnings? How about compiler warnings?（代码层面上，修改全面了吗？是否测试了、warning信息）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Test the change&lt;/strong&gt;: It is very easy to test the change if you have the development environment setup. Run as many tests as you want with the patch. Manually test the change for functionality that you think is not fully covered via the associated tests. If you find a problem, report it.（全面测试、必要的地方手动测）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-give-feedback&quot;&gt;How to give feedback&lt;/h3&gt;

&lt;p&gt;Once you have collected your comments/concerns/feedback you need to send it to back to the contributor. In doing so, please be as courteous as possible and ensure the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your feedback should be clear and actionable. Giving subjective/vague feedback does not add any value or facilitate a constructive dialog.（feedback要清晰、可操作）&lt;/li&gt;
  &lt;li&gt;Where possible, suggest how your concern can be addressed. For example if your testing revealed that a certain use-case is not satisfied, it is acceptable to state that as is, but it would be even better if you could suggest how the developer can address it. Present your suggestion as a possible solution rather than the solution.（对如何解决问题，提出自己的建议）&lt;/li&gt;
  &lt;li&gt;If you do not understand part of the change, or for some reason were not able to review part of the change, state it explicitly so as to encourage other reviewers to jump in and help.（如果读不懂代码，请明确说出来，以方便其他reviewer给予帮助）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you have provided your feedback, wait for the developer to respond. It is possible that the developer may need further clarification on your feedback, in which case you should promptly provide it where necessary. In general, the dialog between the reviewer and developer should lead to finding a reasonable middle ground where key concerns are satisfied and the goals of the review have been met.&lt;/p&gt;

&lt;p&gt;If a change has met all your criteria for review, please +1 the change to indicate that you are happy with it.（如果代码让你满意，请点击+1）&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Contribute&quot;&gt;How to contribute&lt;/a&gt;中提到了各种规范、细节，这些就是参与开源项目的基本准则，大家都按照这个准则来操作，才能保证开源项目的顺利进行。想到了&lt;a href=&quot;http://robbinfan.com/&quot;&gt;Robbin&lt;/a&gt;的一句话：Small is beautiful, constraint is liberty.&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>In-Stream Big Data Processing</title>
     <link href="http://ningg.github.com/in-stream-big-data-processing"/>
     <updated>2014-10-14T00:00:00+08:00</updated>
     <id>http://ningg.github.com/in-stream-big-data-processing</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;英文原文：&lt;a href=&quot;http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/&quot;&gt;In-Stream-big-data-processing&lt;/a&gt;，有人翻译了&lt;a href=&quot;http://blog.csdn.net/idontwantobe/article/details/25938511&quot;&gt;中文版&lt;/a&gt;，也有直接&lt;a href=&quot;http://dirlt.com/in-stream-big-data-processing.html&quot;&gt;中文注释英文版&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The shortcomings and drawbacks of batch-oriented data processing were widely recognized by the Big Data community quite a long time ago. It became clear that real-time query processing and in-stream processing is the immediate need in many practical applications. In recent years, this idea got a lot of traction and a whole bunch of solutions like Twitter’s Storm, Yahoo’s S4, Cloudera’s Impala, Apache Spark, and Apache Tez appeared and joined the army of Big Data and NoSQL systems. This article is an effort to explore techniques used by developers of in-stream data processing systems, trace the connections of these techniques to massive batch processing and OLTP/OLAP databases, and discuss how one unified query engine can support in-stream, batch, and OLAP processing at the same time.&lt;/p&gt;

&lt;p&gt;At Grid Dynamics, we recently faced a necessity to build an in-stream data processing system that aimed to crunch about 8 billion events daily providing fault-tolerance and strict transactioanlity i.e. none of these events can be lost or duplicated. This system has been designed to supplement and succeed the existing Hadoop-based system that had too high latency of data processing and too high maintenance costs. The requirements and the system itself were so generic and typical that we describe it below as a canonical model, just like an abstract problem statement.&lt;/p&gt;

&lt;p&gt;A high-level overview of the environment we worked with is shown in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/cover-2.png&quot; alt=&quot;cover-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One can see that this environment is a typical Big Data installation: there is a set of applications that produce the raw data in multiple datacenters, the data is shipped by means of Data Collection subsystem to HDFS located in the central facility, then the raw data is aggregated and analyzed using the standard Hadoop stack (MapReduce, Pig, Hive) and the aggregated results are stored in HDFS and NoSQL, imported to the OLAP database and accessed by custom user applications. Our goal was to equip all facilities with a new in-stream engine (shown in the bottom of the figure) that processes most intensive data flows and ships the pre-aggregated data to the central facility, thus decreasing the amount of raw data and heavy batch jobs in Hadoop. The design of the in-stream processing engine itself was driven by the following requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;SQL-like functionality&lt;/strong&gt;. The engine has to evaluate SQL-like queries continuously, including joins over time windows and different aggregation functions that implement quite complex custom business logic. The engine can also involve relatively static data (admixtures) loaded from the stores of Aggregated Data. Complex multi-pass data mining algorithms are beyond the immediate goals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Modularity and flexibility&lt;/strong&gt;. It is not to say that one can simply issue a SQL-like query and the corresponding pipeline will be created and deployed automatically, but it should be relatively easy to assemble quite complex data processing chains by linking one block to another.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault-tolerance&lt;/strong&gt;. Strict fault-tolerance is a principal requirement for the engine. As it sketched in the bottom part of the figure, one possible design of the engine is to use distributed data processing pipelines that implement operations like joins and aggregations or chains of such operations, and connect these pipelines by means of fault-tolerant persistent buffers. These buffers also improve modularity of the system by enabling publish/subscribe communication style and easy addition/removal of the pipelines. The pipelines can be stateful and the engine’s middleware should provide a persistent storage to enable state checkpointing. All these topics will be discussed in the later sections of the article.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability with Hadoop&lt;/strong&gt;. The engine should be able to ingest both streaming data and data from Hadoop i.e. serve as a custom query engine atop of HDFS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;High performance and mobility&lt;/strong&gt;. The system should deliver performance of tens of thousands messages per second even on clusters of minimal size. The engine should be compact and efficient, so one can deploy it in multiple datacenters on small clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To find out how such a system can be implemented, we discuss the following topics in the rest of the article:&lt;/p&gt;

&lt;p&gt;First, we explore relations between in-stream data processing systems, massive batch processing systems, and relational query engines to understand how in-stream processing can leverage a huge number of techniques that were devised for other classes of systems.&lt;/p&gt;

&lt;p&gt;Second, we describe a number of patterns and techniques that are frequently used in building of in-stream processing frameworks and systems. In addition, we survey the current and emerging technologies and provide a few implementation tips.&lt;/p&gt;

&lt;p&gt;The article is based on a research project developed at Grid Dynamics Labs. Much of the credit goes to Alexey Kharlamov and Rafael Bagmanov who led the project and other contributors: Dmitry Suslov, Konstantine Golikov, Evelina Stepanova, Anatoly Vinogradov, Roman Belous, and Varvara Strizhkova.&lt;/p&gt;

&lt;h2 id=&quot;basics-of-distributed-query-processing&quot;&gt;Basics of Distributed Query Processing&lt;/h2&gt;

&lt;p&gt;It is clear that distributed in-stream data processing has something to do with query processing in distributed relational databases. Many standard query processing techniques can be employed by in-stream processing engine, so it is extremely useful to understand classical algorithms of distributed query processing and see how it all relates to in-stream processing and other popular paradigms like MapReduce.&lt;/p&gt;

&lt;p&gt;Distributed query processing is a very large area of knowledge that was under development for decades, so we start with a brief overview of the main techniques just to provide a context for further discussion.&lt;/p&gt;

&lt;h3 id=&quot;partitioning-and-shuffling&quot;&gt;Partitioning and Shuffling&lt;/h3&gt;

&lt;p&gt;Distributed and parallel query processing heavily relies on data partitioning to break down a large data set into multiple pieces that can be processed by independent processors. Query processing could consist of multiple steps and each step could require its own partitioning strategy, so data shuffling is an operation frequently performed by distributed databases.&lt;/p&gt;

&lt;p&gt;Although optimal partitioning for selection and projection operations can be tricky (e.g. for range queries), we can assume that for in-stream data filtering it is practically enough to distribute data among the processors using a hash-based partitioning.&lt;/p&gt;

&lt;p&gt;Processing of distributed joins is not so easy and requires a more thorough examination. In distributed environments, parallelism of join processing is achieved through data partitioning, i.e. the data is distributed among processors and each processor employs a serial join algorithm (e.g. nested-loop join or sort-merge join or hash-based join) to process its part of the data. The final results are consolidated from the results obtained from different processors.&lt;/p&gt;

&lt;p&gt;There are two main data partitioning techniques that can be employed by distributed join processing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disjoint data partitioning&lt;/li&gt;
  &lt;li&gt;Divide and broadcast join&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Disjoint data partitioning technique shuffles the data into several partitions in such a way that join keys in different partitions do not overlap. Each processor performs the join operation on each of these partitions and the final result is obtained as a simple concatenation of the results obtained from different processors.  Consider an example where relation R is joined with relation S on a numerical key k and a simple modulo-based hash function is used to produce the partitions (it is assumes that the data initially distributed among the processors based on some other policy):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/disjoint-partitioning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The divide and broadcast join algorithm is illustrated in the figure below. This method divides the first data set into multiple disjoint partitions (R1, R2, and R3 in the figure) and replicates the second data set to all processors. In a distributed database, division typically is not a part of the query processing itself because data sets are initially distributed among multiple nodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/broadcast-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This strategy is applicable for joining of a large relation with a small relation or two small relations. In-stream data processing systems can employ this technique for stream enrichment i.e. joining a static data (admixture) to a data stream.&lt;/p&gt;

&lt;p&gt;Processing of GroupBy queries also relies on shuffling and fundamentally similar to the MapReduce paradigm in its pure form.  Consider an example where the data is grouped by a string key and sum of the numerical values is computed in each group:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/group-by-query.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, computation consists of two steps: local aggregation and global aggregation. These steps basically correspond to Map and Reduce operations. Local aggregation is optional and raw records can be emitted, shuffled, and aggregated on a global aggregation phase.&lt;/p&gt;

&lt;p&gt;The whole point of this section is that all the algorithms above can be naturally implemented using a message passing architectural style i.e. the query execution engine can be considered as a distributed network of nodes connected by the messaging queues. It is conceptually similar to the in-stream processing pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pipelining&quot;&gt;Pipelining&lt;/h3&gt;

&lt;p&gt;In the previous section, we noted that many distributed query processing algorithms resemble message passing networks. However, it is not enough to organize efficient in-stream processing: all operators in a query should be chained in such a way that the data flows smoothly through the entire pipeline i.e. neither operation should block processing by waiting for a large piece of input data without producing any output or by writing intermediate results on disk. Some operations like sorting are inherently incompatible with this concept (obviously, a sorting block cannot produce any output until the entire input is ingested), but in many cases pipelining algorithms are applicable.  A typical example of pipelining is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/join-pipeline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the hash join algorithm is employed to join four relations: R1, S1, S2, and S3 using 3 processors. The idea is to build hash tables for S1, S2 and S3 in parallel and then stream R1 tuples one by one though the pipeline that joins them with S1, S2 and S3 by looking up matches in the hash tables. In-stream processing naturally employs this technique to join a data stream with the static data (admixtures).&lt;/p&gt;

&lt;p&gt;In relational databases, join operation can take advantage of pipelining by using the symmetric hash join algorithm or some of its advanced variants [1,2]. Symmetric hash join is a generalization of hash join. Whereas a normal hash join requires at least one of its inputs to be completely available to produce first results (the input is needed to build a hash table), symmetric hash join is able to produce first results immediately. In contrast to the normal hash join, it maintains hash tables for both inputs and populates these tables as tuples arrive:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/symmetric-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a tuple comes in, the joiner first looks it up in the hash table of the other stream. If match is found, an output tuple is produced. Then the tuple is inserted in its own hash table.&lt;/p&gt;

&lt;p&gt;However, it does not make a lot of sense to perform a complete join of infinite streams. In many cases join is performed on a finite time window or other type of buffer e.g. LFU cache that contains most frequent tuples in the stream. Symmetric hash join can be employed if the buffer is large comparing to the stream rate or buffer is flushed frequently according to some application logic or buffer eviction strategy is not predictable. In other cases, simple hash join is often sufficient since the buffer is constantly full and does not block the processing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/stream-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is worth noting that in-stream processing often deals with sophisticated stream correlation algorithms where records are matched based on scoring metrics, not on field equality condition. A more complex system of buffers can be required for both streams in such cases.&lt;/p&gt;

&lt;h2 id=&quot;in-stream-processing-patterns&quot;&gt;In-Stream Processing Patterns&lt;/h2&gt;

&lt;p&gt;In the previous section, we discussed a number of standard query processing techniques that can be used in massively parallel stream processing. Thus, on a conceptual level, an efficient query engine in a distributed database can act as a stream processing system and vice versa, a stream processing system can act as a distributed database query engine. Shuffling and pipelining are the key techniques of distributed query processing and message passing networks can naturally implement them. However, things are not so simple. In a contrast to database query engines where reliability is not critical because a read-only query can always be restarted, streaming systems should pay a lot of attention to reliable events processing. In this section, we discuss a number of techniques that are used by streaming systems to provide message delivery guarantees and some other patterns that are not typical for standard query processing.&lt;/p&gt;

&lt;h3 id=&quot;stream-replay&quot;&gt;Stream Replay&lt;/h3&gt;

&lt;p&gt;Ability to rewind data stream back in time and replay the data is very important for in-stream processing systems because of the following reasons:
This is the only way to guarantee correct data processing. Even if data processing pipeline is fault-tolerant, it is very problematic to guarantee that the deployed processing logic is defect-free. One can always face a necessity to fix and redeploy the system and replay the data on a new version of the pipeline.&lt;/p&gt;

&lt;p&gt;Issue investigation could require ad hoc queries. If something goes wrong, one could need to rerun the system on the problematic data with better logging or with code alternations.&lt;/p&gt;

&lt;p&gt;Although it is not always the case, the in-stream processing system can be designed in such a way that it re-reads individual messages from the source in case of processing errors and local failures, even if the system in general is fault-tolerant.&lt;/p&gt;

&lt;p&gt;As a result, the input data typically goes from the data source to the in-stream pipeline via a persistent buffer that allows clients to move their reading pointers back and forth.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/replay-buffer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka messaging queue is well known implementation of such a buffer that also supports scalable distributed deployments, fault-tolerance, and provides high performance.
As a bottom line, Stream Replay technique imposes the following requirements of the system design:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The system is able to store the raw input data for a preconfigured period time.&lt;/li&gt;
  &lt;li&gt;The system is able to revoke a part of the produced results, replay the corresponding input data and produce a new version of the results.&lt;/li&gt;
  &lt;li&gt;The system should work fast enough to rewind the data back in time, replay them, and then catch up with the constantly arriving data stream.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lineage-tracking&quot;&gt;Lineage Tracking&lt;/h3&gt;

&lt;p&gt;In a streaming system, events flow through a chain of processors until the result reaches the final destination (like an external database). Each input event produces a directed graph of descendant events (lineage) that ends by the final results. To guarantee reliable data processing, it is necessary to ensure that the entire graph was processed successfully and to restart processing in case of failures.&lt;/p&gt;

&lt;p&gt;Efficient lineage tracking is not a trivial problem. Let us first consider how Twitter’s Storm tracks the messages to guarantee at-least-once delivery semantics (see the diagram below):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All events that emitted by the sources (first nodes in the data processing graph) are marked by a random ID. For each source, the framework maintains a set of pairs [event ID -&amp;gt; signature] for each initial event. The signature is initially initialized by the event ID.&lt;/li&gt;
  &lt;li&gt;Downstream nodes can generate zero or more events based on the received initial event. Each event carries its own random ID and the ID of the initial event.&lt;/li&gt;
  &lt;li&gt;If the event is successfully received and processed by the next node in the graph, this node updates the signature of the corresponding initial event by XORing the signature with (a) ID of the incoming event and (b) IDs of all events produced based on the incoming event. In the part 2 of diagram below, event 01111 produces events 01100, 10010, and 00010, so the signature for event 01111 becomes 11100 (= 01111 (initial value) xor 01111 xor 01100 xor 10010 xor 00010).&lt;/li&gt;
  &lt;li&gt;An event can be produced based on more than one incoming event. In this case, it is attached several initial event and carries more than one initial IDs downstream (yellow-black event in the part 3 of the figure below).&lt;/li&gt;
  &lt;li&gt;The event considered to be successfully processed as soon as its signature turns into zero i.e. the final node acknowledged that the last event in the graph was processed successfully and no events were emitted downstream. The framework sends a commit message to the source node (see part 3 in the diagram below).&lt;/li&gt;
  &lt;li&gt;The framework traverses a table of the initial events periodically looking for old uncommitted events (events with non-zero signature). Such events are considered as failed and the framework asks the source nodes to replay them.&lt;/li&gt;
  &lt;li&gt;It is important to note that the order of signature updates is not important due to commutative nature of the XOR operation. In the figure below, acknowledgements depicted in the part 2 can arrive after acknowledgements depicted in the part 3. This enables fully asynchronous processing.&lt;/li&gt;
  &lt;li&gt;One can note that the algorithm above is not strictly reliable – the signature could turn into zero accidentally due to unfortunate combination of IDs. However, 64-bit IDs are sufficient to guarantee a very low probability of error, about 2^(-64), that is acceptable in almost all practical applications. As result, the table of signatures could have a small memory footprint.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/lineage-tracking-storm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The described approach is elegant due to its decentrilized nature: nodes act independently sending acknowledgement messages, there is no cental entity that tracks all lineages explicitly. However, it could be difficult to manage transactional processing in this way for flows that maintain sliding windows or other buffers. For example, processing on a sliding window can involve hundreds of thousands events at each moment of time, so it becomes difficult to manage acknowledgements because many events stay uncommitted or computational state should be persisted frequently.&lt;/p&gt;

&lt;p&gt;An alternative approach is used in Apache Spark [3]. The idea is to consider the final result as a function of the incoming data. To simplify lineage tracking, the framework processes events in batches, so the result is a sequence of batches where each batch is a function of the input batches. Resulting batches can be computed in parallel and if some computation fails, the framework simply reruns it. Consider an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/stream-join-microbatching-tx.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the framework joins two streams on a sliding window and then the result passes through one more processing stage. The framework considers the incoming streams not as streams, but as set of batches. Each batch has an ID and the framework can fetch it by the ID at any moment of time. So, stream processing can be represented as a bunch of transactions where each transaction takes a group of input batches, transforms them using a processing function, and persists a result. In the figure above, one of such transactions is highlighted in red. If the transaction fails, the framework simply reruns it. It is important that transactions can be executed in parallel.&lt;/p&gt;

&lt;p&gt;This simple but powerful paradigm enables centralized transaction management and inherently provides exactly-once message processing semantics. It is worth noting that this technique can be used both for batch processing and for stream processing because it treats the input data as a set of batches regardless to their streaming of static nature.&lt;/p&gt;

&lt;h3 id=&quot;state-checkpointing&quot;&gt;State Checkpointing&lt;/h3&gt;

&lt;p&gt;In the previous section we have considered the lineage tracking algorithm that uses signatures (checksums) to provide at-least-one message delivery semantics. This technique improves reliability of the system, but it leaves at least two major open questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In many cases, exactly-once processing semantics is required. For example, the pipeline that counts events can produce incorrect results if some messages will be delivered twice.&lt;/li&gt;
  &lt;li&gt;Nodes in the pipeline can have a computational state that is updated as the messages processed. This state can be lost in case of node failure, so it is necessary to persist or replicate it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Twitter’s Storm addresses these issues by using the following protocol:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Events are grouped into batches and each batch is associated with a transaction ID. A transaction ID is a monotonically growing numerical value (e.g. the first batch has ID 1, the second ID 2, and so on). If the pipeline fails to process a batch, this batch is re-emitted with the same transaction ID.&lt;/li&gt;
  &lt;li&gt;First, the framework announces to the nodes in the pipeline that a new transaction attempt is started. Second, the framework to sends the batch through the pipeline. Finally, the framework announces that transaction attempt if completed and all nodes can commit their state e.g. update it in the external database.&lt;/li&gt;
  &lt;li&gt;The framework guarantees that commit phases are globally ordered across all transactions i.e. the transaction 2 can never be committed before the transaction 1. This guarantee enables processing nodes to use following logic of persistent state updates:
    &lt;ul&gt;
      &lt;li&gt;The latest transaction ID is persisted along with the state.&lt;/li&gt;
      &lt;li&gt;If the framework requests to commit the current transaction with the ID that differs from the ID value persisted in the database, the state can be updated e.g. a counter in the database can be incremented. Assuming a strong ordering of transactions, such update will happen exactly one for each batch.&lt;/li&gt;
      &lt;li&gt;If the current transaction ID equals to the value persisted in the storage, the node skips the commit because this is a batch replay. The node must have processed the batch earlier and updated the state accordingly, but the transaction failed due to an error somewhere else in the pipeline.&lt;/li&gt;
      &lt;li&gt;Strong order of commits is important to achieve exactly-once processing semantics. However, strictly sequential processing of transactions is not feasible because first nodes in the pipeline will often be idle waiting until processing on the downstream nodes is completed. This issues can be alleviated by allowing parallel processing of transactions but serialization of commit steps only, as it shown in the figure below:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/pipelining-commits-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This technique allows one to achieve exactly-once processing semantics assuming that data sources are fault-tolerant and can be replayed. However, persistent state updates can cause serious performance degradation even if large batches are used. By this reason, the intermediate computational state should be minimized or avoided whenever possible.&lt;/p&gt;

&lt;p&gt;As a footnote, it is worth mentioning that state writing can be implemented in different ways. The most straightforward approach is to dump in-memory state to the persistent store as part of the transaction commit process. This does not work well for large states (sliding windows an so on). An alternative is to write a kind of transaction log i.e. a sequence of operations that transform the old state into the new one (for a sliding window it can be a set of added and evicted events). This approach complicates crash recovery because the state has to be reconstructed from the log, but can provide performance benefits in a variety of cases.&lt;/p&gt;

&lt;h3 id=&quot;additive-state-and-sketches&quot;&gt;Additive State and Sketches&lt;/h3&gt;

&lt;p&gt;Additivity of intermediate and final computational results is an important property that drastically simplifies design, implementation, maintenance, and recovery of in-stream data processing systems. Additivity means that the computational result for a larger time range or a larger data partition can be calculated as a combination of results for smaller time ranges or smaller partitions. For example, a daily number of page views can be calculated as a sum of hourly numbers of page views. Additive state allows one to split processing of a stream into processing of batches that can be computed and re-computed independently and, as we discussed in the previous sections, this helps to simplify lineage tracking and reduce complexity of state maintenance.&lt;/p&gt;

&lt;p&gt;It is not always trivial to achieve additivity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In many cases, additivity is indeed trivial. For example, simple counters are additive.&lt;/li&gt;
  &lt;li&gt;In some cases, it is possible to achieve additivity by storing a small amount of additional information. For example, consider a system that calculates average purchase value in the internet shop for each hour. Daily average cannot be obtained from 24 hourly average values. However, the system can easily store a number of transactions along with each hourly average and it is enough to calculate the daily average value.&lt;/li&gt;
  &lt;li&gt;In many cases, it is very difficult or impossible to achieve additivity. For example, consider a system that counts unique visitors on some internet site. If 100 unique users visited the site yesterday and 100 unique user visited the site today, the total number of unique user for two days can be from 100 to 200 depends on how many users visited the site both yesterday and today. One have to maintain lists of user IDs to achieve additivity through intersection/union of the ID lists. Size and processing complexity for these lists can be comparable to the size and processing complexity of the raw data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sketches is a very efficient way to transform non-additive values into additive. In the previous example, lists of ID can be replaced by compact additive statistical counters. These counters provide approximations instead of precise result, but it is acceptable for many practical applications. Sketches are very popular in certain areas like internet advertising and can be considered as an independent pattern of in-stream processing. A thorough overview of the sketching techniques can be found in [5].&lt;/p&gt;

&lt;h3 id=&quot;logical-time-tracking&quot;&gt;Logical Time Tracking&lt;/h3&gt;

&lt;p&gt;It is very common for in-stream computations to depend on time: aggregations and joins are often performed on sliding time windows; processing logic often depends on a time interval between events and so on. Obviously, the in-stream processing system should have a notion of application’s view of time, instead of CPU wall-clock. However, proper time tracking is not trivial because data streams and particular events can be replayed in case of failures. It is often a good idea to have a notion of global logical time that can be implemented as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All events should be marked with a timestamp generated by the original application.&lt;/li&gt;
  &lt;li&gt;Each processor in a pipeline tracks the maximal timestamp it has seen in a stream and updates a global persistent clock by this timestamp if the global clock is behind. All other processors synchronize their time with the global clock.&lt;/li&gt;
  &lt;li&gt;Global clock can be reset in case of data replay.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aggregation-in-a-persistent-store&quot;&gt;Aggregation in a Persistent Store&lt;/h3&gt;

&lt;p&gt;We already have discussed that persistent store can be used for state checkpointing. However, it not the only way to employ an external store for in-stream processing. Let us consider an example that employs Cassandra to join multiple data streams over a time window. Instead of maintaining in-memory event buffers, one can simply save all incoming events from all data streams to Casandra using a join key as row key, as it shown in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/cassandra-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other side, the second process traverses the records periodically, assembles and emits joined events, and evicts the events that fell out of the time window. Cassandra even can facilitate this activity by sorting events according to their timestamps.
It is important to understand that such techniques can defeat the whole purpose of in-stream data processing if implemented incorrectly – writing individual events to the data store can introduce a serious performance bottleneck even for fast stores like Cassandra or Redis. On the other hand, this approach provides perfect persistence of the computational state and different performance optimizations – say, batch writes – can help to achieve acceptable performance in many use cases.&lt;/p&gt;

&lt;h3 id=&quot;aggregation-on-a-sliding-window&quot;&gt;Aggregation on a Sliding Window&lt;/h3&gt;

&lt;p&gt;In-stream data processing frequently deals with queries like “What is the sum of the values in the stream over last 10 minutes?” i.e. with continuous queries on a sliding time window. A straightforward approach to processing of such queries is to compute the aggregation function like sum for each instance of the time window independently. It is clear that this approach is not optimal because of the high similarity between two sequential instances of the time window. If the window at the time T contains samples {s(0), s(1), s(2), …, s(T-1), s(T)}, then the window at the time T+1 contains samples {s(1), s(2), s(3), …, s(T), s(T+1)}. This observation suggests that incremental processing might be used.&lt;/p&gt;

&lt;p&gt;Incremental computations over sliding windows is a group of techniques that are widely used in digital signal processing, in both software and hardware. A typical example is a computation of the sum function. If the sum over the current time window is known, then the sum over the next time window can be computed by adding a new sample and subtracting the eldest sample in the window:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/inremental-aggregation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar techniques exist not only for simple aggregations like sums or products, but also for more complex transformations. For example, the SDFT (Sliding Discreet Fourier Transform) algorithm [4] is a computationally efficient alternative to per-window calculation of the FFT (Fast Fourier Transform) algorithm.&lt;/p&gt;

&lt;h2 id=&quot;query-processing-pipeline-storm-cassandra-kafka&quot;&gt;Query Processing Pipeline: Storm, Cassandra, Kafka&lt;/h2&gt;

&lt;p&gt;Now let us return to the practical problem that was stated in the beginning of this article. We have designed and implemented our in-stream data processing system on top of Storm, Kafka, and Cassandra adopting the techniques described earlier in this article. Here we provide just a very brief overview of the solution – a detailed description of all implementation pitfalls and tricks is too large and probably requires a separate article.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/storm-kafka-cassandra-system.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The system naturally uses Kafka 0.8 as a partitioned fault-tolerant event buffer to enable stream replay and improve system extensibility by easy addition of new event producers and consumers. Kafka’s ability to rewind read pointers also enables random access to the incoming batches and, consequently, Spark-style lineage tracking. It is also possible to point the system input to HDFS to process the historical data.&lt;/p&gt;

&lt;p&gt;Cassandra is employed for state checkpointing and in-store aggregation, as described earlier. In many use cases, it also stores the final results.&lt;/p&gt;

&lt;p&gt;Twitter’s Storm is a backbone of the system. All active query processing is performed in Storm’s topologies that interact with Kafka and Cassandra. Some data flows are simple and straightforward: the data arrives to Kafka; Storm reads and processes it and persist the results to Cassandra or other destination. Other flows are more sophisticated: one Storm topology can pass the data to another topology via Kafka or Cassandra. Two examples of such flows are shown in the figure above (red and blue curved arrows).&lt;/p&gt;

&lt;h2 id=&quot;towards-unified-big-data-processing&quot;&gt;Towards Unified Big Data Processing&lt;/h2&gt;

&lt;p&gt;It is great that the existing technologies like Hive, Storm, and Impala enable us to crunch Big Data using both batch processing for complex analytics and machine learning, and real-time query processing for online analytics, and in-stream processing for continuous querying. Moreover, techniques like Lambda Architecture [6, 7] were developed and adopted to combine these solutions efficiently. This brings us to the question of how all these technologies and approaches could converge to a solid solution in the future.  In this section, we discuss the striking similarity between distributed relational query processing, batch processing, and in-stream query processing to figure out the technologies that could cover all these use cases and, consequently, have the highest potential in this area.&lt;/p&gt;

&lt;p&gt;The key observation is that relational query processing, MapReduce, and in-stream processing could be implemented using exactly the same concepts and techniques like shuffling and pipelining. At the same time:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In-stream processing could require strict data delivery guarantees and persistence of the intermediate state. These properties are not crucial for batch processing where computations can be easily restarted.&lt;/li&gt;
  &lt;li&gt;In-stream processing is inseparable from pipelining. For batch processing, pipelining is not so crucial and even inapplicable in certain cases. Systems like Apache 
Hive are based on staged MapReduce with materialization of the intermediate state and do not take full advantage of pipelining.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two statement above imply that tunable persistence (in-memory message passing versus on-disk materialization) and reliability are the distinctive features of the imaginary query engine that provides a set of processing primitives and interfaces to the high-level frameworks:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/unified-engine.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Among the emerging technologies, the following two are especially notable in the context of this discussion:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Tez [8], a part of the Stinger Initiative [9]. Apache Tez is designed to succeed the MapReduce framework introducing a set of fine-grained query processing primitives. The goal is to enable frameworks like Apache Pig and Apache Hive to decompose their queries and scripts into efficient query processing pipelines instead of sequences of MapReduce jobs that are generally slow due to materialization of intermediate results.&lt;/li&gt;
  &lt;li&gt;Apache Spark [10]. This project is probably the most advanced and promising technology for unified Big Data processing that already includes a batch processing framework, SQL query engine, and a stream processing framework.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;A. Wilschut and P. Apers, “Dataflow Query Execution in a Parallel Main-Memory Environment “&lt;/li&gt;
  &lt;li&gt;T. Urhan and M. Franklin, “XJoin: A Reactively-Scheduled Pipelined Join Operator“&lt;/li&gt;
  &lt;li&gt;M. Zaharia, T. Das, H. Li, S. Shenker, and I. Stoica, “Discretized Streams: An Efﬁcient and Fault-Tolerant Model for Stream Processing on Large Clusters”&lt;/li&gt;
  &lt;li&gt;E. Jacobsen and R. Lyons, &lt;a href=&quot;http://www.ingelec.uns.edu.ar/pds2803/materiales/articulos/slidingdft_bw.pdf&quot;&gt;“The Sliding DFT“&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;A. Elmagarmid, Data Streams Models and Algorithms&lt;/li&gt;
  &lt;li&gt;N. Marz, &lt;a href=&quot;http://www.databasetube.com/database/big-data-lambda-architecture/&quot;&gt;“Big Data Lambda Architecture”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. Kinley, &lt;a href=&quot;http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting&quot;&gt;“The Lambda architecture: principles for architecting realtime Big Data systems”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/hadoop/tez/&quot;&gt;http://hortonworks.com/hadoop/tez/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/stinger/&quot;&gt;http://hortonworks.com/stinger/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark-project.org/&quot;&gt;http://spark-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;这次看到dirtysalt的文章：&lt;a href=&quot;http://dirlt.com/in-stream-big-data-processing.html&quot;&gt;In-stream-big-data-processing（英文版+中文注释）&lt;/a&gt;，顿时解决了困扰自己的一个问题，英文的好文章，如何做记录？直接翻译中文？直接转载中文？NO，最佳方式当然是在原文基础上，添加自己的注释，OK，作者dirtysalt已经在做了，今后我也这么办。&lt;/p&gt;

</content>
   </entry>
   
 
</feed>
