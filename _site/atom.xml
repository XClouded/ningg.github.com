<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>NingG.github.com</title>
   <link href="http://ningg.github.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://ningg.github.com" rel="alternate" type="text/html" />
   <updated>2014-10-22T23:22:15+08:00</updated>
   <id>http://ningg.github.com</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>JSON简介以及JAVA API</title>
     <link href="http://ningg.github.com/json-java-api"/>
     <updated>2014-10-24T00:00:00+08:00</updated>
     <id>http://ningg.github.com/json-java-api</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近做个数据采集的东西，初步决定使用JSON作为数据交换格式，OK，学习整理一下。&lt;/p&gt;

&lt;h2 id=&quot;json&quot;&gt;JSON简介&lt;/h2&gt;

&lt;p&gt;JSON（JavaScript Object Notation），轻量级的数据交换格式，易于阅读和编写，同时机器也很容易输出JSON格式、解析JSON格式。JSON是完全独立于语言的文本格式，这使其成为理想的数据交换语言。&lt;/p&gt;

&lt;p&gt;JSON中两类基本结构：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;key:value&lt;/code&gt;：键-值对，通过key来标识value；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;array&lt;/code&gt;：有序的数组；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JSON利用上述的两类基本结构，实现了集中基本数据类型：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Object&lt;/strong&gt;：An object is an unordered set of name/value pairs. An object begins with { (left brace) and ends with } (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma).
（无序的key-value对，以&lt;code&gt;{&lt;/code&gt;开头，以&lt;code&gt;}&lt;/code&gt;结尾，其内部以&lt;code&gt;,&lt;/code&gt;逗号分隔）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/json-java-api/object.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Array&lt;/strong&gt;：An array is an ordered collection of values. An array begins with &lt;code&gt;[&lt;/code&gt; (left bracket) and ends with &lt;code&gt;]&lt;/code&gt; (right bracket). Values are separated by , (comma).
（有序的value序列，以&lt;code&gt;[&lt;/code&gt;开头，以&lt;code&gt;]&lt;/code&gt;结尾，其内部以&lt;code&gt;,&lt;/code&gt;逗号分隔）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/json-java-api/array.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt;：A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested.
（Value表示的内容比较广，既可以是”“包含起来的String，也可以是数字，或者&lt;code&gt;true&lt;/code&gt;`false&lt;code&gt;；另一方面，也可以是&lt;/code&gt;Object`或者array）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/json-java-api/value.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;String&lt;/strong&gt;：A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string. A string is very much like a C or Java string.
（&lt;code&gt;&quot;&quot;&lt;/code&gt;双引号包含起来的Unicode 字符，其中可以使用&lt;code&gt;backslash&lt;/code&gt;来标识转义字符）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/json-java-api/string.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Number&lt;/strong&gt;：A number is very much like a C or Java number, except that the octal and hexadecimal formats are not used.
（不支持octal和hexadecimal formats）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/json-java-api/number.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whitespace can be inserted between any pair of tokens. Excepting a few encoding details, that completely describes the language.
（任何符号之间都可插入空格&lt;code&gt;whitespace&lt;/code&gt;）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：JSON中&lt;code&gt;key&lt;/code&gt;能否重复？&lt;/p&gt;

&lt;h2 id=&quot;jsonjava-api&quot;&gt;处理JSON的JAVA API&lt;/h2&gt;

&lt;p&gt;处理JSON格式数据，无非两条路：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;JDK 自带的 Java API；（官方）&lt;/li&gt;
  &lt;li&gt;第三方jar包提供的java API；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：&lt;a href=&quot;https://jcp.org/en/jsr/detail?id=353&quot;&gt;JSR 353&lt;/a&gt;指出，今后的Java SE 6以及Java EE 7中要添加API来支持JSON格式数据的解析和转换。当前个人查证，在JDK6u30中没有java API来解析JSON；Java EE 7中，已经提供了&lt;code&gt;javax.json&lt;/code&gt;包来支持解析JSON。&lt;/p&gt;

&lt;p&gt;当前项目需求，在JDK5以及之上的版本都能进行JSON字符串与JSON对象之间的转换，OK，那直接上第三方jar包得了。&lt;/p&gt;

&lt;h3 id=&quot;javajsonjar&quot;&gt;java解析JSON的第三方jar包&lt;/h3&gt;

&lt;p&gt;从[JSON 主页][介绍 JSON]可知，当前，有很多的第三方jar包：org.json、org.json.me、jsonp、&lt;a href=&quot;http://jackson.codehaus.org/&quot;&gt;Jackson Json Processor&lt;/a&gt;、&lt;a href=&quot;http://code.google.com/p/google-gson/&quot;&gt;google-gson&lt;/a&gt;、Json-lib…，有点多呀，到底选哪个呢？当前初步考虑在如下两个中选：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spring中使用的是&lt;a href=&quot;http://jackson.codehaus.org/&quot;&gt;org.codehaus.jackson&lt;/a&gt;详细版本号&lt;code&gt;1.9.13&lt;/code&gt;（后来Spring 3.2中已经支持Jackson2了）&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.google.com/p/google-gson/&quot;&gt;google-gson&lt;/a&gt;的官方网站打不开，不过在maven中央仓库找到了gson的&lt;a href=&quot;http://repo1.maven.org/maven2/com/google/code/gson/gson/2.2.3/&quot;&gt;jar包&lt;/a&gt;，虽然无法查看官网的文档，不过maven中央仓库的javadoc、source文件也可以用来学习。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最终决定采用gson，其基本的JSON操作，参考：&lt;a href=&quot;http://blog.csdn.net/lk_blog/article/details/7685169&quot;&gt;JSON转换利器:Gson&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;json-1&quot;&gt;解析JSON字符串的效率问题&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://ifeve.com/json-java-api/&quot;&gt;处理JSON的Java API ：JSON的简介&lt;/a&gt;中提到解析JSON的API分为两类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对象模型API&lt;/li&gt;
  &lt;li&gt;流API&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两种方式在原理、效率上都有差异，TODO&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.json.org/json-zh.html&quot;&gt;介绍 JSON&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ifeve.com/json-java-api/&quot;&gt;处理JSON的Java API ：JSON的简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jcp.org/en/jsr/detail?id=353&quot;&gt;JSR 353&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/lk_blog/article/details/7685169&quot;&gt;JSON转换利器:Gson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume 1.5.0.1：如何将flume聚合的数据送入Kafka</title>
     <link href="http://ningg.github.com/flume-with-kafka"/>
     <updated>2014-10-24T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-with-kafka</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;Flume收集分布在不同机器上的日志信息，聚合之后，将信息送入Kafka消息队列，问题来了：如何将Flume输出的信息送入Kafka中？&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;#&lt;/h1&gt;

&lt;h2 id=&quot;flume&quot;&gt;Flume复习&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;几个概念&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Flume event: a unit of data flow, having a byte payload and an optional set of string attributes.（event中包含了，payload和attributes）&lt;/li&gt;
  &lt;li&gt;Flume agent: a (JVM) process, that hosts the components through which events flow from an external source to the next destination(hop).（agent对应JVM process）&lt;/li&gt;
  &lt;li&gt;Channel:  passive store, keeps the event until it’s consumded by a Flume Sink.（Channel不会主动消费event，其等待Sink来取数据，会在本地备份Event）&lt;/li&gt;
  &lt;li&gt;Sink: remove the event from the channel and put it into external repository.（Sink主动从Channel中取出event）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/flume-user-guide/UserGuide_image00.png&quot; alt=&quot;&quot; /&gt; &lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>win下搭建Storm topology的开发调试环境</title>
     <link href="http://ningg.github.com/storm-dev-env-with-eclipse"/>
     <updated>2014-10-23T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-dev-env-with-eclipse</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;Storm topologies，支撑multilang，不同通常使用java来编写，这样我就想在Eclipse下来编写Storm topologies，毕竟IDE能够加快开发效率。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;下面的内容，基本都是从官网看的，使用自己语言重新写了一遍，建议有追求的Coder/Engineer/Scientist，还是去看官网吧，看官网才是捷径。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;系统环境&lt;/h2&gt;

&lt;p&gt;今天要进行Storm topology开发的系统，基本环境：win xp(x86)操作系统；更详细的编译环境信息，通过如下方式查看：&lt;code&gt;CMD&lt;/code&gt;–&lt;code&gt;systeminfo&lt;/code&gt;，这个命令执行需要时间，10~40s，稍等一会儿，得到如下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Documents and Settings\ningg&amp;gt;systeminfo

OS 名称:      Microsoft Windows XP Professional
OS 版本:      5.1.2600 Service Pack 3 Build 2600
OS 制造商:    Microsoft Corporation
OS 构件类型:  Multiprocessor Free
系统制造商:   LENOVO
系统型号:     ThinkCentre M6400t-N000
系统类型:     X86-based PC
处理器:       安装了 1 个处理器。
       [01]: x86 Family 6 Model 58 Stepping 9 GenuineIntel ~3392 Mhz
BIOS 版本:    LENOVO - 14f0
物理内存总量: 3,546 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;eclipsestorm-start&quot;&gt;eclipse下查看storm-start工程&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/storm/tree/master/examples/storm-starter&quot;&gt;storm-starter&lt;/a&gt;是Storm官网提供的一个例子，简要介绍了storm topology的编写，在&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;storm Tutorial&lt;/a&gt;中重点讲解了这个例子；总之，一点：&lt;a href=&quot;https://github.com/apache/storm/tree/master/examples/storm-starter&quot;&gt;storm-starter&lt;/a&gt;是入门学习的典型例子。OK，我准备在Eclipse下查看&lt;a href=&quot;https://github.com/apache/storm/tree/master/examples/storm-starter&quot;&gt;storm-starter&lt;/a&gt;工程。&lt;/p&gt;

&lt;p&gt;说一下我在Eclipse下的操作步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;下载storm发行版本的源代码：&lt;a href=&quot;http://storm.apache.org/downloads.html&quot;&gt;apache-storm-0.9.2-incubating-src.zip&lt;/a&gt;，并解压；&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;File&lt;/code&gt;–&lt;code&gt;Import&lt;/code&gt;–&lt;code&gt;Existing Maven Projects&lt;/code&gt;；&lt;/li&gt;
  &lt;li&gt;选择storm源代码的&lt;code&gt;examples\storm-starter&lt;/code&gt;目录，对，然后一路next下去；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;好了，中间可能提示maven项目有错误，不要管，一直往下走。接下来说一下如何解决maven项目的bug，我导入storm-starter工程后，&lt;code&gt;pom.xml&lt;/code&gt;文件上冒了个&lt;span style=&quot;color:red&quot;&gt;红色的X&lt;/span&gt;，找到相应位置，按下&lt;code&gt;F2&lt;/code&gt;，显示出错信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Plugin execution not covered by lifecycle configuration: 
com.theoryinpractise:clojure-maven-plugin:1.3.18:compile 
(execution: compile, phase: compile)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-dev-env-with-eclipse/pom-error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OK，不要管这个，直接在storm-starter工程上，&lt;code&gt;右键&lt;/code&gt;–&lt;code&gt;Run As&lt;/code&gt;–&lt;code&gt;Maven build&lt;/code&gt;，输入参数：&lt;code&gt;clean install -DskipTests=true&lt;/code&gt;；然后，&lt;code&gt;Run&lt;/code&gt;；至此，打完收工，妥妥的，结果如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-dev-env-with-eclipse/build-finished.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;到这一步，就可以参照&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;storm Tutorial&lt;/a&gt;、&lt;a href=&quot;https://github.com/apache/storm/tree/master/examples/storm-starter&quot;&gt;storm-starter&lt;/a&gt;中的说明进行一步步的操作，来熟悉Storm topology。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：Eclipse下就可以直接开发、调试topology了吗？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RE&lt;/strong&gt;：是的，直接开发，&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;storm Tutorial&lt;/a&gt;中的例子就是这样。&lt;/p&gt;

&lt;h2 id=&quot;storm&quot;&gt;本地安装Storm&lt;/h2&gt;

&lt;p&gt;下载&lt;a href=&quot;http://storm.apache.org/downloads.html&quot;&gt;Storm的binary版本&lt;/a&gt;，就两点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;把 Storm的&lt;code&gt;bin&lt;/code&gt;目录添加到&lt;code&gt;PATH&lt;/code&gt;中；&lt;/li&gt;
  &lt;li&gt;验证是否安装成功：执行&lt;code&gt;storm&lt;/code&gt;命令，查看是否提示出错；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：如果只是开发Storm topology，需要在本地win xp系统上安装Storm？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RE&lt;/strong&gt;：我来告诉你吧，本地安装Storm，核心用途是：充当client，向远端Storm cluster提交编写好的topology。重新来理一下，eclipse下新建工程，maven添加storm的依赖，即可进行topology的开发；然后通过本地安装的storm，可以进行本地的test、develop；最终，通过本地安装的storm充当client，可以向storm cluster提交topology。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;疑问：上面已经可以进行Storm topology的开发了，但如果希望查看Storm源代码，特别是Clojure编写的那部分，怎么办？&lt;/p&gt;

  &lt;p&gt;关于这个问题，官网有提示：&lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-2&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-storm/tree/master/examples/storm-starter&quot;&gt;Storm Example: storm-starter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Storm: Setting up development environment&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Storm: Creating a new Storm project&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm：Creating a new Storm project</title>
     <link href="http://ningg.github.com/storm-creating-a-new-strom-project"/>
     <updated>2014-10-22T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-creating-a-new-strom-project</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This page outlines how to set up a Storm project for development. The steps are:
（本文重点：介绍如何设置一个Storm project用于开发。）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add Storm jars to classpath（把storm的jar添加到classpath中）&lt;/li&gt;
  &lt;li&gt;If using multilang, add multilang dir to classpath（如果用了multilang，将dir添加到classpath中）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Follow along to see how to set up the &lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt; project in Eclipse.&lt;/p&gt;

&lt;h2 id=&quot;add-storm-jars-to-classpath&quot;&gt;Add Storm jars to classpath&lt;/h2&gt;

&lt;p&gt;You’ll need the Storm jars on your classpath to develop Storm topologies. Using &lt;a href=&quot;http://storm.apache.org/documentation/Maven.html&quot;&gt;Maven&lt;/a&gt; is highly recommended. &lt;a href=&quot;https://github.com/nathanmarz/storm-starter/blob/master/m2-pom.xml&quot;&gt;Here’s an example&lt;/a&gt; of how to setup your pom.xml for a Storm project. If you don’t want to use Maven, you can include the jars from the Storm release on your classpath.
（推荐使用Maven方式，将Storm的jar包添加到classpath中）&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt; uses &lt;a href=&quot;http://github.com/technomancy/leiningen&quot;&gt;Leiningen&lt;/a&gt; for build and dependency resolution. You can install leiningen by downloading &lt;a href=&quot;https://raw.github.com/technomancy/leiningen/stable/bin/lein&quot;&gt;this script&lt;/a&gt;, placing it on your path, and making it executable. To retrieve the dependencies for Storm, simply run &lt;code&gt;lein deps&lt;/code&gt; in the project root.
（storm-starter project用了Leiningen来构建和进行依赖管理的，因此，下载Leiningen到本地，保证其可以执行，然后到project的根目录，执行&lt;code&gt;lein deps&lt;/code&gt;命令，来获取Storm的依赖）&lt;/p&gt;

&lt;p&gt;To set up the classpath in Eclipse, create a new Java project, include &lt;code&gt;src/jvm/&lt;/code&gt; as a source path, and make sure all the jars in &lt;code&gt;lib/&lt;/code&gt; and &lt;code&gt;lib/dev/&lt;/code&gt; are in the Referenced Libraries section of the project.
（创建java project，将&lt;code&gt;src/jvm/&lt;/code&gt;设置为source path，并将&lt;code&gt;lib/&lt;/code&gt;和&lt;code&gt;lib/dev/&lt;/code&gt;添加到build path中）&lt;/p&gt;

&lt;h2 id=&quot;if-using-multilang-add-multilang-dir-to-classpath&quot;&gt;If using multilang, add multilang dir to classpath&lt;/h2&gt;

&lt;p&gt;If you implement spouts or bolts in languages other than Java, then those implementations should be under the &lt;code&gt;multilang/resources/&lt;/code&gt; directory of the project. For Storm to find these files in local mode, the &lt;code&gt;resources/&lt;/code&gt; dir needs to be on the classpath. You can do this in Eclipse by adding &lt;code&gt;multilang/&lt;/code&gt; as a source folder. You may also need to add &lt;code&gt;multilang/resources&lt;/code&gt; as a source directory.
（使用multilang编写spout和bolt时，需要将实际的源代码放在&lt;code&gt;multilang/resources/&lt;/code&gt;目录）&lt;/p&gt;

&lt;p&gt;For more information on writing topologies in other languages, see &lt;a href=&quot;http://storm.apache.org/documentation/Using-non-JVM-languages-with-Storm.html&quot;&gt;Using non-JVM languages with Storm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To test that everything is working in Eclipse, you should now be able to &lt;code&gt;Run&lt;/code&gt; the &lt;code&gt;WordCountTopology.java&lt;/code&gt; file. You will see messages being emitted at the console for 10 seconds.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;Apache Storm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Apache Storm: Documentation Rationale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm：setting up a development environment</title>
     <link href="http://ningg.github.com/storm-setting-up-dev-env"/>
     <updated>2014-10-21T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-setting-up-dev-env</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Setting up a development environment&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This page outlines what you need to do to get a Storm development environment set up. In summary, the steps are:
（本文重点：set up a Storm dev env。概括一下，基本步骤如下）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download a &lt;a href=&quot;http://storm.apache.org//downloads.html&quot;&gt;Storm release&lt;/a&gt; , unpack it, and put the unpacked &lt;code&gt;bin/&lt;/code&gt; directory on your &lt;code&gt;PATH&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;To be able to start and stop topologies on a remote cluster, put the cluster information in &lt;code&gt;~/.storm/storm.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More detail on each of these steps is below.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-development-environment&quot;&gt;What is a development environment?&lt;/h2&gt;

&lt;p&gt;Storm has two modes of operation: &lt;code&gt;local mode&lt;/code&gt; and &lt;code&gt;remote mode&lt;/code&gt;. In local mode, you can develop and test topologies completely in process on your local machine. In remote mode, you submit topologies for execution on a cluster of machines.
（两种mode：local mode，develop和test topologies；remote mode，真正执行时，submit topologies到cluster）&lt;/p&gt;

&lt;p&gt;A Storm development environment has everything installed so that you can develop and test Storm topologies in local mode, package topologies for execution on a remote cluster, and submit/kill topologies on a remote cluster.&lt;/p&gt;

&lt;p&gt;Let’s quickly go over the relationship between your machine and a remote cluster. A Storm cluster is managed by a master node called “Nimbus”. Your machine communicates with Nimbus to submit code (packaged as a jar) and topologies for execution on the cluster, and Nimbus will take care of distributing that code around the cluster and assigning workers to run your topology. Your machine uses a command line client called storm to communicate with Nimbus. The storm client is only used for remote mode; it is not used for developing and testing topologies in local mode.
（master node，called &lt;code&gt;Nimbus&lt;/code&gt;，管理整个cluster，提交的code（jar包）、topologies都是交给&lt;code&gt;Nimbus&lt;/code&gt;负责接收的；之后，&lt;code&gt;Nimbus&lt;/code&gt;负责distribute code、assign worker to run topologies；本地提交topologies的机器，通过调用 storm client来通知storm与&lt;code&gt;Nimbus&lt;/code&gt;通信。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：distribute code？与assign worker to run topologies有区别吗？&lt;/p&gt;

&lt;h2 id=&quot;installing-a-storm-release-locally&quot;&gt;Installing a Storm release locally&lt;/h2&gt;

&lt;p&gt;If you want to be able to submit topologies to a remote cluster from your machine, you should install a Storm release locally. Installing a Storm release will give you the &lt;code&gt;storm&lt;/code&gt; client that you can use to interact with remote clusters. To install Storm locally, download a release &lt;a href=&quot;https://github.com/apache/incubator-storm/downloads&quot;&gt;from here&lt;/a&gt; and unzip it somewhere on your computer. Then add the unpacked &lt;code&gt;bin/&lt;/code&gt; directory onto your &lt;code&gt;PATH&lt;/code&gt; and make sure the &lt;code&gt;bin/storm&lt;/code&gt; script is executable.
（本地安装的Storm，也可以作为与remote cluster交互的client；安装办法：下载、解压、添加bin到PATH）&lt;/p&gt;

&lt;p&gt;Installing a Storm release locally is only for interacting with remote clusters. For developing and testing topologies in local mode, it is recommended that you use Maven to include Storm as a dev dependency for your project. You can read more about using Maven for this purpose on &lt;a href=&quot;http://storm.apache.org/documentation/Maven.html&quot;&gt;Maven&lt;/a&gt;.
（本地安装Storm唯一目标：interact with remote cluster；如果想利用local mode来进行develop、test，建议使用Maven将Storm作为依赖导入。）&lt;/p&gt;

&lt;h2 id=&quot;starting-and-stopping-topologies-on-a-remote-cluster&quot;&gt;Starting and stopping topologies on a remote cluster&lt;/h2&gt;

&lt;p&gt;The previous step installed the &lt;code&gt;storm&lt;/code&gt; client on your machine which is used to communicate with remote Storm clusters. Now all you have to do is tell the client which Storm cluster to talk to. To do this, all you have to do is put the host address of the master in the &lt;code&gt;~/.storm/storm.yaml&lt;/code&gt; file. It should look something like this:
（在本地安装storm实质是为了interact with remote cluster，这就需要告诉本地storm：remote cluster的位置。）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nimbus.host: &quot;123.45.678.890&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, if you use the &lt;a href=&quot;https://github.com/nathanmarz/storm-deploy&quot;&gt;storm-deploy&lt;/a&gt; project to provision Storm clusters on AWS, it will automatically set up your ~/.storm/storm.yaml file. You can manually attach to a Storm cluster (or switch between multiple clusters) using the “attach” command, like so:
（还有一种方法：如果Storm部署在AWS上，可直接使用 “attach” command）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lein run :deploy --attach --name mystormcluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More information is on the storm-deploy &lt;a href=&quot;https://github.com/nathanmarz/storm-deploy/wiki&quot;&gt;wiki&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;Apache Storm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Apache Storm: Documentation Rationale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm：Tutorial</title>
     <link href="http://ningg.github.com/storm-tutorial"/>
     <updated>2014-10-20T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-tutorial</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;Storm Tutorial&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this tutorial, you’ll learn how to create Storm topologies and deploy them to a Storm cluster. Java will be the main language used, but a few examples will use Python to illustrate Storm’s multi-language capabilities.
（这个tutorial中，着重说明：如何创建Storm topologies，如何将Storm topologies部署到Storm cluster中。）&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;p&gt;This tutorial uses examples from the &lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt; project. It’s recommended that you clone the project and follow along with the examples. Read &lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Setting up a development environment&lt;/a&gt; and &lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt; to get your machine set up.
（example都是来自&lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt; project，建议clone一份，跟着例子操作一把。在此之前，需要参考&lt;a href=&quot;http://storm.apache.org/documentation/Setting-up-development-environment.html&quot;&gt;Setting up a development environment&lt;/a&gt; 和 &lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt;来配置一下环境）&lt;/p&gt;

&lt;h2 id=&quot;components-of-a-storm-cluster&quot;&gt;Components of a Storm cluster&lt;/h2&gt;

&lt;p&gt;A Storm cluster is superficially similar to a Hadoop cluster. Whereas on Hadoop you run “MapReduce jobs”, on Storm you run “topologies”. “Jobs” and “topologies” themselves are very different – one key difference is that a MapReduce job eventually finishes, whereas a topology processes messages forever (or until you kill it).
（Storm cluster与Hadoop cluster类似，Hadoop中运行&lt;code&gt;MapReduce jobs&lt;/code&gt;，Storm中运行&lt;code&gt;topologies&lt;/code&gt;。）&lt;/p&gt;

&lt;p&gt;There are two kinds of nodes on a Storm cluster: the master node and the worker nodes. The master node runs a daemon called “Nimbus” that is similar to Hadoop’s “JobTracker”. Nimbus is responsible for distributing code around the cluster, assigning tasks to machines, and monitoring for failures.
（Storm cluster中两类node：master node、worker node；master上运行一个daemon——&lt;code&gt;Nimbus&lt;/code&gt;，其负责：distribute code，assign task，monitor failures）&lt;/p&gt;

&lt;p&gt;Each worker node runs a daemon called the “Supervisor”. The supervisor listens for work assigned to its machine and starts and stops worker processes as necessary based on what Nimbus has assigned to it. Each worker process executes a subset of a topology; a running topology consists of many worker processes spread across many machines.
（worker上运行daemon——&lt;code&gt;Supervisor&lt;/code&gt;，监听来自&lt;code&gt;Nimbus&lt;/code&gt;的任务，并starts或stop worker process。每个worker上执行topology的一部分，也就是说，a running topology 包含很多分布在不同机器上的worker process。）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/storm-cluster.png&quot; alt=&quot;Storm cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All coordination between Nimbus and the Supervisors is done through a &lt;a href=&quot;http://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt; cluster. Additionally, the Nimbus daemon and Supervisor daemons are fail-fast and stateless; all state is kept in Zookeeper or on local disk. This means you can kill -9 Nimbus or the Supervisors and they’ll start back up like nothing happened. This design leads to Storm clusters being incredibly stable.
（Zookeeper cluster负责&lt;code&gt;Nimbus&lt;/code&gt;和&lt;code&gt;Supervisor&lt;/code&gt;之间的协调；Nimbus和Supervisor deamon是fail-fast和stateless的，所有的状态都存储在Zookeeper or local disk；即，通过&lt;code&gt;kill -9 Nimbus/Supervisor&lt;/code&gt;，然后重启进程，对于用户基本是透明的）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;a href=&quot;http://en.wikipedia.org/wiki/Fail-fast&quot;&gt;fail-fast&lt;/a&gt;，表示如果输入有误，或者其他异常，则系统立刻停止运行。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;举例说明一下&lt;code&gt;fail-fast&lt;/code&gt;：在并发的时候，如果线程A正遍历一个collection(List, Map, Set etc.)，这时另外一个线程B却修改了该collection的size，线程A就会抛出一个错：ConcurrentModificationException，表明：我正读取的内容被修改掉了，你是否需要重新遍历？或是做其它处理？这就是fail-fast的含义。 &lt;/p&gt;

  &lt;p&gt;Fail-fast是并发中乐观(optimistic)策略的具体应用，它允许线程自由竞争，但在出现冲突的情况下假设你能应对，即你能判断出问题何在，并且给出解决办法。悲观(pessimistic)策略就正好相反，它总是预先设置足够的限制，通常是采用锁(lock)，来保证程序进行过程中的无错，付出的代价是其它线程的等待开销&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;topologies&quot;&gt;Topologies&lt;/h2&gt;

&lt;p&gt;To do realtime computation on Storm, you create what are called “topologies”. A topology is a graph of computation. Each node in a topology contains processing logic, and links between nodes indicate how data should be passed around between nodes.
（topology中每个node都包含了相应的processing logic）&lt;/p&gt;

&lt;p&gt;Running a topology is straightforward. First, you package all your code and dependencies into a single jar. Then, you run a command like the following:（执行 a topology：将your code和dependencies都放到一个jar中）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;storm jar all-my-code.jar backtype.storm.MyTopology arg1 arg2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This runs the class &lt;code&gt;backtype.storm.MyTopology&lt;/code&gt; with the arguments &lt;code&gt;arg1&lt;/code&gt; and &lt;code&gt;arg2&lt;/code&gt;. The main function of the class defines the topology and submits it to Nimbus. The &lt;code&gt;storm jar&lt;/code&gt; part takes care of connecting to Nimbus and uploading the jar.
（&lt;code&gt;storm jar&lt;/code&gt;负责connecting to Nimbus，以及uploading the jar）&lt;/p&gt;

&lt;p&gt;Since topology definitions are just Thrift structs, and Nimbus is a Thrift service, you can create and submit topologies using any programming language. The above example is the easiest way to do it from a JVM-based language. See &lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;Running topologies on a production cluster&lt;/a&gt; for more information on starting and stopping topologies.
（利用Thrift structs，来定义topology，并且，Nimbus是Thrift service，因此，可以使用任何编程语言。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Thrift structs? Thrift service?&lt;/p&gt;

&lt;h2 id=&quot;streams&quot;&gt;Streams&lt;/h2&gt;

&lt;p&gt;The core abstraction in Storm is the “stream”. A stream is an unbounded sequence of tuples. Storm provides the primitives for transforming a stream into a new stream in a distributed and reliable way. For example, you may transform a stream of tweets into a stream of trending topics.
（Storm的核心是&lt;code&gt;Stream&lt;/code&gt;，Storm提供一种分布式的、可靠的数据流转换方式，将原始数据流处理后，转换为其他的数据流）&lt;/p&gt;

&lt;p&gt;The basic primitives Storm provides for doing stream transformations are “spouts” and “bolts”. Spouts and bolts have interfaces that you implement to run your application-specific logic.
（Storm通过&lt;code&gt;spouts&lt;/code&gt;和&lt;code&gt;bolts&lt;/code&gt;来实现stream transformation）&lt;/p&gt;

&lt;p&gt;A spout is a source of streams. For example, a spout may read tuples off of a Kestrel queue and emit them as a stream. Or a spout may connect to the Twitter API and emit a stream of tweets.
（spout是a source of streams）&lt;/p&gt;

&lt;p&gt;A bolt consumes any number of input streams, does some processing, and possibly emits new streams. Complex stream transformations, like computing a stream of trending topics from a stream of tweets, require multiple steps and thus multiple bolts. Bolts can do anything from run functions, filter tuples, do streaming aggregations, do streaming joins, talk to databases, and more.
（bolt负责处理input stream，也可能产生new stream；bolt通过run functions，filter tuples，do streaming aggregations，do streaming joins，talk to database，etc…来做任何事情）&lt;/p&gt;

&lt;p&gt;Networks of spouts and bolts are packaged into a “topology” which is the top-level abstraction that you submit to Storm clusters for execution. A topology is a graph of stream transformations where each node is a spout or bolt. Edges in the graph indicate which bolts are subscribing to which streams. When a spout or bolt emits a tuple to a stream, it sends the tuple to every bolt that subscribed to that stream.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology.png&quot; alt=&quot;A Storm topology&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Links between nodes in your topology indicate how tuples should be passed around. For example, if there is a link between Spout A and Bolt B, a link from Spout A to Bolt C, and a link from Bolt B to Bolt C, then everytime Spout A emits a tuple, it will send the tuple to both Bolt B and Bolt C. All of Bolt B’s output tuples will go to Bolt C as well.&lt;/p&gt;

&lt;p&gt;Each node in a Storm topology executes in parallel. In your topology, you can specify how much parallelism you want for each node, and then Storm will spawn that number of threads across the cluster to do the execution.
（topology中所有node都是并发运行的，可以配置每个node的并发数。）&lt;/p&gt;

&lt;p&gt;A topology runs forever, or until you kill it. Storm will automatically reassign any failed tasks. Additionally, Storm guarantees that there will be no data loss, even if machines go down and messages are dropped.
（topology会自动重启失败的任务，并且run forever——因为是stream processing；即使node机器崩溃并且message丢失，Storm也能保证no data loss）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Storm如何保证node崩溃后，no data loss的？&lt;/p&gt;

&lt;h2 id=&quot;data-model&quot;&gt;Data model&lt;/h2&gt;

&lt;p&gt;Storm uses tuples as its data model. A tuple is a named list of values, and a field in a tuple can be an object of any type. Out of the box, Storm supports all the primitive types, strings, and byte arrays as tuple field values. To use an object of another type, you just need to implement &lt;a href=&quot;http://storm.apache.org/documentation/Serialization.html&quot;&gt;a serializer&lt;/a&gt; for the type.
（Storm中用&lt;code&gt;tuple&lt;/code&gt;结构来存储数据，tuple中的field可以是任何类型的object，自定义的类型，需要实现&lt;a href=&quot;http://storm.apache.org/documentation/Serialization.html&quot;&gt;a serializer&lt;/a&gt;接口）&lt;/p&gt;

&lt;p&gt;Every node in a topology must declare the output fields for the tuples it emits. For example, this bolt declares that it emits 2-tuples with the fields “double” and “triple”:
（每个node都要定义输出的tuple的结构）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
public class DoubleAndTripleBolt extends BaseRichBolt { 

   private OutputCollectorBase _collector;
   
   @Override
   public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
	   _collector = collector;
   }
   
   @Override
   public void execute(Tuple input) {
	   int val = input.getInteger(0);        
	   _collector.emit(input, new Values(val*2, val*3));
	   _collector.ack(input);
   }
   
   @Override
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;double&quot;, &quot;triple&quot;));
   }     

} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The declareOutputFields function declares the output fields &lt;code&gt;[&quot;double&quot;, &quot;triple&quot;]&lt;/code&gt; for the component. The rest of the bolt will be explained in the upcoming sections.&lt;/p&gt;

&lt;h2 id=&quot;a-simple-topology&quot;&gt;A simple topology&lt;/h2&gt;

&lt;p&gt;Let’s take a look at a simple topology to explore the concepts more and see how the code shapes up. Let’s look at the &lt;code&gt;ExclamationTopology&lt;/code&gt; definition from storm-starter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
TopologyBuilder builder = new TopologyBuilder(); 
builder.setSpout(&quot;words&quot;, new TestWordSpout(), 10); 
builder.setBolt(&quot;exclaim1&quot;, new ExclamationBolt(), 3).shuffleGrouping(&quot;words&quot;); 
builder.setBolt(&quot;exclaim2&quot;, new ExclamationBolt(), 2).shuffleGrouping(&quot;exclaim1&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This topology contains a spout and two bolts. The spout emits words, and each bolt appends the string “!!!” to its input. The nodes are arranged in a line: the spout emits to the first bolt which then emits to the second bolt. If the spout emits the tuples “bob” and “john”, then the second bolt will emit the words “bob!!!!!!” and “john!!!!!!”.&lt;/p&gt;

&lt;p&gt;This code defines the nodes using the &lt;code&gt;setSpout&lt;/code&gt; and &lt;code&gt;setBolt&lt;/code&gt; methods. These methods take as input a user-specified id, an object containing the processing logic, and the amount of parallelism you want for the node. In this example, the spout is given id “words” and the bolts are given ids “exclaim1” and “exclaim2”.
（两个方法&lt;code&gt;setSpout&lt;/code&gt;`setBolt`，参数的含义：node id、processing logic、amount of parallelism。）&lt;/p&gt;

&lt;p&gt;The object containing the processing logic implements the &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/topology/IRichSpout.html&quot;&gt;IRichSpout&lt;/a&gt; interface for spouts and the &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/topology/IRichBolt.html&quot;&gt;IRichBolt&lt;/a&gt; interface for bolts.
（包含processing logic的object需要实现IRichSpout\IRichBolt）&lt;/p&gt;

&lt;p&gt;The last parameter, how much parallelism you want for the node, is optional. It indicates how many threads should execute that component across the cluster. If you omit it, Storm will only allocate one thread for that node.
（如果不设置 amount of parallelism的，系统默认自启动一个thread）&lt;/p&gt;

&lt;p&gt;&lt;code&gt;setBolt&lt;/code&gt; returns an &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/topology/InputDeclarer.html&quot;&gt;InputDeclarer&lt;/a&gt; object that is used to define the inputs to the Bolt. Here, component “exclaim1” declares that it wants to read all the tuples emitted by component “words” using a shuffle grouping, and component “exclaim2” declares that it wants to read all the tuples emitted by component “exclaim1” using a shuffle grouping. “shuffle grouping” means that tuples should be randomly distributed from the input tasks to the bolt’s tasks. There are many ways to group data between components. These will be explained in a few sections.
（&lt;code&gt;shuffle grouping&lt;/code&gt;，随机分发tuple）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：Storm是流式处理，需要保证tuple的顺序执行吗？还有tuple需要顺序执行吗？&lt;/p&gt;

&lt;p&gt;If you wanted component “exclaim2” to read all the tuples emitted by both component “words” and component “exclaim1”, you would write component “exclaim2”’s definition like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
builder.setBolt(&quot;exclaim2&quot;, new ExclamationBolt(), 5) .shuffleGrouping(&quot;words&quot;) .shuffleGrouping(&quot;exclaim1&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, input declarations can be chained to specify multiple sources for the Bolt.&lt;/p&gt;

&lt;p&gt;Let’s dig into the implementations of the spouts and bolts in this topology. Spouts are responsible for emitting new messages into the topology. &lt;code&gt;TestWordSpout&lt;/code&gt; in this topology emits a random word from the list &lt;code&gt;[“nathan”, “mike”, “jackson”, “golda”, “bertels”]&lt;/code&gt; as a 1-tuple every 100ms. The implementation of &lt;code&gt;nextTuple()&lt;/code&gt; in TestWordSpout looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
public void nextTuple() { 
    Utils.sleep(100); 
    final String[] words = new String[] {&quot;nathan&quot;, &quot;mike&quot;, &quot;jackson&quot;, &quot;golda&quot;, &quot;bertels&quot;}; 
    final Random rand = new Random(); 
    final String word = words[rand.nextInt(words.length)]; 
    _collector.emit(new Values(word)); 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the implementation is very straightforward.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ExclamationBolt&lt;/code&gt; appends the string “!!!” to its input. Let’s take a look at the full implementation for &lt;code&gt;ExclamationBolt&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
public static class ExclamationBolt implements IRichBolt { OutputCollector _collector;
   
   public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	   _collector = collector;
   }
   
   public void execute(Tuple tuple) {
	   _collector.emit(tuple, new Values(tuple.getString(0) + &quot;!!!&quot;));
	   _collector.ack(tuple);
   }
   
   public void cleanup() {
   }
   
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;word&quot;));
   }
   
   public Map getComponentConfiguration() {
	   return null;
   } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;prepare&lt;/code&gt; method provides the bolt with an &lt;code&gt;OutputCollector&lt;/code&gt; that is used for emitting tuples from this bolt. Tuples can be emitted at anytime from the bolt – in the &lt;code&gt;prepare&lt;/code&gt;, &lt;code&gt;execute&lt;/code&gt;, or &lt;code&gt;cleanup&lt;/code&gt; methods, or even asynchronously in another thread. This &lt;code&gt;prepare&lt;/code&gt; implementation simply saves the &lt;code&gt;OutputCollector&lt;/code&gt; as an instance variable to be used later on in the &lt;code&gt;execute&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;execute&lt;/code&gt; method receives a tuple from one of the bolt’s inputs. The &lt;code&gt;ExclamationBolt&lt;/code&gt; grabs the first field from the tuple and emits a new tuple with the string “!!!” appended to it. If you implement a bolt that subscribes to multiple input sources, you can find out which component the Tuple came from by using the &lt;code&gt;Tuple#getSourceComponent&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;There’s a few other things going in in the &lt;code&gt;execute&lt;/code&gt; method, namely that the input tuple is passed as the first argument to &lt;code&gt;emit&lt;/code&gt; and the input tuple is acked on the final line. These are part of Storm’s reliability API for guaranteeing no data loss and will be explained later in this tutorial.
（在&lt;code&gt;execute&lt;/code&gt;方法中，最后一行进行&lt;code&gt;ack&lt;/code&gt;）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;ack&lt;/code&gt;方法的作用？如何实现的？&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;cleanup&lt;/code&gt; method is called when a Bolt is being shutdown and should cleanup any resources that were opened. There’s no guarantee that this method will be called on the cluster: for example, if the machine the task is running on blows up, there’s no way to invoke the method. The &lt;code&gt;cleanup&lt;/code&gt; method is intended for when you run topologies in &lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;local mode&lt;/a&gt; (where a Storm cluster is simulated in process), and you want to be able to run and kill many topologies without suffering any resource leaks.
（&lt;code&gt;cleanup&lt;/code&gt;的一个用途：local mode中运行Storm时，可以释放资源，防止资源泄漏。）&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;declareOutputFields&lt;/code&gt; method declares that the &lt;code&gt;ExclamationBolt&lt;/code&gt; emits 1-tuples with one field called “word”.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;getComponentConfiguration&lt;/code&gt; method allows you to configure various aspects of how this component runs. This is a more advanced topic that is explained further on &lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;Configuration&lt;/a&gt;.
（component中很多方面的属性都可进行设置）&lt;/p&gt;

&lt;p&gt;Methods like cleanup and getComponentConfiguration are often not needed in a bolt implementation. You can define bolts more succinctly by using a base class that provides default implementations where appropriate. ExclamationBolt can be written more succinctly by extending BaseRichBolt, like so:
（通常，bolt并不需要实现cleanup和getComponentConfiguration；建议：为多个功能相似的bolt创建一个base类，然后所有的实现都继承base类）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
public static class ExclamationBolt extends BaseRichBolt { OutputCollector _collector;
   
   public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	   _collector = collector;
   }
   
   public void execute(Tuple tuple) {
	   _collector.emit(tuple, new Values(tuple.getString(0) + &quot;!!!&quot;));
	   _collector.ack(tuple);
   }
   
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;word&quot;));
   }     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;running-exclamationtopology-in-local-mode&quot;&gt;Running ExclamationTopology in local mode&lt;/h2&gt;

&lt;p&gt;Let’s see how to run the ExclamationTopology in local mode and see that it’s working.&lt;/p&gt;

&lt;p&gt;Storm has two modes of operation: local mode and distributed mode. In local mode, Storm executes completely in process by simulating worker nodes with threads. Local mode is useful for testing and development of topologies. When you run the topologies in storm-starter, they’ll run in local mode and you’ll be able to see what messages each component is emitting. You can read more about running topologies in local mode on &lt;a href=&quot;http://storm.apache.org/documentation/Local-mode.html&quot;&gt;Local mode&lt;/a&gt;.
（local mode，用于testing和development，因为，其可以查看到component发出的所有message）&lt;/p&gt;

&lt;p&gt;In distributed mode, Storm operates as a cluster of machines. When you submit a topology to the master, you also submit all the code necessary to run the topology. The master will take care of distributing your code and allocating workers to run your topology. If workers go down, the master will reassign them somewhere else. You can read more about running topologies on a cluster on &lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;Running topologies on a production cluster&lt;/a&gt;.
（distributed mode，用户需要向master提交topology，以及运行这个topology所需要的所有code，详细信息推荐&lt;a href=&quot;http://storm.apache.org/documentation/Running-topologies-on-a-production-cluster.html&quot;&gt;阅读&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;Here’s the code that runs &lt;code&gt;ExclamationTopology&lt;/code&gt; in local mode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java 
Config conf = new Config(); 
conf.setDebug(true); 
conf.setNumWorkers(2);

LocalCluster cluster = new LocalCluster(); 
cluster.submitTopology(“test”, conf, builder.createTopology()); 

Utils.sleep(10000); 

cluster.killTopology(“test”); 
cluster.shutdown();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, the code defines an in-process cluster by creating a &lt;code&gt;LocalCluster&lt;/code&gt; object. Submitting topologies to this virtual cluster is identical to submitting topologies to distributed clusters. It submits a topology to the &lt;code&gt;LocalCluster&lt;/code&gt; by calling &lt;code&gt;submitTopology&lt;/code&gt;, which takes as arguments a name for the running topology, a configuration for the topology, and then the topology itself.
（为topology指定一个唯一标识的名字）&lt;/p&gt;

&lt;p&gt;The name is used to identify the topology so that you can kill it later on. A topology will run indefinitely until you kill it.
（除非kill掉topology，否则，其永久的运行下去，因为是stream processing嘛，自然是endless）&lt;/p&gt;

&lt;p&gt;The configuration is used to tune various aspects of the running topology. The two configurations specified here are very common:
（有两个参数，重点说一下）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;TOPOLOGY_WORKERS&lt;/strong&gt; (set with &lt;code&gt;setNumWorkers&lt;/code&gt;) specifies how many &lt;code&gt;processes&lt;/code&gt; you want allocated around the cluster to execute the topology. Each component in the topology will execute as many &lt;code&gt;threads&lt;/code&gt;. The number of threads allocated to a given component is configured through the &lt;code&gt;setBolt&lt;/code&gt; and &lt;code&gt;setSpout&lt;/code&gt; methods. Those threads exist within worker processes. Each &lt;code&gt;worker process&lt;/code&gt; contains within it some number of threads for some number of components. For instance, you may have 300 threads specified across all your components and 50 worker processes specified in your config. Each worker process will execute 6 threads, each of which of could belong to a different component. You tune the performance of Storm topologies by tweaking the parallelism for each component and the number of worker processes those threads should run within.（每个component都会启动几个thread，这些Thread都是包含在worker process中的，每个worker process都会包含来自一些components的一些thread。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TOPOLOGY_DEBUG&lt;/strong&gt; (set with &lt;code&gt;setDebug&lt;/code&gt;), when set to true, tells Storm to log every message every emitted by a component. This is useful in local mode when testing topologies, but you probably want to keep this turned off when running topologies on the cluster.（设置Debug模式启动后，component emit的所有message都会被记录下来）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There’s many other configurations you can set for the topology. The various configurations are detailed on &lt;a href=&quot;http://storm.apache.org/apidocs/backtype/storm/Config.html&quot;&gt;the Javadoc for Config&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：worker process、thread、component之间什么关系？worker process只是作为支撑component与thread间的映射关系？&lt;/p&gt;

&lt;p&gt;To learn about how to set up your development environment so that you can run topologies in local mode (such as in Eclipse), see &lt;a href=&quot;http://storm.apache.org/documentation/Creating-a-new-Storm-project.html&quot;&gt;Creating a new Storm project&lt;/a&gt;.
（设置本地的开发环境，这样就可以在Eclipse等中进行调试开发）	&lt;/p&gt;

&lt;h2 id=&quot;stream-groupings&quot;&gt;Stream groupings&lt;/h2&gt;

&lt;p&gt;A stream grouping tells a topology how to send tuples between two components. Remember, spouts and bolts execute in parallel as many tasks across the cluster. If you look at how a topology is executing at the task level, it looks something like this:
（stream grouping负责一个topology中tuples在components间的传递；）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/storm-tutorial/topology-tasks.png&quot; alt=&quot;Tasks in a topology&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When a task for Bolt A emits a tuple to Bolt B, which task should it send the tuple to?
（从Bolt A发出的tuple，发送给Bolt B的那一个task？）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：tuple与task之间什么关系？&lt;/p&gt;

&lt;p&gt;A “stream grouping” answers this question by telling Storm how to send tuples between sets of tasks. Before we dig into the different kinds of stream groupings, let’s take a look at another topology from &lt;a href=&quot;http://github.com/nathanmarz/storm-starter&quot;&gt;storm-starter&lt;/a&gt;. This &lt;a href=&quot;https://github.com/nathanmarz/storm-starter/blob/master/src/jvm/storm/starter/WordCountTopology.java&quot;&gt;WordCountTopology&lt;/a&gt; reads sentences off of a spout and streams out of &lt;code&gt;WordCountBolt&lt;/code&gt; the total number of times it has seen that word before:
（stream grouping，负责分配tuple到task中去；）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
TopologyBuilder builder = new TopologyBuilder();

builder.setSpout(“sentences”, new RandomSentenceSpout(), 5); 
builder.setBolt(“split”, new SplitSentence(), 8) .shuffleGrouping(“sentences”); 
builder.setBolt(“count”, new WordCount(), 12) .fieldsGrouping(“split”, new Fields(“word”));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;SplitSentence&lt;/code&gt; emits a tuple for each word in each sentence it receives, and WordCount keeps a map in memory from word to count. Each time WordCount receives a word, it updates its state and emits the new word count.&lt;/p&gt;

&lt;p&gt;There’s a few different kinds of stream groupings.&lt;/p&gt;

&lt;p&gt;The simplest kind of grouping is called a “shuffle grouping” which sends the tuple to a random task. A shuffle grouping is used in the &lt;code&gt;WordCountTopology&lt;/code&gt; to send tuples from RandomSentenceSpout to the SplitSentence bolt. It has the effect of evenly distributing the work of processing the tuples across all of SplitSentence bolt’s tasks.
（shuffle grouping，随机分发tuple到所有的bolt）&lt;/p&gt;

&lt;p&gt;A more interesting kind of grouping is the “fields grouping”. A fields grouping is used between the SplitSentence bolt and the WordCount bolt. It is critical for the functioning of the WordCount bolt that the same word always go to the same task. Otherwise, more than one task will see the same word, and they’ll each emit incorrect values for the count since each has incomplete information. A fields grouping lets you group a stream by a subset of its fields. This causes equal values for that subset of fields to go to the same task. Since &lt;code&gt;WordCount&lt;/code&gt; subscribes to SplitSentence’s output stream using a fields grouping on the “word” field, the same word always goes to the same task and the bolt produces the correct output.
（fields grouping，根据给定的fields进行group操作）&lt;/p&gt;

&lt;p&gt;Fields groupings are the basis of implementing streaming joins and streaming aggregations as well as a plethora of other use cases. Underneath the hood, fields groupings are implemented using mod hashing.
（fields grouping，应用很广泛，本质上，其使用mod hashing方式实现）&lt;/p&gt;

&lt;p&gt;There’s a few other kinds of stream groupings. You can read more about them on &lt;a href=&quot;http://storm.apache.org/documentation/Concepts.html&quot;&gt;Concepts&lt;/a&gt;.
（关于stream grouping的更多内容，参考&lt;a href=&quot;http://storm.apache.org/documentation/Concepts.html&quot;&gt;Concepts&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;defining-bolts-in-other-languages&quot;&gt;Defining Bolts in other languages&lt;/h2&gt;

&lt;p&gt;Bolts can be defined in any language. Bolts written in another language are executed as subprocesses, and Storm communicates with those subprocesses with JSON messages over stdin/stdout. The communication protocol just requires an ~100 line adapter library, and Storm ships with adapter libraries for Ruby, Python, and Fancy.
（bolt可以使用其他语言实现，本质上其他语言编写的bolt都是subprocess，Storm通过stdin/stdout上的JSON串来与其进行通讯）&lt;/p&gt;

&lt;p&gt;Here’s the definition of the SplitSentence bolt from WordCountTopology:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// java
public static class SplitSentence extends ShellBolt implements IRichBolt { 
   public SplitSentence() { 
       super(“python”, “splitsentence.py”); 
   }
   
   public void declareOutputFields(OutputFieldsDeclarer declarer) {
	   declarer.declare(new Fields(&quot;word&quot;));
   } 
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SplitSentence overrides ShellBolt and declares it as running using python with the arguments splitsentence.py. Here’s the implementation of splitsentence.py:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// python 
import storm

class SplitSentenceBolt(storm.BasicBolt): 
	def process(self, tup): 
		words = tup.values[0].split(“ “) 
		for word in words: 
			storm.emit([word])

SplitSentenceBolt().run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For more information on writing spouts and bolts in other languages, and to learn about how to create topologies in other languages (and avoid the JVM completely), see &lt;a href=&quot;http://storm.apache.org/documentation/Using-non-JVM-languages-with-Storm.html&quot;&gt;Using non-JVM languages with Storm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;guaranteeing-message-processing&quot;&gt;Guaranteeing message processing&lt;/h2&gt;

&lt;p&gt;Earlier on in this tutorial, we skipped over a few aspects of how tuples are emitted. Those aspects were part of Storm’s reliability API: how Storm guarantees that every message coming off a spout will be fully processed. See &lt;a href=&quot;http://storm.apache.org/documentation/Guaranteeing-message-processing.html&quot;&gt;Guaranteeing message processing&lt;/a&gt; for information on how this works and what you have to do as a user to take advantage of Storm’s reliability capabilities.
（本文中，并没有深入介绍how tuples are emitted，其中涉及一个关键问题：如何保证spout提供的message一定会被成功处理，详细信息可参考&lt;a href=&quot;http://storm.apache.org/documentation/Guaranteeing-message-processing.html&quot;&gt;Guaranteeing message processing&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;transactional-topologies&quot;&gt;Transactional topologies&lt;/h2&gt;

&lt;p&gt;Storm guarantees that every message will be played through the topology at least once. A common question asked is “how do you do things like counting on top of Storm? Won’t you overcount?” Storm has a feature called transactional topologies that let you achieve exactly-once messaging semantics for most computations. Read more about transactional topologies &lt;a href=&quot;http://storm.apache.org/documentation/Transactional-topologies.html&quot;&gt;here&lt;/a&gt;.
（如何保证message只被处理一次，而不被重复处理？Storm提供了transactional topologies来保证只执行一次）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message到底是什么？tuple？task？&lt;/p&gt;

&lt;h2 id=&quot;distributed-rpc&quot;&gt;Distributed RPC&lt;/h2&gt;

&lt;p&gt;This tutorial showed how to do basic stream processing on top of Storm. There’s lots more things you can do with Storm’s primitives. One of the most interesting applications of Storm is Distributed RPC, where you parallelize the computation of intense functions on the fly. Read more about Distributed RPC &lt;a href=&quot;http://storm.apache.org/documentation/Distributed-RPC.html&quot;&gt;here&lt;/a&gt;.
（Distribute RPC很有意思的，能够parallelize the computation of intense functions on the fly。）&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This tutorial gave a broad overview of developing, testing, and deploying Storm topologies. The rest of the documentation dives deeper into all the aspects of using Storm.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Tutorial.html&quot;&gt;Storm Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Storm：Rationale</title>
     <link href="http://ningg.github.com/storm-documentation-rationale"/>
     <updated>2014-10-20T00:00:00+08:00</updated>
     <id>http://ningg.github.com/storm-documentation-rationale</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;原文地址：&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Storm Rationale&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The past decade has seen a revolution in data processing. MapReduce, Hadoop, and related technologies have made it possible to store and process data at scales previously unthinkable. Unfortunately, these data processing technologies are not realtime systems, nor are they meant to be. There’s no hack that will turn Hadoop into a realtime system; realtime data processing has a fundamentally different set of requirements than batch processing.
（Hadoop的相关技术已经很多，大规模数据处理方面很强，但realtime processing却不行）&lt;/p&gt;

&lt;p&gt;However, realtime data processing at massive scale is becoming more and more of a requirement for businesses. The lack of a “Hadoop of realtime” has become the biggest hole in the data processing ecosystem.&lt;/p&gt;

&lt;p&gt;Storm fills that hole.
（Storm将解决大规模数据的实时处理问题。）&lt;/p&gt;

&lt;p&gt;Before Storm, you would typically have to manually build a network of queues and workers to do realtime processing. Workers would process messages off a queue, update databases, and send new messages to other queues for further processing. Unfortunately, this approach has serious limitations:
（在Storm之前，需要手动创建queues和workers，其中，worker从queue中取出message，并进行处理；不幸呐，这种方式有很严重的限制）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Tedious&lt;/strong&gt;: You spend most of your development time configuring where to send messages, deploying workers, and deploying intermediate queues. The realtime processing logic that you care about corresponds to a relatively small percentage of your codebase.（重复劳动：花费大量时间来配置和部署）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Brittle&lt;/strong&gt;: There’s little fault-tolerance. You’re responsible for keeping each worker and queue up.（系统很脆弱：需要不停地检测并保证worker、queue都是存活的）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Painful to scale&lt;/strong&gt;: When the message throughput get too high for a single worker or queue, you need to partition how the data is spread around. You need to reconfigure the other workers to know the new locations to send messages. This introduces moving parts and new pieces that can fail.（扩展困难：流量上升后，添加worker、queue，需要重新进行处理逻辑配置，并且由于结构变得复杂，系统可靠性会降低）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although the queues and workers paradigm breaks down for large numbers of messages, message processing is clearly the fundamental paradigm for realtime computation. The question is: how do you do it in a way that doesn’t lose data, scales to huge volumes of messages, and is dead-simple to use and operate?
（&lt;strong&gt;问题是：能否实现一个方案，满足：不丢数据、易于扩展、使用和操作极其简单？&lt;/strong&gt;）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：paradigm? break down? fundamental paradigm?什么含义？&lt;/p&gt;

&lt;p&gt;Storm satisfies these goals.
（Storm就是这么一个方案）&lt;/p&gt;

&lt;h2 id=&quot;why-storm-is-important&quot;&gt;Why Storm is important&lt;/h2&gt;

&lt;p&gt;Storm exposes a set of primitives for doing realtime computation. Like how MapReduce greatly eases the writing of parallel batch processing, Storm’s primitives greatly ease the writing of parallel realtime computation.
（Storm暴漏了a set of primitives/原语，来进行realtime computation。就像MapReduce极大改善了parallel batch processing一样，Storm改善了parallel realtime computation）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：primitives？对于一个software，primitives什么含义？&lt;/p&gt;

&lt;p&gt;The key properties of Storm are:
（Storm的关键特性如下）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Extremely broad set of use cases&lt;/strong&gt;: Storm can be used for processing messages and updating databases (stream processing), doing a continuous query on data streams and streaming the results into clients (continuous computation), parallelizing an intense query like a search query on the fly (distributed RPC), and more. Storm’s small set of primitives satisfy a stunning number of use cases.（广泛的应用场景）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Storm scales to massive numbers of messages per second. To scale a topology, all you have to do is add machines and increase the parallelism settings of the topology. As an example of Storm’s scale, one of Storm’s initial applications processed 1,000,000 messages per second on a 10 node cluster, including hundreds of database calls per second as part of the topology. Storm’s usage of Zookeeper for cluster coordination makes it scale to much larger cluster sizes.（易于扩展：增加机器，配置并发数；备注：这跟使用zookeeper进行cluster管理相关。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Guarantees no data loss&lt;/strong&gt;: A realtime system must have strong guarantees about data being successfully processed. A system that drops data has a very limited set of use cases. Storm guarantees that every message will be processed, and this is in direct contrast with other systems like S4.（数据不丢失：Storm保证every message will be prcessed，而S4不行）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extremely robust&lt;/strong&gt;: Unlike systems like Hadoop, which are notorious for being difficult to manage, Storm clusters just work. It is an explicit goal of the Storm project to make the user experience of managing Storm clusters as painless as possible.（极其健壮：Storm Cluster非常易于管理，这也是其设计的初衷。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault-tolerant&lt;/strong&gt;: If there are faults during execution of your computation, Storm will reassign tasks as necessary. Storm makes sure that a computation can run forever (or until you kill the computation).（容错性：出错后，storm会reassign task。）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Programming language agnostic&lt;/strong&gt;: Robust and scalable realtime processing shouldn’t be limited to a single platform. Storm topologies and processing components can be defined in any language, making Storm accessible to nearly anyone.（语言无关性：Storm topologies、processing components可使用多种语言实现）&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;Apache Storm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/documentation/Rationale.html&quot;&gt;Apache Storm: Documentation Rationale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1 Documentation：Design</title>
     <link href="http://ningg.github.com/kafka-documentation-design"/>
     <updated>2014-10-18T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-documentation-design</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1 Documentation：Getting Started</title>
     <link href="http://ningg.github.com/kafka-documentation"/>
     <updated>2014-10-18T00:00:00+08:00</updated>
     <id>http://ningg.github.com/kafka-documentation</id>
     <content type="html">&lt;h2 id=&quot;getting-started&quot;&gt;1. Getting Started&lt;/h2&gt;

&lt;h3 id=&quot;introduction&quot;&gt;1.1 Introduction&lt;/h3&gt;

&lt;p&gt;Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：distributed, partitioned, replicated commit log service?&lt;/p&gt;

&lt;p&gt;First let’s review some basic messaging terminology:（几个messaging概念）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka maintains feeds of messages in categories called topics.（按topic来分类message？）&lt;/li&gt;
  &lt;li&gt;We’ll call processes that publish messages to a Kafka topic producers.（调用process，向topic producer中写message）&lt;/li&gt;
  &lt;li&gt;We’ll call processes that subscribe to topics and process the feed of published messages consumers..&lt;/li&gt;
  &lt;li&gt;Kafka is run as a cluster comprised of one or more servers each of which is called a broker.（kafka集群由borker构成）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:（producer向kafka集群写入message，consumer从kafka集群中读取message）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/producer_consumer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Communication between the clients and the servers is done with a simple, high-performance, language agnostic &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol&quot;&gt;TCP protocol&lt;/a&gt;. We provide a Java client for Kafka, but clients are available in &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Clients&quot;&gt;many languages&lt;/a&gt;.（client与server之间通过TCP协议通信，默认为kafka提供了java client，当然也可以用其他语言实现client）&lt;/p&gt;

&lt;h4 id=&quot;topics-and-logs&quot;&gt;Topics and Logs&lt;/h4&gt;

&lt;p&gt;A topic is a category or feed name to which messages are published. For each topic, the Kafka cluster maintains a partitioned log that looks like this:（topic，就是category、feed name，message按此分开存放；每个topic，对应一个partitioned log）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/log_anatomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each partition is an ordered, immutable sequence of messages that is continually appended to—a commit log. The messages in the partitions are each assigned a sequential id number called the offset that uniquely identifies each message within the partition.（partition是ordered、immutable sequence of message，其中的message被唯一标识，partition对应 a commit log）&lt;/p&gt;

&lt;p&gt;The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. For example if the log retention is set to two days, then for the two days after a message is published it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so retaining lots of data is not a problem.（在一段可配置的时间内，kafka始终保存所有的published messages，即使message已经被consume；Kafka对data size不敏感，lots of data对performance造成太大影响）&lt;/p&gt;

&lt;p&gt;In fact the only metadata retained on a per-consumer basis is the position of the consumer in the log, called the “offset”. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages, but in fact the position is controlled by the consumer and it can consume messages in any order it likes. For example a consumer can reset to an older offset to reprocess.
（on a per-consumer basis，只需保存元数据：consumer在log中的position，即，offset；这个offset完全由consumer自己决定，offset默认是顺序递增，但实际上consumer可以任意调整。）&lt;/p&gt;

&lt;p&gt;This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers.（总之，consumer在kafka中非常cheap：随意的come and go，对系统影响很小，consumer相互之间的影响也很小）&lt;/p&gt;

&lt;p&gt;The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit.（对log分partition，有几点目的：1.single server支撑较大的log，单个partition受到server的限制，但partition的数量不受限；2.多partition可以支撑并发处理，每个partition作为一个unit。）&lt;/p&gt;

&lt;h4 id=&quot;distribution&quot;&gt;Distribution&lt;/h4&gt;

&lt;p&gt;The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.
（partition分布式存储，方便共享；同时可配置每个patition的复制份数，以提升系统可靠性）&lt;/p&gt;

&lt;p&gt;Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.
（每个partition都对应一个server担当”leader”角色，也可能有其他server担当”follower”角色；leader负责所有的Read、write；follower只replicate the leader；如果leader崩溃，则自动推选一个follower升级为leader；server只对其上的部分partition充当leader角色，方便cluster的均衡。）&lt;/p&gt;

&lt;h4 id=&quot;producers&quot;&gt;Producers&lt;/h4&gt;

&lt;p&gt;Producers publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the message). More on the use of partitioning in a second.
（producer复制将message分发到相应的topic，具体：1.将message分发到哪个topic的哪个partition，常用方式，轮询、函数；）&lt;/p&gt;

&lt;h4 id=&quot;consumers&quot;&gt;Consumers&lt;/h4&gt;

&lt;p&gt;Messaging traditionally has two models: &lt;a href=&quot;http://en.wikipedia.org/wiki/Message_queue&quot;&gt;queuing&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern&quot;&gt;publish-subscribe&lt;/a&gt;. In a queue, a pool of consumers may read from a server and each message goes to one of them; in publish-subscribe the message is broadcast to all consumers. Kafka offers a single consumer abstraction that generalizes both of these—the consumer group.
（messaging，消息发送，由两种方式：queuing、publish-subscribe。Queuing，message发送到某一个consumer；publish-subscribe，message广播到所有的consumers。Kafka，通过将consumer泛化为consumer group，来支持这两种方式）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：publish-subscribe，发布-订阅模式的含义？&lt;/p&gt;

&lt;p&gt;Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.
（每个consumer都标记有consumer group name，每个message都被分发给subscribing consumer group中的一个consumer instance，consumer instances可以是不同的进程，也可以分布在不同的物理机器上。）&lt;/p&gt;

&lt;p&gt;If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.（若所有的consumer都属于同一个consumer group，则，情况变为：queue的负载均衡？）&lt;/p&gt;

&lt;p&gt;If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.
（若所有的consumer都属不同的consumer group，则，情况变为：publish-subscribe，message广播发送到所有consumer）&lt;/p&gt;

&lt;p&gt;More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is cluster of consumers instead of a single process.
（topics只对应少数的consumer group，即，consumer group类似&lt;code&gt;logical subscriber&lt;/code&gt;；每个group中有多个consumer，目的是提升可扩展性、容错能力）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：consumer group下有多个consumer？这些consumer怎么调用的？相互之间有什么差异？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-documentation/consumer-groups.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kafka has stronger ordering guarantees than a traditional messaging system, too.（kafka有strong ordering guarantees）&lt;/p&gt;

&lt;p&gt;A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored. However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the messages is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.
（message queue中存放的message，按照顺序发送到不同的consumers，但是这些发送是异步的，因此，后发送的message可能先到达consumer，即，并行处理时，有可能message乱序。现有的Messaging system，常用&lt;code&gt;exclusive consumer&lt;/code&gt;，独占消费，仅仅启动一个process来读取一个queue中的数据，此时，就无法实现并行处理。）&lt;/p&gt;

&lt;p&gt;Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances than partitions.
（Kafka，采用&lt;code&gt;partition&lt;/code&gt;的方式解决上述问题：每个partition被指定给topic对应的consumer group中的特定的consumer，这样能保证一点：一个partition中的message被顺序处理。由于有多个partition，并且对应多个consumer instance来处理，从而实现负载均衡；特别注意：consumer instance个数不能多于partitions个数）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：message是怎么分配到topic对应的partition中的？consumer instance为什么不能多于partition个数？&lt;/p&gt;

&lt;p&gt;Kafka only provides a total order over messages within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process.
（Kafka只保证partition内mesaage的顺序处理，不保证partition之间的处理顺序。per-partition ordering和partition data by key，满足了大部分需求。如果要保证所有message顺序处理，则，将topic设置为only one partition，此时，变为串行处理。）&lt;/p&gt;

&lt;h4 id=&quot;guarantees&quot;&gt;Guarantees&lt;/h4&gt;

&lt;p&gt;At a high-level Kafka gives the following guarantees:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.（同一个producer发送到a particular topic partition的message，保证在partition中是有序的）&lt;/li&gt;
  &lt;li&gt;A consumer instance sees messages in the order they are stored in the log.（partition对应的commit log中message是有序的）&lt;/li&gt;
  &lt;li&gt;For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.（复制N份的topic，保证N-1份都丢失的情况下能够恢复。）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More details on these guarantees are given in the design section of the documentation.&lt;/p&gt;

&lt;h3 id=&quot;use-cases&quot;&gt;1.2 Use Cases&lt;/h3&gt;

&lt;p&gt;Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see &lt;a href=&quot;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;this blog post&lt;/a&gt;.
（使用Kafka的典型场景，详细应用参考&lt;a href=&quot;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;this blog post&lt;/a&gt;）&lt;/p&gt;

&lt;h4 id=&quot;messaging&quot;&gt;Messaging&lt;/h4&gt;

&lt;p&gt;Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.
（替换传统的message broker/消息代理，其基本用途：解耦processing和data producer，缓存message，etc。）&lt;/p&gt;

&lt;p&gt;In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.
（实验发现messaging过程中，对broker的吞吐量要求不高，但要求低延迟、高可靠，这些kafka都满足。）&lt;/p&gt;

&lt;p&gt;In this domain Kafka is comparable to traditional messaging systems such as &lt;a href=&quot;http://activemq.apache.org/&quot;&gt;ActiveMQ&lt;/a&gt; or &lt;a href=&quot;https://www.rabbitmq.com/&quot;&gt;RabbitMQ&lt;/a&gt;.
（在messaging方面，Kafka的性能可与ActiveMQ、RabbitMQ相匹敌。）&lt;/p&gt;

&lt;h4 id=&quot;website-activity-tracking&quot;&gt;Website Activity Tracking&lt;/h4&gt;

&lt;p&gt;The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.&lt;/p&gt;

&lt;p&gt;Activity tracking is often very high volume as many activity messages are generated for each user page view.
（活动追踪，数据流量很大）&lt;/p&gt;

&lt;h4 id=&quot;metrics&quot;&gt;Metrics&lt;/h4&gt;

&lt;p&gt;Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.
（运行状态监控系统，从分布式应用中，汇总统计数据，形成集中的运行监控数据）&lt;/p&gt;

&lt;h4 id=&quot;log-aggregation&quot;&gt;Log Aggregation&lt;/h4&gt;

&lt;p&gt;Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.
（收集不同物理机器上的log，汇总到a central place：a file server or HDFS。与 Scribe or Flume相比，Kafka提供相当的performance、可靠性、低延迟。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：日志收集方面，Kafka的性能与Flume相当？Kafka能取代掉Flume吗？&lt;/p&gt;

&lt;h4 id=&quot;stream-processing&quot;&gt;Stream Processing&lt;/h4&gt;

&lt;p&gt;Many users end up doing stage-wise processing of data where data is consumed from topics of raw data and then aggregated, enriched, or otherwise transformed into new Kafka topics for further consumption. For example a processing flow for article recommendation might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might help normalize or deduplicate this content to a topic of cleaned article content; a final stage might attempt to match this content to users. This creates a graph of real-time data flow out of the individual topics. &lt;a href=&quot;https://github.com/nathanmarz/storm&quot;&gt;Storm&lt;/a&gt; and &lt;a href=&quot;http://samza.incubator.apache.org/&quot;&gt;Samza&lt;/a&gt; are popular frameworks for implementing these kinds of transformations.
（在Stream Processing中，Kafka担当data存储功能，即，raw data存储到Kafka中，consumer处理后的结果存储到new kafka topics中）&lt;/p&gt;

&lt;h4 id=&quot;event-sourcing&quot;&gt;Event Sourcing&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/eaaDev/EventSourcing.html&quot;&gt;Event sourcing&lt;/a&gt; is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka’s support for very large stored log data makes it an excellent backend for an application built in this style.
（Event sourcing，事件溯源，记录不同时间点的应用状态变化，通常log数据很大，Kafka满足此需求）&lt;/p&gt;

&lt;h4 id=&quot;commit-log&quot;&gt;Commit Log&lt;/h4&gt;

&lt;p&gt;Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The &lt;a href=&quot;http://kafka.apache.org/documentation.html#compaction&quot;&gt;log compaction&lt;/a&gt; feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.&lt;/p&gt;

&lt;h3 id=&quot;quick-start&quot;&gt;1.3 Quick Start&lt;/h3&gt;

&lt;p&gt;This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
（新手入门，对Kafka、Zookeeper一知半解的人，看这儿就对了）&lt;/p&gt;

&lt;h4 id=&quot;step-1-download-the-code&quot;&gt;Step 1: Download the code&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz&quot;&gt;Download&lt;/a&gt; the 0.8.1.1 release and un-tar it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; tar -xzf kafka_2.9.2-0.8.1.1.tgz
&amp;gt; cd kafka_2.9.2-0.8.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-2-start-the-server&quot;&gt;Step 2: Start the server&lt;/h4&gt;

&lt;p&gt;Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.
（kafka自带了ZooKeeper，不推荐使用）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/zookeeper-server-start.sh config/zookeeper.properties
[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now start the Kafka server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-server-start.sh config/server.properties
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-3-create-a-topic&quot;&gt;Step 3: Create a topic&lt;/h4&gt;

&lt;p&gt;Let’s create a topic named “test” with a single partition and only one replica:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see that topic if we run the list topic command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --list --zookeeper localhost:2181
test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.
（可通过配置文件，让broker自动创建topic）&lt;/p&gt;

&lt;h4 id=&quot;step-4-send-some-messages&quot;&gt;Step 4: Send some messages&lt;/h4&gt;

&lt;p&gt;Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.
（kafka自带了一个工具，能够将file或者standard input作为输入，按行传送到kafka cluster中。）&lt;/p&gt;

&lt;p&gt;Run the producer and then type a few messages into the console to send to the server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
This is a message
This is another message
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;step-5-start-a-consumer&quot;&gt;Step 5: Start a consumer&lt;/h4&gt;

&lt;p&gt;Kafka also has a command line consumer that will dump out messages to standard output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
This is a message
This is another message
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.&lt;/p&gt;

&lt;p&gt;All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.
（所有命令行，不夹带参数启动时，会自动弹出usage info）&lt;/p&gt;

&lt;h4 id=&quot;step-6-setting-up-a-multi-broker-cluster&quot;&gt;Step 6: Setting up a multi-broker cluster&lt;/h4&gt;

&lt;p&gt;So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).
（上述例子中，只启动了一个broker，其最多能够启动几个broker instances。下面说一下如何启动多个broker，构造cluster）&lt;/p&gt;

&lt;p&gt;First we make a config file for each of the brokers:（为每个broker，设定属性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; cp config/server.properties config/server-1.properties 
&amp;gt; cp config/server.properties config/server-2.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now edit these new files and set the following properties:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config/server-1.properties:
	broker.id=1
	port=9093
	log.dir=/tmp/kafka-logs-1
 
config/server-2.properties:
	broker.id=2
	port=9094
	log.dir=/tmp/kafka-logs-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;broker.id&lt;/code&gt; property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.
（不同的broker，应该设置不同的&lt;code&gt;port&lt;/code&gt;和&lt;code&gt;log.dir&lt;/code&gt;，否则，broker的数据会相互覆盖。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：同一台物理主机上，可以启动多个node，每个node通过&lt;code&gt;broker.id&lt;/code&gt;唯一标识。&lt;/p&gt;

&lt;p&gt;We already have Zookeeper and our single node started, so we just need to start the two new nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-server-start.sh config/server-1.properties &amp;amp;
...
&amp;gt; bin/kafka-server-start.sh config/server-2.properties &amp;amp;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a new topic with a replication factor of three:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay but now that we have a cluster how can we know which broker is doing what? To see that run the &lt;code&gt;describe topics&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.
（&lt;code&gt;describe topics&lt;/code&gt;命令的输出结果说明：first line，partition的汇总信息；remaining lines 分别说明每个partition的详细信息）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;leader&lt;/strong&gt; is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;replicas&lt;/strong&gt; is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive. （备份当前partition的node列表，包含当前已经不再存活的node）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;isr&lt;/strong&gt; is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.（&lt;code&gt;replicas&lt;/code&gt;内的node中，存活的node列表）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：&lt;code&gt;leader&lt;/code&gt;后的数字&lt;code&gt;1&lt;/code&gt;，对应的含义？leader是怎么标识的？node怎么标识的？&lt;/p&gt;

&lt;p&gt;Note that in my example node 1 is the leader for the only partition of the topic.
We can run the same command on the original topic we created to see where it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.&lt;/p&gt;

&lt;p&gt;Let’s publish a few messages to our new topic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
...
my test message 1
my test message 2
^C 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s consume these messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:（验证kafka的容错性：kill leader）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ps | grep server-1.properties
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java...
&amp;gt; kill -9 7564
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:（leader终止后，slave自动升级为leader）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But the messages are still be available for consumption even though the leader that took the writes originally is down:（新选出的leader，对用户是透明的，consumer感觉不到异常）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ecosystem&quot;&gt;1.4 Ecosystem&lt;/h3&gt;

&lt;p&gt;There are a plethora of tools that integrate with Kafka outside the main distribution. The &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;ecosystem page&lt;/a&gt; lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.
（有很多工具与Kafka集成，参考&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;页面&lt;/a&gt;）&lt;/p&gt;

&lt;h3 id=&quot;upgrading-from-previous-versions&quot;&gt;1.5 Upgrading From Previous Versions&lt;/h3&gt;

&lt;h4 id=&quot;upgrading-from-080-to-081&quot;&gt;Upgrading from 0.8.0 to 0.8.1&lt;/h4&gt;

&lt;p&gt;0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.&lt;/p&gt;

&lt;h4 id=&quot;upgrading-from-07&quot;&gt;Upgrading from 0.7&lt;/h4&gt;

&lt;p&gt;0.8, the release in which added replication, was our first backwards-incompatible release: major changes were made to the API, ZooKeeper data structures, and protocol, and configuration. The upgrade from 0.7 to 0.8.x requires a &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8&quot;&gt;special tool&lt;/a&gt; for migration. This migration can be done without downtime.&lt;/p&gt;

&lt;h2 id=&quot;api&quot;&gt;2. API&lt;/h2&gt;

&lt;h3 id=&quot;producer-api&quot;&gt;2.1 Producer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;high-level-consumer-api&quot;&gt;2.2 High Level Consumer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;simple-consumer-api&quot;&gt;2.3 Simple Consumer API&lt;/h3&gt;

&lt;p&gt;(todo)&lt;/p&gt;

&lt;h3 id=&quot;kafka-hadoop-consumer-api&quot;&gt;2.4 Kafka Hadoop Consumer API&lt;/h3&gt;

&lt;p&gt;Providing a horizontally scalable solution for aggregating and loading data into Hadoop was one of our basic use cases. To support this use case, we provide a Hadoop-based consumer which spawns off many map tasks to pull data from the Kafka cluster in parallel. This provides extremely fast pull-based Hadoop data load capabilities (we were able to fully saturate the network with only a handful of Kafka servers).
（Hadoop-based consumer，并行的从Kafka cluster中pull data，速度很快）&lt;/p&gt;

&lt;p&gt;Usage information on the hadoop consumer can be found &lt;a href=&quot;https://github.com/linkedin/camus/tree/camus-kafka-0.8/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;3. Configuration&lt;/h2&gt;

&lt;p&gt;Kafka uses key-value pairs in the &lt;a href=&quot;http://en.wikipedia.org/wiki/.properties&quot;&gt;property file format&lt;/a&gt; for configuration. These values can be supplied either from a file or programmatically.&lt;/p&gt;

&lt;h3 id=&quot;broker-configs&quot;&gt;3.1 Broker Configs&lt;/h3&gt;

&lt;p&gt;The essential configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;broker.id&lt;/li&gt;
  &lt;li&gt;log.dirs&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#brokerconfigs&quot;&gt;Broker Configs&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;consumer-configs&quot;&gt;3.2 Consumer Configs&lt;/h3&gt;

&lt;p&gt;The essential consumer configurations are the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;group.id&lt;/li&gt;
  &lt;li&gt;zookeeper.connect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#consumerconfigs&quot;&gt;Consumer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;producer-configs&quot;&gt;3.3 Producer Configs&lt;/h3&gt;

&lt;p&gt;Essential configuration properties for the producer include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;metadata.broker.list&lt;/li&gt;
  &lt;li&gt;request.required.acks&lt;/li&gt;
  &lt;li&gt;producer.type&lt;/li&gt;
  &lt;li&gt;serializer.class&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#producerconfigs&quot;&gt;Producer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;new-producer-configs&quot;&gt;3.4 New Producer Configs&lt;/h3&gt;

&lt;p&gt;We are working on a replacement for our existing producer. The code is available in trunk now and can be considered beta quality. Below is the configuration for the new producer.&lt;/p&gt;

&lt;p&gt;详细信息，请参考官网：&lt;a href=&quot;http://kafka.apache.org/documentation.html#newproducerconfigs&quot;&gt;New Producer Configs&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Flume 1.5.0.1 User Guide：Flume Sources</title>
     <link href="http://ningg.github.com/flume-user-guide-source"/>
     <updated>2014-10-17T00:00:00+08:00</updated>
     <id>http://ningg.github.com/flume-user-guide-source</id>
     <content type="html">&lt;h2 id=&quot;avro-source&quot;&gt;Avro Source&lt;/h2&gt;

&lt;p&gt;Listens on Avro port and receives events from external Avro client streams. When paired with the built-in Avro Sink on another (previous hop) Flume agent, it can create tiered collection topologies. Required properties are in bold.（必须属性加黑了）&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be &lt;code&gt;avro&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;bind&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;hostname or IP address to listen on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;port&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Port # to bind to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;threads&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Maximum number of worker threads to spawn&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space-separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;compression-type&lt;/td&gt;
      &lt;td&gt;none&lt;/td&gt;
      &lt;td&gt;This can be “none” or “deflate”. The compression-type must match the compression-type of matching AvroSource&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ssl&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Set this to true to enable SSL encryption. You must also specify a “keystore” and a “keystore-password”.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;This is the path to a Java keystore file. Required for SSL.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore-password&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The password for the Java keystore. Required for SSL.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;keystore-type&lt;/td&gt;
      &lt;td&gt;JKS	The type of the Java keystore. This can be “JKS” or “PKCS12”.&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ipFilter&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Set this to true to enable ipFiltering for netty&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ipFilter.rules&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Define N netty ipFilter pattern rules with this config.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example of ipFilter.rules&lt;/p&gt;

&lt;p&gt;ipFilter.rules defines N netty ipFilters separated by a comma(&lt;code&gt;,&lt;/code&gt;) a pattern rule must be in this format.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;`allow` or `deny`&amp;gt;:&amp;lt;`ip` or `name` for computer name&amp;gt;:&amp;lt;`pattern`&amp;gt; 
allow/deny:ip/name:pattern

# example
ipFilter.rules=allow:ip:127.*,allow:name:localhost,deny:ip:*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the first rule to match will apply as the example below shows from a client on the localhost（从左向右，第一个匹配出的rules生效）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# This will Allow the client on localhost be deny clients from any other ip 
ipFilter.rules = allow:name:localhost,deny:ip:

# This will deny the client on localhost be allow clients from any other ip 
ipFilter.rules = deny:name:localhost,allow:ip:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;thrift-source&quot;&gt;Thrift Source&lt;/h2&gt;

&lt;p&gt;Listens on Thrift port and receives events from external Thrift client streams. When paired with the built-in ThriftSink on another (previous hop) Flume agent, it can create tiered collection topologies. Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be thrift&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;bind&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;hostname or IP address to listen on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;port&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Port # to bind to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;threads&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Maximum number of worker threads to spawn&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = thrift
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;exec-source&quot;&gt;Exec Source&lt;/h2&gt;

&lt;p&gt;Exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as &lt;code&gt;cat [named pipe]&lt;/code&gt; or &lt;code&gt;tail -F [file]&lt;/code&gt; are going to produce the desired results where as &lt;code&gt;date&lt;/code&gt; will probably not - the former two commands produce streams of data where as the latter produces a single event and exits.（捕获命令的输出，并按行处理，当&lt;code&gt;logStdErr&lt;/code&gt;设为true时，也将捕获stderr的输出；）&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be exec&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;command&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The command to execute&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shell&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;A shell invocation used to run the command. e.g. /bin/sh -c. Required only for commands relying on shell features like wildcards, back ticks, pipes etc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restartThrottle&lt;/td&gt;
      &lt;td&gt;10000&lt;/td&gt;
      &lt;td&gt;Amount of time (in millis) to wait before attempting a restart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;restart&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Whether the executed cmd should be restarted if it dies&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;logStdErr&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt;Whether the command’s stderr should be logged&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batchSize&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;The max number of lines to read and send to the channel at a time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.type&lt;/td&gt;
      &lt;td&gt;replicating&lt;/td&gt;
      &lt;td&gt;replicating or multiplexing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;selector.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Depends on the selector.type value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Space-separated list of interceptors&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interceptors.*&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;： The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost. As a for instance, one of the most commonly requested features is the &lt;code&gt;tail -F [file]&lt;/code&gt;-like use case where an application writes to a log file on disk and Flume tails the file, sending each line as an event. While this is possible, there’s an obvious problem; what happens if the channel fills up and Flume can’t send an event? Flume has no way of indicating to the application writing the log file that it needs to retain the log or that the event hasn’t been sent, for some reason. If this doesn’t make sense, you need only know this: Your application can never guarantee data has been received when using a unidirectional asynchronous interface such as ExecSource! As an extension of this warning - and to be completely clear - there is absolutely zero guarantee of event delivery when using this source. For stronger reliability guarantees, consider the Spooling Directory Source or direct integration with Flume via the SDK.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：ExecSource方式，当command异常退出后，会丢失数据。解决办法：考虑Spooling Directory Source或者通过SDK直接与Flume集成。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;：You can use ExecSource to emulate TailSource from Flume 0.9x (flume og). Just use unix command tail -F /full/path/to/your/file. Parameter -F is better in this case than -f as it will also follow file rotation.（Flume 0.9x版本中，可以使用 &lt;code&gt;tail -F path&lt;/code&gt;命令模仿 TailSource）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = exec
# follow file rotation
a1.sources.r1.command = tail -F /var/log/secure
a1.sources.r1.channels = c1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ‘shell’ config is used to invoke the ‘command’ through a command shell (such as Bash or Powershell). The ‘command’ is passed as an argument to ‘shell’ for execution. This allows the ‘command’ to use features from the shell such as wildcards, back ticks, pipes, loops, conditionals etc. In the absence of the ‘shell’ config, the ‘command’ will be invoked directly. Common values for ‘shell’ : ‘/bin/sh -c’, ‘/bin/ksh -c’, ‘cmd /c’, ‘powershell -Command’, etc.（启用shell选项时，系统会将command当作参数，传送给shell执行，此时，能利用不同shell的特性）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;agent_foo.sources.tailsource-1.type = exec
agent_foo.sources.tailsource-1.shell = /bin/bash -c
agent_foo.sources.tailsource-1.command = for i in /path/*.txt; do cat $i; done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;jms-source&quot;&gt;JMS Source&lt;/h2&gt;

&lt;p&gt;JMS Source reads messages from a JMS destination such as a queue or topic. Being a JMS application it should work with any JMS provider but has only been tested with ActiveMQ. The JMS source provides configurable batch size, message selector, user/pass, and message to flume event converter. Note that the vendor provided JMS jars should be included in the Flume classpath using plugins.d directory (preferred), –classpath on command line, or via FLUME_CLASSPATH variable in flume-env.sh.&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Property Name&lt;/td&gt;
      &lt;td&gt;Default&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;channels&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The component type name, needs to be jms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;initialContextFactory&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Inital Context Factory, e.g: org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;connectionFactory&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The JNDI name the connection factory shoulld appear as&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;providerURL&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;The JMS provider URL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;destinationName&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Destination name&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;destinationType&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Destination type (queue or topic)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;messageSelector&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Message selector to use when creating the consumer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;userName&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Username for the destination/provider&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;passwordFile&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;File containing the password for the destination/provider&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batchSize&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;Number of messages to consume in one batch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.type&lt;/td&gt;
      &lt;td&gt;DEFAULT&lt;/td&gt;
      &lt;td&gt;Class to use to convert messages to flume events. See below.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.*&lt;/td&gt;
      &lt;td&gt;–&lt;/td&gt;
      &lt;td&gt;Converter properties.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;converter.charset&lt;/td&gt;
      &lt;td&gt;UTF-8&lt;/td&gt;
      &lt;td&gt;Default converter only. Charset to use when converting JMS TextMessages to byte arrays.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)&lt;/strong&gt;：JMS是java方面的消息队列？做几年java了，对这个我还不清楚~~&lt;/p&gt;

&lt;h3 id=&quot;converter&quot;&gt;Converter&lt;/h3&gt;

&lt;p&gt;The JMS source allows pluggable converters, though it’s likely the default converter will work for most purposes. The default converter is able to convert Bytes, Text, and Object messages to FlumeEvents. In all cases, the properties in the message are added as headers to the FlumeEvent.（默认，message的properties会转换为FlumeEvent的headers）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BytesMessage: Bytes of message are copied to body of the FlumeEvent. Cannot convert more than 2GB of data per message.&lt;/li&gt;
  &lt;li&gt;TextMessage: Text of message is converted to a byte array and copied to the body of the FlumeEvent. The default converter uses UTF-8 by default but this is configurable.&lt;/li&gt;
  &lt;li&gt;ObjectMessage: Object is written out to a ByteArrayOutputStream wrapped in an ObjectOutputStream and the resulting array is copied to the body of the FlumeEvent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example for agent named a1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = jms
a1.sources.r1.channels = c1
a1.sources.r1.initialContextFactory = org.apache.activemq.jndi.ActiveMQInitialContextFactory
a1.sources.r1.connectionFactory = GenericConnectionFactory
a1.sources.r1.providerURL = tcp://mqserver:61616
a1.sources.r1.destinationName = BUSINESS_DATA
a1.sources.r1.destinationType = QUEUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;spooling-directory-source&quot;&gt;Spooling Directory Source&lt;/h2&gt;

&lt;p&gt;This source lets you ingest data by placing files to be ingested into a “spooling” directory on disk. This source will watch the specified directory for new files, and will parse events out of new files as they appear. The event parsing logic is pluggable. After a given file has been fully read into the channel, it is renamed to indicate completion (or optionally deleted).&lt;/p&gt;

&lt;p&gt;Unlike the Exec source, this source is reliable and will not miss data, even if Flume is restarted or killed. In exchange for this reliability, only immutable, uniquely-named files must be dropped into the spooling directory. Flume tries to detect these problem conditions and will fail loudly if they are violated:&lt;/p&gt;

&lt;p&gt;If a file is written to after being placed into the spooling directory, Flume will print an error to its log file and stop processing.
If a file name is reused at a later time, Flume will print an error to its log file and stop processing.
To avoid the above issues, it may be useful to add a unique identifier (such as a timestamp) to log file names when they are moved into the spooling directory.&lt;/p&gt;

&lt;p&gt;Despite the reliability guarantees of this source, there are still cases in which events may be duplicated if certain downstream failures occur. This is consistent with the guarantees offered by other Flume components.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be spooldir.
spoolDir	–	The directory from which to read files from.
fileSuffix	.COMPLETED	Suffix to append to completely ingested files
deletePolicy	never	When to delete completed files: never or immediate
fileHeader	false	Whether to add a header storing the absolute path filename.
fileHeaderKey	file	Header key to use when appending absolute path filename to event header.
basenameHeader	false	Whether to add a header storing the basename of the file.
basenameHeaderKey	basename	Header Key to use when appending basename of file to event header.
ignorePattern	^$	Regular expression specifying which files to ignore (skip)
trackerDir	.flumespool	Directory to store metadata related to processing of files. If this path is not an absolute path, then it is interpreted as relative to the spoolDir.
consumeOrder	oldest	In which order files in the spooling directory will be consumed oldest, youngest and random. In case of oldest and youngest, the last modified time of the files will be used to compare the files. In case of a tie, the file with smallest laxicographical order will be consumed first. In case of random any file will be picked randomly. When using oldest and youngest the whole directory will be scanned to pick the oldest/youngest file, which might be slow if there are a large number of files, while using random may cause old files to be consumed very late if new files keep coming in the spooling directory.
maxBackoff	4000	The maximum time (in millis) to wait between consecutive attempts to write to the channel(s) if the channel is full. The source will start at a low backoff and increase it exponentially each time the channel throws a ChannelException, upto the value specified by this parameter.
batchSize	100	Granularity at which to batch transfer to the channel
inputCharset	UTF-8	Character set used by deserializers that treat the input file as text.
decodeErrorPolicy	FAIL	What to do when we see a non-decodable character in the input file. FAIL: Throw an exception and fail to parse the file. REPLACE: Replace the unparseable character with the “replacement character” char, typically Unicode U+FFFD. IGNORE: Drop the unparseable character sequence.
deserializer	LINE	Specify the deserializer used to parse the file into events. Defaults to parsing each line as an event. The class specified must implement EventDeserializer.Builder.
deserializer.*	 	Varies per event deserializer.
bufferMaxLines	–	(Obselete) This option is now ignored.
bufferMaxLineLength	5000	(Deprecated) Maximum length of a line in the commit buffer. Use deserializer.maxLineLength instead.
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for an agent named agent-1:&lt;/p&gt;

&lt;p&gt;agent-1.channels = ch-1
agent-1.sources = src-1&lt;/p&gt;

&lt;p&gt;agent-1.sources.src-1.type = spooldir
agent-1.sources.src-1.channels = ch-1
agent-1.sources.src-1.spoolDir = /var/log/apache/flumeSpool
agent-1.sources.src-1.fileHeader = true
Twitter 1% firehose Source (experimental)&lt;/p&gt;

&lt;p&gt;Warning This source is hightly experimental and may change between minor versions of Flume. Use at your own risk.
Experimental source that connects via Streaming API to the 1% sample twitter firehose, continously downloads tweets, converts them to Avro format and sends Avro events to a downstream Flume sink. Requires the consumer and access tokens and secrets of a Twitter developer account. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.twitter.TwitterSource
consumerKey	–	OAuth consumer key
consumerSecret	–	OAuth consumer secret
accessToken	–	OAuth access token
accessTokenSecret	–	OAuth toekn secret
maxBatchSize	1000	Maximum number of twitter messages to put in a single batch
maxBatchDurationMillis	1000	Maximum number of milliseconds to wait before closing a batch
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.twitter.TwitterSource
a1.sources.r1.channels = c1
a1.sources.r1.consumerKey = YOUR_TWITTER_CONSUMER_KEY
a1.sources.r1.consumerSecret = YOUR_TWITTER_CONSUMER_SECRET
a1.sources.r1.accessToken = YOUR_TWITTER_ACCESS_TOKEN
a1.sources.r1.accessTokenSecret = YOUR_TWITTER_ACCESS_TOKEN_SECRET
a1.sources.r1.maxBatchSize = 10
a1.sources.r1.maxBatchDurationMillis = 200
Event Deserializers&lt;/p&gt;

&lt;p&gt;The following event deserializers ship with Flume.&lt;/p&gt;

&lt;p&gt;LINE&lt;/p&gt;

&lt;p&gt;This deserializer generates one event per line of text input.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer.maxLineLength	2048	Maximum number of characters to include in a single event. If a line exceeds this length, it is truncated, and the remaining characters on the line will appear in a subsequent event.
deserializer.outputCharset	UTF-8	Charset to use for encoding events put into the channel.
AVRO&lt;/p&gt;

&lt;p&gt;This deserializer is able to read an Avro container file, and it generates one event per Avro record in the file. Each event is annotated with a header that indicates the schema used. The body of the event is the binary Avro record data, not including the schema or the rest of the container file elements.&lt;/p&gt;

&lt;p&gt;Note that if the spool directory source must retry putting one of these events onto a channel (for example, because the channel is full), then it will reset and retry from the most recent Avro container file sync point. To reduce potential event duplication in such a failure scenario, write sync markers more frequently in your Avro input files.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer.schemaType	HASH	How the schema is represented. By default, or when the value HASH is specified, the Avro schema is hashed and the hash is stored in every event in the event header “flume.avro.schema.hash”. If LITERAL is specified, the JSON-encoded schema itself is stored in every event in the event header “flume.avro.schema.literal”. Using LITERAL mode is relatively inefficient compared to HASH mode.
BlobDeserializer&lt;/p&gt;

&lt;p&gt;This deserializer reads a Binary Large Object (BLOB) per event, typically one BLOB per file. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because the entire BLOB is buffered in RAM.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
deserializer	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder
deserializer.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request
NetCat Source&lt;/p&gt;

&lt;p&gt;A netcat-like source that listens on a given port and turns each line of text into an event. Acts like nc -k -l [host] [port]. In other words, it opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel.&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be netcat
bind	–	Host name or IP address to bind to
port	–	Port # to bind to
max-line-length	512	Max line length per event body (in bytes)
ack-every-event	true	Respond with an “OK” for every event received
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Sequence Generator Source&lt;/p&gt;

&lt;p&gt;A simple sequence generator that continuously generates events with a counter that starts from 0 and increments by 1. Useful mainly for testing. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be seq
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
batchSize	1	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = seq
a1.sources.r1.channels = c1
Syslog Sources&lt;/p&gt;

&lt;p&gt;Reads syslog data and generate Flume events. The UDP source treats an entire message as a single event. The TCP sources create a new event for each string of characters separated by a newline (‘n’).&lt;/p&gt;

&lt;p&gt;Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Syslog TCP Source&lt;/p&gt;

&lt;p&gt;The original, tried-and-true syslog TCP source.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be syslogtcp
host	–	Host name or IP address to bind to
port	–	Port # to bind to
eventSize	2500	Maximum size of a single event line, in bytes
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
For example, a syslog TCP source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = syslogtcp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
Multiport Syslog TCP Source&lt;/p&gt;

&lt;p&gt;This is a newer, faster, multi-port capable version of the Syslog TCP source. Note that the ports configuration setting has replaced port. Multi-port capability means that it can listen on many ports at once in an efficient manner. This source uses the Apache Mina library to do that. Provides support for RFC-3164 and many common RFC-5424 formatted messages. Also provides the capability to configure the character set used on a per-port basis.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be multiport_syslogtcp
host	–	Host name or IP address to bind to.
ports	–	Space-separated list (one or more) of ports to bind to.
eventSize	2500	Maximum size of a single event line, in bytes.
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
portHeader	–	If specified, the port number will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the incoming port.
charset.default	UTF-8	Default character set used while parsing syslog events into strings.
charset.port.&lt;port&gt;	–	Character set is configurable on a per-port basis.
batchSize	100	Maximum number of events to attempt to process per request loop. Using the default is usually fine.
readBufferSize	1024	Size of the internal Mina read buffer. Provided for performance tuning. Using the default is usually fine.
numProcessors	(auto-detected)	Number of processors available on the system for use while processing messages. Default is to auto-detect # of CPUs using the Java Runtime API. Mina will spawn 2 request-processing threads per detected CPU, which is often reasonable.
selector.type	replicating	replicating, multiplexing, or custom
selector.*	–	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors.
interceptors.*	 	 
For example, a multiport syslog TCP source for agent named a1:&lt;/port&gt;&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = multiport_syslogtcp
a1.sources.r1.channels = c1
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.ports = 10001 10002 10003
a1.sources.r1.portHeader = port
Syslog UDP Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be syslogudp
host	–	Host name or IP address to bind to
port	–	Port # to bind to
keepFields	false	Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event.
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
For example, a syslog UDP source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = syslogudp
a1.sources.r1.port = 5140
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1
HTTP Source&lt;/p&gt;

&lt;p&gt;A source which accepts Flume Events by HTTP POST and GET. GET should be used for experimentation only. HTTP requests are converted into flume events by a pluggable “handler” which must implement the HTTPSourceHandler interface. This handler takes a HttpServletRequest and returns a list of flume events. All events handled from one Http request are committed to the channel in one transaction, thus allowing for increased efficiency on channels like the file channel. If the handler throws an exception, this source will return a HTTP status of 400. If the channel is full, or the source is unable to append events to the channel, the source will return a HTTP 503 - Temporarily unavailable status.&lt;/p&gt;

&lt;p&gt;All events sent in one post request are considered to be one batch and inserted into the channel in one transaction.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
type	 	The component type name, needs to be http
port	–	The port the source should bind to.
bind	0.0.0.0	The hostname or IP address to listen on
handler	org.apache.flume.source.http.JSONHandler	The FQCN of the handler class.
handler.*	–	Config parameters for the handler
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
enableSSL	false	Set the property true, to enable SSL
keystore	 	Location of the keystore includng keystore file name
keystorePassword Keystore password
For example, a http source for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = http
a1.sources.r1.port = 5140
a1.sources.r1.channels = c1
a1.sources.r1.handler = org.example.rest.RestHandler
a1.sources.r1.handler.nickname = random props
JSONHandler&lt;/p&gt;

&lt;p&gt;A handler is provided out of the box which can handle events represented in JSON format, and supports UTF-8, UTF-16 and UTF-32 character sets. The handler accepts an array of events (even if there is only one event, the event has to be sent in an array) and converts them to a Flume event based on the encoding specified in the request. If no encoding is specified, UTF-8 is assumed. The JSON handler supports UTF-8, UTF-16 and UTF-32. Events are represented as follows.&lt;/p&gt;

&lt;p&gt;[{
  “headers” : {
             “timestamp” : “434324343”,
             “host” : “random_host.example.com”
             },
  “body” : “random_body”
  },
  {
  “headers” : {
             “namenode” : “namenode.example.com”,
             “datanode” : “random_datanode.example.com”
             },
  “body” : “really_random_body”
  }]
To set the charset, the request must have content type specified as application/json; charset=UTF-8 (replace UTF-8 with UTF-16 or UTF-32 as required).&lt;/p&gt;

&lt;p&gt;One way to create an event in the format expected by this handler is to use JSONEvent provided in the Flume SDK and use Google Gson to create the JSON string using the Gson#fromJson(Object, Type) method. The type token to pass as the 2nd argument of this method for list of events can be created by:&lt;/p&gt;

&lt;p&gt;Type type = new TypeToken&amp;lt;List&lt;jsonevent&gt;&amp;gt;() {}.getType();
BlobHandler&lt;/jsonevent&gt;&lt;/p&gt;

&lt;p&gt;By default HTTPSource splits JSON input into Flume events. As an alternative, BlobHandler is a handler for HTTPSource that returns an event that contains the request parameters as well as the Binary Large Object (BLOB) uploaded with this request. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because it buffers up the entire BLOB in RAM.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
handler	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobHandler
handler.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request
Legacy Sources&lt;/p&gt;

&lt;p&gt;The legacy sources allow a Flume 1.x agent to receive events from Flume 0.9.4 agents. It accepts events in the Flume 0.9.4 format, converts them to the Flume 1.0 format, and stores them in the connected channel. The 0.9.4 event properties like timestamp, pri, host, nanos, etc get converted to 1.x event header attributes. The legacy source supports both Avro and Thrift RPC connections. To use this bridge between two Flume versions, you need to start a Flume 1.x agent with the avroLegacy or thriftLegacy source. The 0.9.4 agent should have the agent Sink pointing to the host/port of the 1.x agent.&lt;/p&gt;

&lt;p&gt;Note The reliability semantics of Flume 1.x are different from that of Flume 0.9.x. The E2E or DFO mode of a Flume 0.9.x agent will not be supported by the legacy source. The only supported 0.9.x mode is the best effort, though the reliability setting of the 1.x flow will be applicable to the events once they are saved into the Flume 1.x channel by the legacy source.
Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Avro Legacy Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.avroLegacy.AvroLegacySource
host	–	The hostname or IP address to bind to
port	–	The port # to listen on
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.avroLegacy.AvroLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Thrift Legacy Source&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be org.apache.flume.source.thriftLegacy.ThriftLegacySource
host	–	The hostname or IP address to bind to
port	–	The port # to listen on
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.thriftLegacy.ThriftLegacySource
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.bind = 6666
a1.sources.r1.channels = c1
Custom Source&lt;/p&gt;

&lt;p&gt;A custom source is your own implementation of the Source interface. A custom source’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom source is its FQCN.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be your FQCN
selector.type	 	replicating or multiplexing
selector.*	replicating	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.example.MySource
a1.sources.r1.channels = c1
Scribe Source&lt;/p&gt;

&lt;p&gt;Scribe is another type of ingest system. To adopt existing Scribe ingest system, Flume should use ScribeSource based on Thrift with compatible transfering protocol. For deployment of Scribe please follow the guide from Facebook. Required properties are in bold.&lt;/p&gt;

&lt;p&gt;Property Name	Default	Description
type	–	The component type name, needs to be org.apache.flume.source.scribe.ScribeSource
port	1499	Port that Scribe should be connected
workerThreads	5	Handing threads number in Thrift
selector.type	 	 
selector.*	 	 
Example for agent named a1:&lt;/p&gt;

&lt;p&gt;a1.sources = r1
a1.channels = c1
a1.sources.r1.type = org.apache.flume.source.scribe.ScribeSource
a1.sources.r1.port = 1463
a1.sources.r1.workerThreads = 5
a1.sources.r1.channels = c1&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;Flume User Guide 1.5.0.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
 
</feed>
