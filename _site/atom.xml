<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>NingG.github.com</title>
   <link href="http://ningg.github.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://ningg.github.com" rel="alternate" type="text/html" />
   <updated>2014-10-15T23:17:43+08:00</updated>
   <id>http://ningg.github.com</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>如何参与开源项目</title>
     <link href="http://ningg.github.com/how-to-contribute-open-source-project"/>
     <updated>2014-10-15T00:00:00+08:00</updated>
     <id>http://ningg.github.com/how-to-contribute-open-source-project</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;本来今天晚上想浏览一下flume官网的，不过突然看到How to Get Involved，再看看哪些提交了代码的名单，很是羡慕，我这个人爱吹牛，如果我也在名单中，那岂不又能吹牛一把？哈哈~想想都能笑出声。另一方面，用过的开源工具不少，但是如何参与到开源项目中，我还真不知道，碰巧在看flume官网，那就看看如何参与到flume这个开源项目中去吧。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;官方原文地址：&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Contribute&quot;&gt;How to Contribute&lt;/a&gt;，本文使用&lt;code&gt;英文原文+中文注释&lt;/code&gt;方式来写。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;Welcome contributors! We strive to include everyone’s contributions. This page provides necessary guidelines on how to contribute effectively towards furthering the development and evolution of Flume. You should also read the guide on setting up &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/Development+Environment&quot;&gt;Development Environment&lt;/a&gt; where you will find details on how to checkout, build and test Flume.
（如何下载源码、编译源码、测试源码，需要先阅读&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/Development+Environment&quot;&gt;Development Environment&lt;/a&gt;。）&lt;/p&gt;

&lt;p&gt;Note: This guide applies to general contributors. If you are a committer, please read the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Commit&quot;&gt;How to Commit&lt;/a&gt; as well.
（committer还需阅读&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLUME/How+to+Commit&quot;&gt;How to Commit&lt;/a&gt;）&lt;/p&gt;

&lt;h2 id=&quot;what-can-be-contributed&quot;&gt;What can be contributed?&lt;/h2&gt;

&lt;p&gt;There are many ways you can contribute towards the project. A few of these are:（参与方式，有如下几种）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Jump in on discussions&lt;/strong&gt;: It is possible that someone initiates a thread on the mailing list describing a problem that you have dealt with in the past. You can help the project by chiming in on that thread and guiding that user to overcome or workaround that problem or limitation.（查看邮件列表，参与讨论，帮助他人解决问题）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;File Bugs&lt;/strong&gt;: If you notice a problem and are sure it is a bug, then go ahead and file a JIRA. If however, you are not very sure that it is a bug, you should first confirm it by discussing it on the Mailing Lists.（通过JIRA，提交bug；如果不确定，通过邮件列表提问，确认是否为bug）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Review Code&lt;/strong&gt;: If you see that a JIRA ticket has a “Patch Available” status, go ahead and review it. It cannot be stressed enough that you must be kind in your review and explain the rationale for your feedback and suggestions. Also note that not all review feedback is accepted - often times it is a compromise between the contributor and reviewer. If you are happy with the change and do not spot any major issues, then +1 it. More information on this is available in the following sections.（通过JIRA，检查代码-Patch补丁，提出反馈意见）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Provide Patches&lt;/strong&gt;: We encourage you to assign the relevant JIRA issue to yourself and supply a patch for it. The patch you provide can be code, documentation, build changes, or any combination of these. More information on this is available in the following sections.（通过JIRA，提交代码-patch补丁，可以是代码、文档、编译细节等）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;notes(ningg)：邮件列表、JIRA，我都没有关注过，也不会使用；patch文件了解一点。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;providing-patches&quot;&gt;Providing Patches&lt;/h2&gt;

&lt;p&gt;In order to provide patches, follow these guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make sure there is a JIRA:
    &lt;ol&gt;
      &lt;li&gt;If you are working on fixing a problem that already has an associated JIRA, then go ahead and assign it to yourself.&lt;/li&gt;
      &lt;li&gt;If it is already assigned to someone else, check with the current assignee before moving it over to your queue.&lt;/li&gt;
      &lt;li&gt;If the current assignee has already worked out some part of the fix, suggest that you can take that change over from them and complete the remaining parts.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Attach the patches as you go through development:
While small fixes are easily done in a single patch, it is preferable that you attach patches to the JIRA as you go along. This serves as an early feedback mechanism where interested folks can look it over and suggest changes where necessary. It also ensures that if for some reason you are not able to find the time to complete the change, someone else can take up your initial patches and drive them to completion.
Before you submit your patch:
Your change should be well-formatted and readable. Please use two spaces for indentation (no tabs).
Carefully consider whether you have handled all boundary conditions and have provided sufficiently defensive code where necessary.
Add one or more unit tests, if your change is not covered by existing automated tests.
Insert javadocs and code comments where appropriate.
Update the Flume User Guide (source) if your change affects the Flume config file or any user interface. Include those changes in your patch.
Make sure you update the relevant developer documentation, wiki pages, etc. if your change affects the development environment.
Test your changes before submitting a review:
Before you make the JIRA status as “Patch Available”, please test your changes thoroughly. Try any new feature or fix out for yourself, and make sure that it works.
Make sure that all unit/integration tests are passing, and that the functionality you have worked on is tested through existing or new tests.
You can run all the tests by going to the root level of the source tree and typing mvn clean install.
How to create a patch file:
The preferred naming convention for Flume patches is FLUME-12345.patch, or FLUME-12345-0.patch where 12345 is the JIRA number. You might want to name successive versions of the patch something like FLUME-12345-1.patch, FLUME-12345-2.patch, etc. as you iterate on your changes based on review feedback and re-submit them.
The command to generate the patch is “git diff”. Example:
$ git diff &amp;gt; /path/to/FLUME-1234-0.patch
How to apply someone else’s patch file:
You can apply someone else’s patch with the GNU patch tool. Example:
$ cd ~/src/flume # or wherever you keep the root of your Flume source tree
$ patch -p1 &amp;lt; FLUME-1234.patch
Contributors may variously submit patches in a couple of different formats. If you get some dialog from the patch tool asking which file you want to patch, try variously the “-p1” or “-p0” flags to patch. Without any additional arguments, git diff generates patches that are applied using patch -p1. If you use git diff –no-prefix to generate your patch, you have to apply it using patch -p0. The ReviewBoard tool understands both formats and is able to apply both types automatically.
Submitting your patch for review:
To submit a patch, attach the patch file to the JIRA and change the status of the JIRA to “Patch Available”.
If the change is non-trivial, please also post it for review on the Review Board. Use the Repository “flume-git” on Review Board.
Link the JIRA to the Review Board review. JIRA has a feature you can use for this by going to More Actions &amp;gt; Link &amp;gt; Web Link when logged into JIRA.
Identify a reviewer:
When posting on review board (repository: “flume-git”), always add the Group “Flume” to the list of reviewers.
Optionally, you may also add a specific reviewer to the review. You can pick any of the project committers for review. Note that identifying a reviewer does not stop others from reviewing your change. Be prepared for having your change reviewed by others at any time.
If you have posted your change for review and no one has had a chance to review it yet, you can gently remind everyone by dropping a note on the developer mailing list with a link to the review.
Work with reviewers to get your change fleshed out:
When your change is reviewed, please engage with the reviewer via JIRA or review board to get necessary clarifications and work out other details.
The goal is to ensure that the final state of your change is acceptable to the reviewer so that they can +1 it.&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>In-Stream Big Data Processing</title>
     <link href="http://ningg.github.com/in-stream-big-data-processing"/>
     <updated>2014-10-14T00:00:00+08:00</updated>
     <id>http://ningg.github.com/in-stream-big-data-processing</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;英文原文：&lt;a href=&quot;http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/&quot;&gt;In-Stream-big-data-processing&lt;/a&gt;，有人翻译了&lt;a href=&quot;http://blog.csdn.net/idontwantobe/article/details/25938511&quot;&gt;中文版&lt;/a&gt;，也有直接&lt;a href=&quot;http://dirlt.com/in-stream-big-data-processing.html&quot;&gt;中文注释英文版&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The shortcomings and drawbacks of batch-oriented data processing were widely recognized by the Big Data community quite a long time ago. It became clear that real-time query processing and in-stream processing is the immediate need in many practical applications. In recent years, this idea got a lot of traction and a whole bunch of solutions like Twitter’s Storm, Yahoo’s S4, Cloudera’s Impala, Apache Spark, and Apache Tez appeared and joined the army of Big Data and NoSQL systems. This article is an effort to explore techniques used by developers of in-stream data processing systems, trace the connections of these techniques to massive batch processing and OLTP/OLAP databases, and discuss how one unified query engine can support in-stream, batch, and OLAP processing at the same time.&lt;/p&gt;

&lt;p&gt;At Grid Dynamics, we recently faced a necessity to build an in-stream data processing system that aimed to crunch about 8 billion events daily providing fault-tolerance and strict transactioanlity i.e. none of these events can be lost or duplicated. This system has been designed to supplement and succeed the existing Hadoop-based system that had too high latency of data processing and too high maintenance costs. The requirements and the system itself were so generic and typical that we describe it below as a canonical model, just like an abstract problem statement.&lt;/p&gt;

&lt;p&gt;A high-level overview of the environment we worked with is shown in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/cover-2.png&quot; alt=&quot;cover-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One can see that this environment is a typical Big Data installation: there is a set of applications that produce the raw data in multiple datacenters, the data is shipped by means of Data Collection subsystem to HDFS located in the central facility, then the raw data is aggregated and analyzed using the standard Hadoop stack (MapReduce, Pig, Hive) and the aggregated results are stored in HDFS and NoSQL, imported to the OLAP database and accessed by custom user applications. Our goal was to equip all facilities with a new in-stream engine (shown in the bottom of the figure) that processes most intensive data flows and ships the pre-aggregated data to the central facility, thus decreasing the amount of raw data and heavy batch jobs in Hadoop. The design of the in-stream processing engine itself was driven by the following requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;SQL-like functionality&lt;/strong&gt;. The engine has to evaluate SQL-like queries continuously, including joins over time windows and different aggregation functions that implement quite complex custom business logic. The engine can also involve relatively static data (admixtures) loaded from the stores of Aggregated Data. Complex multi-pass data mining algorithms are beyond the immediate goals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Modularity and flexibility&lt;/strong&gt;. It is not to say that one can simply issue a SQL-like query and the corresponding pipeline will be created and deployed automatically, but it should be relatively easy to assemble quite complex data processing chains by linking one block to another.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault-tolerance&lt;/strong&gt;. Strict fault-tolerance is a principal requirement for the engine. As it sketched in the bottom part of the figure, one possible design of the engine is to use distributed data processing pipelines that implement operations like joins and aggregations or chains of such operations, and connect these pipelines by means of fault-tolerant persistent buffers. These buffers also improve modularity of the system by enabling publish/subscribe communication style and easy addition/removal of the pipelines. The pipelines can be stateful and the engine’s middleware should provide a persistent storage to enable state checkpointing. All these topics will be discussed in the later sections of the article.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability with Hadoop&lt;/strong&gt;. The engine should be able to ingest both streaming data and data from Hadoop i.e. serve as a custom query engine atop of HDFS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;High performance and mobility&lt;/strong&gt;. The system should deliver performance of tens of thousands messages per second even on clusters of minimal size. The engine should be compact and efficient, so one can deploy it in multiple datacenters on small clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To find out how such a system can be implemented, we discuss the following topics in the rest of the article:&lt;/p&gt;

&lt;p&gt;First, we explore relations between in-stream data processing systems, massive batch processing systems, and relational query engines to understand how in-stream processing can leverage a huge number of techniques that were devised for other classes of systems.&lt;/p&gt;

&lt;p&gt;Second, we describe a number of patterns and techniques that are frequently used in building of in-stream processing frameworks and systems. In addition, we survey the current and emerging technologies and provide a few implementation tips.&lt;/p&gt;

&lt;p&gt;The article is based on a research project developed at Grid Dynamics Labs. Much of the credit goes to Alexey Kharlamov and Rafael Bagmanov who led the project and other contributors: Dmitry Suslov, Konstantine Golikov, Evelina Stepanova, Anatoly Vinogradov, Roman Belous, and Varvara Strizhkova.&lt;/p&gt;

&lt;h2 id=&quot;basics-of-distributed-query-processing&quot;&gt;Basics of Distributed Query Processing&lt;/h2&gt;

&lt;p&gt;It is clear that distributed in-stream data processing has something to do with query processing in distributed relational databases. Many standard query processing techniques can be employed by in-stream processing engine, so it is extremely useful to understand classical algorithms of distributed query processing and see how it all relates to in-stream processing and other popular paradigms like MapReduce.&lt;/p&gt;

&lt;p&gt;Distributed query processing is a very large area of knowledge that was under development for decades, so we start with a brief overview of the main techniques just to provide a context for further discussion.&lt;/p&gt;

&lt;h3 id=&quot;partitioning-and-shuffling&quot;&gt;Partitioning and Shuffling&lt;/h3&gt;

&lt;p&gt;Distributed and parallel query processing heavily relies on data partitioning to break down a large data set into multiple pieces that can be processed by independent processors. Query processing could consist of multiple steps and each step could require its own partitioning strategy, so data shuffling is an operation frequently performed by distributed databases.&lt;/p&gt;

&lt;p&gt;Although optimal partitioning for selection and projection operations can be tricky (e.g. for range queries), we can assume that for in-stream data filtering it is practically enough to distribute data among the processors using a hash-based partitioning.&lt;/p&gt;

&lt;p&gt;Processing of distributed joins is not so easy and requires a more thorough examination. In distributed environments, parallelism of join processing is achieved through data partitioning, i.e. the data is distributed among processors and each processor employs a serial join algorithm (e.g. nested-loop join or sort-merge join or hash-based join) to process its part of the data. The final results are consolidated from the results obtained from different processors.&lt;/p&gt;

&lt;p&gt;There are two main data partitioning techniques that can be employed by distributed join processing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disjoint data partitioning&lt;/li&gt;
  &lt;li&gt;Divide and broadcast join&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Disjoint data partitioning technique shuffles the data into several partitions in such a way that join keys in different partitions do not overlap. Each processor performs the join operation on each of these partitions and the final result is obtained as a simple concatenation of the results obtained from different processors.  Consider an example where relation R is joined with relation S on a numerical key k and a simple modulo-based hash function is used to produce the partitions (it is assumes that the data initially distributed among the processors based on some other policy):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/disjoint-partitioning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The divide and broadcast join algorithm is illustrated in the figure below. This method divides the first data set into multiple disjoint partitions (R1, R2, and R3 in the figure) and replicates the second data set to all processors. In a distributed database, division typically is not a part of the query processing itself because data sets are initially distributed among multiple nodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/broadcast-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This strategy is applicable for joining of a large relation with a small relation or two small relations. In-stream data processing systems can employ this technique for stream enrichment i.e. joining a static data (admixture) to a data stream.&lt;/p&gt;

&lt;p&gt;Processing of GroupBy queries also relies on shuffling and fundamentally similar to the MapReduce paradigm in its pure form.  Consider an example where the data is grouped by a string key and sum of the numerical values is computed in each group:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/group-by-query.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, computation consists of two steps: local aggregation and global aggregation. These steps basically correspond to Map and Reduce operations. Local aggregation is optional and raw records can be emitted, shuffled, and aggregated on a global aggregation phase.&lt;/p&gt;

&lt;p&gt;The whole point of this section is that all the algorithms above can be naturally implemented using a message passing architectural style i.e. the query execution engine can be considered as a distributed network of nodes connected by the messaging queues. It is conceptually similar to the in-stream processing pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pipelining&quot;&gt;Pipelining&lt;/h3&gt;

&lt;p&gt;In the previous section, we noted that many distributed query processing algorithms resemble message passing networks. However, it is not enough to organize efficient in-stream processing: all operators in a query should be chained in such a way that the data flows smoothly through the entire pipeline i.e. neither operation should block processing by waiting for a large piece of input data without producing any output or by writing intermediate results on disk. Some operations like sorting are inherently incompatible with this concept (obviously, a sorting block cannot produce any output until the entire input is ingested), but in many cases pipelining algorithms are applicable.  A typical example of pipelining is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/join-pipeline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the hash join algorithm is employed to join four relations: R1, S1, S2, and S3 using 3 processors. The idea is to build hash tables for S1, S2 and S3 in parallel and then stream R1 tuples one by one though the pipeline that joins them with S1, S2 and S3 by looking up matches in the hash tables. In-stream processing naturally employs this technique to join a data stream with the static data (admixtures).&lt;/p&gt;

&lt;p&gt;In relational databases, join operation can take advantage of pipelining by using the symmetric hash join algorithm or some of its advanced variants [1,2]. Symmetric hash join is a generalization of hash join. Whereas a normal hash join requires at least one of its inputs to be completely available to produce first results (the input is needed to build a hash table), symmetric hash join is able to produce first results immediately. In contrast to the normal hash join, it maintains hash tables for both inputs and populates these tables as tuples arrive:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/symmetric-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a tuple comes in, the joiner first looks it up in the hash table of the other stream. If match is found, an output tuple is produced. Then the tuple is inserted in its own hash table.&lt;/p&gt;

&lt;p&gt;However, it does not make a lot of sense to perform a complete join of infinite streams. In many cases join is performed on a finite time window or other type of buffer e.g. LFU cache that contains most frequent tuples in the stream. Symmetric hash join can be employed if the buffer is large comparing to the stream rate or buffer is flushed frequently according to some application logic or buffer eviction strategy is not predictable. In other cases, simple hash join is often sufficient since the buffer is constantly full and does not block the processing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/stream-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is worth noting that in-stream processing often deals with sophisticated stream correlation algorithms where records are matched based on scoring metrics, not on field equality condition. A more complex system of buffers can be required for both streams in such cases.&lt;/p&gt;

&lt;h2 id=&quot;in-stream-processing-patterns&quot;&gt;In-Stream Processing Patterns&lt;/h2&gt;

&lt;p&gt;In the previous section, we discussed a number of standard query processing techniques that can be used in massively parallel stream processing. Thus, on a conceptual level, an efficient query engine in a distributed database can act as a stream processing system and vice versa, a stream processing system can act as a distributed database query engine. Shuffling and pipelining are the key techniques of distributed query processing and message passing networks can naturally implement them. However, things are not so simple. In a contrast to database query engines where reliability is not critical because a read-only query can always be restarted, streaming systems should pay a lot of attention to reliable events processing. In this section, we discuss a number of techniques that are used by streaming systems to provide message delivery guarantees and some other patterns that are not typical for standard query processing.&lt;/p&gt;

&lt;h3 id=&quot;stream-replay&quot;&gt;Stream Replay&lt;/h3&gt;

&lt;p&gt;Ability to rewind data stream back in time and replay the data is very important for in-stream processing systems because of the following reasons:
This is the only way to guarantee correct data processing. Even if data processing pipeline is fault-tolerant, it is very problematic to guarantee that the deployed processing logic is defect-free. One can always face a necessity to fix and redeploy the system and replay the data on a new version of the pipeline.&lt;/p&gt;

&lt;p&gt;Issue investigation could require ad hoc queries. If something goes wrong, one could need to rerun the system on the problematic data with better logging or with code alternations.&lt;/p&gt;

&lt;p&gt;Although it is not always the case, the in-stream processing system can be designed in such a way that it re-reads individual messages from the source in case of processing errors and local failures, even if the system in general is fault-tolerant.&lt;/p&gt;

&lt;p&gt;As a result, the input data typically goes from the data source to the in-stream pipeline via a persistent buffer that allows clients to move their reading pointers back and forth.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/replay-buffer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka messaging queue is well known implementation of such a buffer that also supports scalable distributed deployments, fault-tolerance, and provides high performance.
As a bottom line, Stream Replay technique imposes the following requirements of the system design:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The system is able to store the raw input data for a preconfigured period time.&lt;/li&gt;
  &lt;li&gt;The system is able to revoke a part of the produced results, replay the corresponding input data and produce a new version of the results.&lt;/li&gt;
  &lt;li&gt;The system should work fast enough to rewind the data back in time, replay them, and then catch up with the constantly arriving data stream.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lineage-tracking&quot;&gt;Lineage Tracking&lt;/h3&gt;

&lt;p&gt;In a streaming system, events flow through a chain of processors until the result reaches the final destination (like an external database). Each input event produces a directed graph of descendant events (lineage) that ends by the final results. To guarantee reliable data processing, it is necessary to ensure that the entire graph was processed successfully and to restart processing in case of failures.&lt;/p&gt;

&lt;p&gt;Efficient lineage tracking is not a trivial problem. Let us first consider how Twitter’s Storm tracks the messages to guarantee at-least-once delivery semantics (see the diagram below):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All events that emitted by the sources (first nodes in the data processing graph) are marked by a random ID. For each source, the framework maintains a set of pairs [event ID -&amp;gt; signature] for each initial event. The signature is initially initialized by the event ID.&lt;/li&gt;
  &lt;li&gt;Downstream nodes can generate zero or more events based on the received initial event. Each event carries its own random ID and the ID of the initial event.&lt;/li&gt;
  &lt;li&gt;If the event is successfully received and processed by the next node in the graph, this node updates the signature of the corresponding initial event by XORing the signature with (a) ID of the incoming event and (b) IDs of all events produced based on the incoming event. In the part 2 of diagram below, event 01111 produces events 01100, 10010, and 00010, so the signature for event 01111 becomes 11100 (= 01111 (initial value) xor 01111 xor 01100 xor 10010 xor 00010).&lt;/li&gt;
  &lt;li&gt;An event can be produced based on more than one incoming event. In this case, it is attached several initial event and carries more than one initial IDs downstream (yellow-black event in the part 3 of the figure below).&lt;/li&gt;
  &lt;li&gt;The event considered to be successfully processed as soon as its signature turns into zero i.e. the final node acknowledged that the last event in the graph was processed successfully and no events were emitted downstream. The framework sends a commit message to the source node (see part 3 in the diagram below).&lt;/li&gt;
  &lt;li&gt;The framework traverses a table of the initial events periodically looking for old uncommitted events (events with non-zero signature). Such events are considered as failed and the framework asks the source nodes to replay them.&lt;/li&gt;
  &lt;li&gt;It is important to note that the order of signature updates is not important due to commutative nature of the XOR operation. In the figure below, acknowledgements depicted in the part 2 can arrive after acknowledgements depicted in the part 3. This enables fully asynchronous processing.&lt;/li&gt;
  &lt;li&gt;One can note that the algorithm above is not strictly reliable – the signature could turn into zero accidentally due to unfortunate combination of IDs. However, 64-bit IDs are sufficient to guarantee a very low probability of error, about 2^(-64), that is acceptable in almost all practical applications. As result, the table of signatures could have a small memory footprint.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/lineage-tracking-storm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The described approach is elegant due to its decentrilized nature: nodes act independently sending acknowledgement messages, there is no cental entity that tracks all lineages explicitly. However, it could be difficult to manage transactional processing in this way for flows that maintain sliding windows or other buffers. For example, processing on a sliding window can involve hundreds of thousands events at each moment of time, so it becomes difficult to manage acknowledgements because many events stay uncommitted or computational state should be persisted frequently.&lt;/p&gt;

&lt;p&gt;An alternative approach is used in Apache Spark [3]. The idea is to consider the final result as a function of the incoming data. To simplify lineage tracking, the framework processes events in batches, so the result is a sequence of batches where each batch is a function of the input batches. Resulting batches can be computed in parallel and if some computation fails, the framework simply reruns it. Consider an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/stream-join-microbatching-tx.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the framework joins two streams on a sliding window and then the result passes through one more processing stage. The framework considers the incoming streams not as streams, but as set of batches. Each batch has an ID and the framework can fetch it by the ID at any moment of time. So, stream processing can be represented as a bunch of transactions where each transaction takes a group of input batches, transforms them using a processing function, and persists a result. In the figure above, one of such transactions is highlighted in red. If the transaction fails, the framework simply reruns it. It is important that transactions can be executed in parallel.&lt;/p&gt;

&lt;p&gt;This simple but powerful paradigm enables centralized transaction management and inherently provides exactly-once message processing semantics. It is worth noting that this technique can be used both for batch processing and for stream processing because it treats the input data as a set of batches regardless to their streaming of static nature.&lt;/p&gt;

&lt;h3 id=&quot;state-checkpointing&quot;&gt;State Checkpointing&lt;/h3&gt;

&lt;p&gt;In the previous section we have considered the lineage tracking algorithm that uses signatures (checksums) to provide at-least-one message delivery semantics. This technique improves reliability of the system, but it leaves at least two major open questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In many cases, exactly-once processing semantics is required. For example, the pipeline that counts events can produce incorrect results if some messages will be delivered twice.&lt;/li&gt;
  &lt;li&gt;Nodes in the pipeline can have a computational state that is updated as the messages processed. This state can be lost in case of node failure, so it is necessary to persist or replicate it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Twitter’s Storm addresses these issues by using the following protocol:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Events are grouped into batches and each batch is associated with a transaction ID. A transaction ID is a monotonically growing numerical value (e.g. the first batch has ID 1, the second ID 2, and so on). If the pipeline fails to process a batch, this batch is re-emitted with the same transaction ID.&lt;/li&gt;
  &lt;li&gt;First, the framework announces to the nodes in the pipeline that a new transaction attempt is started. Second, the framework to sends the batch through the pipeline. Finally, the framework announces that transaction attempt if completed and all nodes can commit their state e.g. update it in the external database.&lt;/li&gt;
  &lt;li&gt;The framework guarantees that commit phases are globally ordered across all transactions i.e. the transaction 2 can never be committed before the transaction 1. This guarantee enables processing nodes to use following logic of persistent state updates:
    &lt;ul&gt;
      &lt;li&gt;The latest transaction ID is persisted along with the state.&lt;/li&gt;
      &lt;li&gt;If the framework requests to commit the current transaction with the ID that differs from the ID value persisted in the database, the state can be updated e.g. a counter in the database can be incremented. Assuming a strong ordering of transactions, such update will happen exactly one for each batch.&lt;/li&gt;
      &lt;li&gt;If the current transaction ID equals to the value persisted in the storage, the node skips the commit because this is a batch replay. The node must have processed the batch earlier and updated the state accordingly, but the transaction failed due to an error somewhere else in the pipeline.&lt;/li&gt;
      &lt;li&gt;Strong order of commits is important to achieve exactly-once processing semantics. However, strictly sequential processing of transactions is not feasible because first nodes in the pipeline will often be idle waiting until processing on the downstream nodes is completed. This issues can be alleviated by allowing parallel processing of transactions but serialization of commit steps only, as it shown in the figure below:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/pipelining-commits-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This technique allows one to achieve exactly-once processing semantics assuming that data sources are fault-tolerant and can be replayed. However, persistent state updates can cause serious performance degradation even if large batches are used. By this reason, the intermediate computational state should be minimized or avoided whenever possible.&lt;/p&gt;

&lt;p&gt;As a footnote, it is worth mentioning that state writing can be implemented in different ways. The most straightforward approach is to dump in-memory state to the persistent store as part of the transaction commit process. This does not work well for large states (sliding windows an so on). An alternative is to write a kind of transaction log i.e. a sequence of operations that transform the old state into the new one (for a sliding window it can be a set of added and evicted events). This approach complicates crash recovery because the state has to be reconstructed from the log, but can provide performance benefits in a variety of cases.&lt;/p&gt;

&lt;h3 id=&quot;additive-state-and-sketches&quot;&gt;Additive State and Sketches&lt;/h3&gt;

&lt;p&gt;Additivity of intermediate and final computational results is an important property that drastically simplifies design, implementation, maintenance, and recovery of in-stream data processing systems. Additivity means that the computational result for a larger time range or a larger data partition can be calculated as a combination of results for smaller time ranges or smaller partitions. For example, a daily number of page views can be calculated as a sum of hourly numbers of page views. Additive state allows one to split processing of a stream into processing of batches that can be computed and re-computed independently and, as we discussed in the previous sections, this helps to simplify lineage tracking and reduce complexity of state maintenance.&lt;/p&gt;

&lt;p&gt;It is not always trivial to achieve additivity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In many cases, additivity is indeed trivial. For example, simple counters are additive.&lt;/li&gt;
  &lt;li&gt;In some cases, it is possible to achieve additivity by storing a small amount of additional information. For example, consider a system that calculates average purchase value in the internet shop for each hour. Daily average cannot be obtained from 24 hourly average values. However, the system can easily store a number of transactions along with each hourly average and it is enough to calculate the daily average value.&lt;/li&gt;
  &lt;li&gt;In many cases, it is very difficult or impossible to achieve additivity. For example, consider a system that counts unique visitors on some internet site. If 100 unique users visited the site yesterday and 100 unique user visited the site today, the total number of unique user for two days can be from 100 to 200 depends on how many users visited the site both yesterday and today. One have to maintain lists of user IDs to achieve additivity through intersection/union of the ID lists. Size and processing complexity for these lists can be comparable to the size and processing complexity of the raw data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sketches is a very efficient way to transform non-additive values into additive. In the previous example, lists of ID can be replaced by compact additive statistical counters. These counters provide approximations instead of precise result, but it is acceptable for many practical applications. Sketches are very popular in certain areas like internet advertising and can be considered as an independent pattern of in-stream processing. A thorough overview of the sketching techniques can be found in [5].&lt;/p&gt;

&lt;h3 id=&quot;logical-time-tracking&quot;&gt;Logical Time Tracking&lt;/h3&gt;

&lt;p&gt;It is very common for in-stream computations to depend on time: aggregations and joins are often performed on sliding time windows; processing logic often depends on a time interval between events and so on. Obviously, the in-stream processing system should have a notion of application’s view of time, instead of CPU wall-clock. However, proper time tracking is not trivial because data streams and particular events can be replayed in case of failures. It is often a good idea to have a notion of global logical time that can be implemented as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All events should be marked with a timestamp generated by the original application.&lt;/li&gt;
  &lt;li&gt;Each processor in a pipeline tracks the maximal timestamp it has seen in a stream and updates a global persistent clock by this timestamp if the global clock is behind. All other processors synchronize their time with the global clock.&lt;/li&gt;
  &lt;li&gt;Global clock can be reset in case of data replay.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aggregation-in-a-persistent-store&quot;&gt;Aggregation in a Persistent Store&lt;/h3&gt;

&lt;p&gt;We already have discussed that persistent store can be used for state checkpointing. However, it not the only way to employ an external store for in-stream processing. Let us consider an example that employs Cassandra to join multiple data streams over a time window. Instead of maintaining in-memory event buffers, one can simply save all incoming events from all data streams to Casandra using a join key as row key, as it shown in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/cassandra-join.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other side, the second process traverses the records periodically, assembles and emits joined events, and evicts the events that fell out of the time window. Cassandra even can facilitate this activity by sorting events according to their timestamps.
It is important to understand that such techniques can defeat the whole purpose of in-stream data processing if implemented incorrectly – writing individual events to the data store can introduce a serious performance bottleneck even for fast stores like Cassandra or Redis. On the other hand, this approach provides perfect persistence of the computational state and different performance optimizations – say, batch writes – can help to achieve acceptable performance in many use cases.&lt;/p&gt;

&lt;h3 id=&quot;aggregation-on-a-sliding-window&quot;&gt;Aggregation on a Sliding Window&lt;/h3&gt;

&lt;p&gt;In-stream data processing frequently deals with queries like “What is the sum of the values in the stream over last 10 minutes?” i.e. with continuous queries on a sliding time window. A straightforward approach to processing of such queries is to compute the aggregation function like sum for each instance of the time window independently. It is clear that this approach is not optimal because of the high similarity between two sequential instances of the time window. If the window at the time T contains samples {s(0), s(1), s(2), …, s(T-1), s(T)}, then the window at the time T+1 contains samples {s(1), s(2), s(3), …, s(T), s(T+1)}. This observation suggests that incremental processing might be used.&lt;/p&gt;

&lt;p&gt;Incremental computations over sliding windows is a group of techniques that are widely used in digital signal processing, in both software and hardware. A typical example is a computation of the sum function. If the sum over the current time window is known, then the sum over the next time window can be computed by adding a new sample and subtracting the eldest sample in the window:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/inremental-aggregation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar techniques exist not only for simple aggregations like sums or products, but also for more complex transformations. For example, the SDFT (Sliding Discreet Fourier Transform) algorithm [4] is a computationally efficient alternative to per-window calculation of the FFT (Fast Fourier Transform) algorithm.&lt;/p&gt;

&lt;h2 id=&quot;query-processing-pipeline-storm-cassandra-kafka&quot;&gt;Query Processing Pipeline: Storm, Cassandra, Kafka&lt;/h2&gt;

&lt;p&gt;Now let us return to the practical problem that was stated in the beginning of this article. We have designed and implemented our in-stream data processing system on top of Storm, Kafka, and Cassandra adopting the techniques described earlier in this article. Here we provide just a very brief overview of the solution – a detailed description of all implementation pitfalls and tricks is too large and probably requires a separate article.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/storm-kafka-cassandra-system.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The system naturally uses Kafka 0.8 as a partitioned fault-tolerant event buffer to enable stream replay and improve system extensibility by easy addition of new event producers and consumers. Kafka’s ability to rewind read pointers also enables random access to the incoming batches and, consequently, Spark-style lineage tracking. It is also possible to point the system input to HDFS to process the historical data.&lt;/p&gt;

&lt;p&gt;Cassandra is employed for state checkpointing and in-store aggregation, as described earlier. In many use cases, it also stores the final results.&lt;/p&gt;

&lt;p&gt;Twitter’s Storm is a backbone of the system. All active query processing is performed in Storm’s topologies that interact with Kafka and Cassandra. Some data flows are simple and straightforward: the data arrives to Kafka; Storm reads and processes it and persist the results to Cassandra or other destination. Other flows are more sophisticated: one Storm topology can pass the data to another topology via Kafka or Cassandra. Two examples of such flows are shown in the figure above (red and blue curved arrows).&lt;/p&gt;

&lt;h2 id=&quot;towards-unified-big-data-processing&quot;&gt;Towards Unified Big Data Processing&lt;/h2&gt;

&lt;p&gt;It is great that the existing technologies like Hive, Storm, and Impala enable us to crunch Big Data using both batch processing for complex analytics and machine learning, and real-time query processing for online analytics, and in-stream processing for continuous querying. Moreover, techniques like Lambda Architecture [6, 7] were developed and adopted to combine these solutions efficiently. This brings us to the question of how all these technologies and approaches could converge to a solid solution in the future.  In this section, we discuss the striking similarity between distributed relational query processing, batch processing, and in-stream query processing to figure out the technologies that could cover all these use cases and, consequently, have the highest potential in this area.&lt;/p&gt;

&lt;p&gt;The key observation is that relational query processing, MapReduce, and in-stream processing could be implemented using exactly the same concepts and techniques like shuffling and pipelining. At the same time:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In-stream processing could require strict data delivery guarantees and persistence of the intermediate state. These properties are not crucial for batch processing where computations can be easily restarted.&lt;/li&gt;
  &lt;li&gt;In-stream processing is inseparable from pipelining. For batch processing, pipelining is not so crucial and even inapplicable in certain cases. Systems like Apache 
Hive are based on staged MapReduce with materialization of the intermediate state and do not take full advantage of pipelining.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two statement above imply that tunable persistence (in-memory message passing versus on-disk materialization) and reliability are the distinctive features of the imaginary query engine that provides a set of processing primitives and interfaces to the high-level frameworks:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/in-stream-big-data-processing/unified-engine.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Among the emerging technologies, the following two are especially notable in the context of this discussion:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Tez [8], a part of the Stinger Initiative [9]. Apache Tez is designed to succeed the MapReduce framework introducing a set of fine-grained query processing primitives. The goal is to enable frameworks like Apache Pig and Apache Hive to decompose their queries and scripts into efficient query processing pipelines instead of sequences of MapReduce jobs that are generally slow due to materialization of intermediate results.&lt;/li&gt;
  &lt;li&gt;Apache Spark [10]. This project is probably the most advanced and promising technology for unified Big Data processing that already includes a batch processing framework, SQL query engine, and a stream processing framework.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;A. Wilschut and P. Apers, “Dataflow Query Execution in a Parallel Main-Memory Environment “&lt;/li&gt;
  &lt;li&gt;T. Urhan and M. Franklin, “XJoin: A Reactively-Scheduled Pipelined Join Operator“&lt;/li&gt;
  &lt;li&gt;M. Zaharia, T. Das, H. Li, S. Shenker, and I. Stoica, “Discretized Streams: An Efﬁcient and Fault-Tolerant Model for Stream Processing on Large Clusters”&lt;/li&gt;
  &lt;li&gt;E. Jacobsen and R. Lyons, &lt;a href=&quot;http://www.ingelec.uns.edu.ar/pds2803/materiales/articulos/slidingdft_bw.pdf&quot;&gt;“The Sliding DFT“&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;A. Elmagarmid, Data Streams Models and Algorithms&lt;/li&gt;
  &lt;li&gt;N. Marz, &lt;a href=&quot;http://www.databasetube.com/database/big-data-lambda-architecture/&quot;&gt;“Big Data Lambda Architecture”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;J. Kinley, &lt;a href=&quot;http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting&quot;&gt;“The Lambda architecture: principles for architecting realtime Big Data systems”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/hadoop/tez/&quot;&gt;http://hortonworks.com/hadoop/tez/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/stinger/&quot;&gt;http://hortonworks.com/stinger/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark-project.org/&quot;&gt;http://spark-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;杂谈&lt;/h2&gt;

&lt;p&gt;这次看到dirtysalt的文章：&lt;a href=&quot;http://dirlt.com/in-stream-big-data-processing.html&quot;&gt;In-stream-big-data-processing（英文版+中文注释）&lt;/a&gt;，顿时解决了困扰自己的一个问题，英文的好文章，如何做记录？直接翻译中文？直接转载中文？NO，最佳方式当然是在原文基础上，添加自己的注释，OK，作者dirtysalt已经在做了，今后我也这么办。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>MySQL安装</title>
     <link href="http://ningg.github.com/install-mysql"/>
     <updated>2014-10-13T00:00:00+08:00</updated>
     <id>http://ningg.github.com/install-mysql</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;“在服务器上装个MySQL，我要用。”“OK，稍等。”这么一稍等，就等了40mins，而且，自己之前整理的MySQL安装步骤，并不完整，借这次安装的机会，整理一下吧。&lt;/p&gt;

&lt;h2 id=&quot;mysql&quot;&gt;安装MySQL&lt;/h2&gt;

&lt;p&gt;通常安装MySQL分为几个基本步骤：本地安装MySQL、设置MySQL的root密码、开启MySQL允许远程访问。&lt;/p&gt;

&lt;h3 id=&quot;mysql-1&quot;&gt;本地安装MySQL&lt;/h3&gt;

&lt;p&gt;Linux环境下安装MySQL，有两种方式：rpm包方式、yum源方式（暂不考虑编译源代码方式）。&lt;/p&gt;

&lt;h4 id=&quot;rpm&quot;&gt;rpm包方式&lt;/h4&gt;

&lt;p&gt;到MySQL官网，下载MySQL社区开源版本，详细版本号为：MySQL-5.6.21-1.linux_glibc2.5.x86_64.rpm-bundle.tar。这是一个集合，包含了如下组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MySQL-server&lt;/li&gt;
  &lt;li&gt;MySQL-client&lt;/li&gt;
  &lt;li&gt;MySQL-embedded&lt;/li&gt;
  &lt;li&gt;MySQL-shared&lt;/li&gt;
  &lt;li&gt;MySQL-shared-compat&lt;/li&gt;
  &lt;li&gt;MySQL-test&lt;/li&gt;
  &lt;li&gt;MySQL-devel&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;mysql-2&quot;&gt;1.解压MySQL安装包&lt;/h5&gt;

&lt;p&gt;执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]#tar -xf MySQL-5.6.21-1.linux_glibc2.5.x86_64.rpm-bundle.tar
[root@ningg mysql]#ls
	MySQL-server-5.6.20-1.el6.x86_64.rpm
	MySQL-client-5.6.20-1.el6.x86_64.rpm      
	MySQL-shared-5.6.20-1.el6.x86_64.rpm
	MySQL-devel-5.6.20-1.el6.x86_64.rpm       
	MySQL-shared-compat-5.6.20-1.el6.x86_64.rpm
	MySQL-embedded-5.6.20-1.el6.x86_64.rpm    
	MySQL-test-5.6.20-1.el6.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&quot;mysql-3&quot;&gt;2.创建MySQL系统管理员&lt;/h5&gt;

&lt;p&gt;执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]#groupadd mysql
[root@ningg mysql]#useradd -g mysql mysql
[root@ningg mysql]#id mysql
	uid=27(mysql) gid=27(mysql) groups=27(mysql)
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&quot;mysql-rpm&quot;&gt;3.安装MySQL rpm包&lt;/h5&gt;

&lt;p&gt;执行命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]#rpm -ivh &quot;*.rpm&quot;
Preparing...              ########### [100%]
   1:MySQL-devel          ########### [ 14%]
   2:MySQL-client         ########### [ 29%]
   3:MySQL-test           ########### [ 43%]
   4:MySQL-embedded       ########### [ 57%]
   5:MySQL-shared-compat  ########### [ 71%]
   6:MySQL-shared         ########### [ 86%]
   7:MySQL-server         ########### [100%]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;补充一下，如果安装出现意外，希望卸载MySQL组件，则，卸载顺序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg ~]# rpm -e MySQL-server-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-embedded-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-shared-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-devel-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-test-5.5.24-1.rhel5
[root@ningg ~]# rpm -e MySQL-client-5.5.24-1.rhel5
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;思考：如何保证是mysql用户启动的MySQL？如果使用root运行MySQL，一旦MySQL进程被Hacker控制，Hacker就拥有了root权限？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;yum&quot;&gt;yum源方式&lt;/h4&gt;

&lt;p&gt;（doing…）&lt;/p&gt;

&lt;h3 id=&quot;mysqlroot&quot;&gt;设置MySQL的root密码&lt;/h3&gt;

&lt;p&gt;更详细内容参考&lt;a href=&quot;http://dev.mysql.com/doc/mysql-security-excerpt/5.6/en/user-account-management.html&quot;&gt;MySQL 5.6 Manual: User account management&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;mysql-4&quot;&gt;1.修改MySQL启动配置&lt;/h4&gt;

&lt;p&gt;查找&lt;code&gt;my.cnf&lt;/code&gt;文件位置，两个命令：&lt;code&gt;locate &quot;my.cnf&quot;&lt;/code&gt;和&lt;code&gt;find / -name &quot;my.cnf&quot;&lt;/code&gt;（备注：两个命令有差异，具体参考&lt;a href=&quot;http://312788172.iteye.com/blog/730280&quot;&gt;文章&lt;/a&gt;）。
通常文件位置&lt;code&gt;/etc/my.cnf&lt;/code&gt;或者&lt;code&gt;/usr/my.cnf&lt;/code&gt;，依具体情况行事，在其中设置不启用授权表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]# vim /usr/my.cnf
# For advice on how to change settings please see
# http://dev.mysql.com/doc/refman/5.6/en/server-configuration-defaults.html

[mysqld]
# 新增加下面一行，含义：设置不启用授权表
skip-grant-tables
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;root&quot;&gt;2.重置root密码&lt;/h4&gt;

&lt;p&gt;重新启动MySQL：&lt;code&gt;service mysql restart&lt;/code&gt;，然后进行如下操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ningg mysql]# mysql

mysql&amp;gt; use mysql
mysql&amp;gt; update user set Password=PASSWORD(&#39;1234&#39;) where User=&#39;root&#39;;
mysql&amp;gt; flush privileges;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后，修改&lt;code&gt;my.cnf&lt;/code&gt;文件，注释掉&lt;code&gt;skip-grant-tables&lt;/code&gt;；然后，重启MySQL：&lt;code&gt;service mysql restart&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;3.补充说明&lt;/h4&gt;

&lt;p&gt;针对msyql数据库下的user表，说明几点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; use mysql
#使用下面命令查看表格当前记录
mysql&amp;gt; select * from user \G;
#查看user表格的字段类型
mysql&amp;gt; describe user;
#查看Host\User\Password字段；
mysql&amp;gt; select host,user,password from user;
+-----------+------+---------------+
| host      | user | password      |
+-----------+------+---------------+
| %         | root | *81B936FD50F6 |
| cib02167  | root | *81B936FD50F6 |
| 127.0.0.1 | root | *81B936FD50F6 |
| ::1       | root | *81B936FD50F6 |
+-----------+------+---------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;user表：用户信息、用户权限、密码、可以登录访问的远端主机等。&lt;/li&gt;
  &lt;li&gt;host字段：表示登录MySQL的主机，可以是IP、主机名，如果为&lt;code&gt;%&lt;/code&gt;则表示任何客户端主机都能登录，建议开发时，设置为&lt;code&gt;%&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;命令SET PASSWORD&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#设置用户在不同主机环境下的登录密码
SET PASSWORD FOR &#39;root&#39;@&#39;%&#39; = PASSWORD(&#39;newpass&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令UPDATE&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 可修改user表格内容（MySQL不区分大小写）
update user set password=password(&#39;new-pw&#39;) where user=&#39;root&#39; and host=&#39;%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新授权表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 修改用户信息等，务必flush
flush privileges
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;mysql-5&quot;&gt;开启MySQL允许远程访问&lt;/h3&gt;

&lt;p&gt;在user表中，添加一条&lt;code&gt;user=root&lt;/code&gt;且&lt;code&gt;host=%&lt;/code&gt;的记录，并且通过SET PASSWORD命令重置密码即可。host字段取值&lt;code&gt;%&lt;/code&gt;，即表示任何客户端机器，涵盖远程访问的机器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 可修改user表格内容（MySQL不区分大小写）
update user set password=password(&#39;new-pw&#39;) where user=&#39;root&#39; and host=&#39;%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;疑问：如果没有&lt;code&gt;user=root&lt;/code&gt;且&lt;code&gt;host=%&lt;/code&gt;的记录怎么办？
RE：新建一条记录，或者将&lt;code&gt;user=root&lt;/code&gt;的记录，利用update命令修改为&lt;code&gt;host=%&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-2&quot;&gt;常见问题&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;问题1&lt;/strong&gt;：You must SET PASSWORD before executing this statement&lt;/p&gt;

&lt;p&gt;解决办法：根据提示直接使用set password命令重置密码即可，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; set password=password(&#39;new-pw&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;问题2&lt;/strong&gt;：这个本质上是不是MySQL的管理问题？有哪些用户，哪些用户可以远程登录？&lt;/p&gt;

&lt;p&gt;回应：是的，你很用心在思考，官方文档有很多细节，很有意思的，可以看一下，具体：&lt;code&gt;MySQL Manual&lt;/code&gt;–&lt;code&gt;Security in MySQL&lt;/code&gt;–&lt;code&gt;User Account Management&lt;/code&gt;，有详尽的说明。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/mysql-security-excerpt/5.6/en/grant-table-structure.html&quot;&gt;MySQL 5.6 Manual: Privilege system grant tables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/mysql-security-excerpt/5.6/en/user-account-management.html&quot;&gt;MySQL 5.6 Manual: User account management&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/downloads/mysql/&quot;&gt;MySQL开源社区版本下载地址&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev.mysql.com/doc/&quot;&gt;MySQL官方文档下载地址&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://312788172.iteye.com/blog/730280&quot;&gt;linux下which、whereis、locate、find 命令的区别&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;这篇文章，写的都是小问题，如果读过MySQL的官方文档，就知道&lt;code&gt;User Account Management&lt;/code&gt;的基本知识了，也不用遇到什么问题就蒙圈了，用一个东西，先浏览学习一下官方文档很必要的，看似浪费时间，其实是捷径。当然，一个个小问题折磨自己，才让我意识到读一遍MySQL官方文档的好处。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>消息队列（Message Queue）基本概念</title>
     <link href="http://ningg.github.com/message-queue-intro"/>
     <updated>2014-10-08T00:00:00+08:00</updated>
     <id>http://ningg.github.com/message-queue-intro</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;之前做日志收集模块时，用到flume，另外也有的方案，集成kafaka来提升系统可扩展性，其中涉及到&lt;code&gt;消息队列&lt;/code&gt;，当时自己并不清楚为什么要使用&lt;code&gt;消息队列&lt;/code&gt;，而在我自己提出的原始日志采集方案中不适用&lt;code&gt;消息队列&lt;/code&gt;时，有几个基本问题：1.日志文件上传过程，有个基本的&lt;code&gt;生产者-消费者&lt;/code&gt;问题；2.另外系统崩溃时，数据丢失的处理问题。&lt;/p&gt;

&lt;p&gt;今天，几位同事再次谈到&lt;code&gt;消息队列&lt;/code&gt;这么个东西，很NB的样子，我也想弄清楚，OK，搞起。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;什么是消息队列&lt;/h2&gt;

&lt;p&gt;消息队列（Message Queue，简称MQ），从字面意思上看，本质是个队列，FIFO先入先出，只不过队列中存放的内容是&lt;code&gt;message&lt;/code&gt;而已。其主要用途：不同进程Process/线程Thread之间通信。为什么会产生&lt;code&gt;消息队列&lt;/code&gt;？这个问题问的好，我大概查了一下，没有查到最初产生消息队列的背景，但我猜测可能几个原因：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不同进程（process）之间传递消息时，两个进程之间耦合程度过高，改动一个进程，引发必须修改另一个进程，为了隔离这两个进程，在两进程间抽离出一层（一个模块），所有两进程之间传递的消息，都必须通过&lt;code&gt;消息队列&lt;/code&gt;来传递，单独修改某一个进程，不会影响另一个；&lt;/li&gt;
  &lt;li&gt;不同进程（process）之间传递消息时，为了实现标准化，将消息的格式规范化了，并且，某一个进程接受的消息太多，一下子无法处理完，并且也有先后顺序，必须对收到的消息进行排队，因此诞生了事实上的&lt;code&gt;消息队列&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不管到底是什么原因催生了&lt;code&gt;消息队列&lt;/code&gt;，总之，上面两个猜测是其实际应用的典型场景。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;为什么要用&lt;/h2&gt;

&lt;p&gt;切合前一部分猜测的&lt;code&gt;消息队列&lt;/code&gt;产生背景，其主要解决两个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;系统解耦：项目开始时，无法确定最终需求，不同进程间，添加一层，实现解耦，方便今后的扩展。&lt;/li&gt;
  &lt;li&gt;消息缓存：系统中，不同进程处理消息速度不同，MQ，可以实现不同Process之间的缓冲，即，写入MQ的速度可以尽可能地快，而处理消息的速度可以适当调整（或快、或慢）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面针对&lt;strong&gt;系统解耦&lt;/strong&gt;、&lt;strong&gt;消息缓存&lt;/strong&gt;两点，来分析实际应用&lt;code&gt;消息队列&lt;/code&gt;过程中，可能遇到的问题。虚拟场景：Process_A通过消息队列MQ_1向Process_B传递消息，几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;针对MQ_1中一条消息message_1，如何确保Process_B从MQ_1中只取一次message_1，不会重复多次取出message_1？&lt;/li&gt;
  &lt;li&gt;如果MQ_1中message_1已经被Process_B取出，正在处理的关键时刻，Process_B崩溃了，哭啊，我的问题是，如果重启Process_B，是否会丢失message_1？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不要着急，阅读了下面的简要介绍后，水到渠成，上面几个问题就可以解决了。
消息队列有如下几个好处，这大都是由其&lt;strong&gt;系统解耦&lt;/strong&gt;和&lt;strong&gt;消息缓存&lt;/strong&gt;两点扩展而来的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提升系统可靠性：
    &lt;ul&gt;
      &lt;li&gt;冗余：Process_B崩溃之后，数据并不会丢失，因为MQ多采用&lt;code&gt;put-get-delete&lt;/code&gt;模式，即，仅当确认message被完成处理之后，才从MQ中移除message；&lt;/li&gt;
      &lt;li&gt;可恢复：MQ实现解耦，部分进程崩溃，不会拖累整个系统瘫痪，例，Process_B崩溃之后，Process_A仍可向MQ中添加message，并等待Process_B恢复；&lt;/li&gt;
      &lt;li&gt;可伸缩：有较强的峰值处理能力，通常应用会有突发的访问流量上升情况，使用足够的硬件资源时刻待命，空闲时刻较长，资源浪费，而&lt;code&gt;消息队列&lt;/code&gt;却能够平滑峰值流量，缓解系统组件的峰值压力；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;提升系统可扩展性：
    &lt;ul&gt;
      &lt;li&gt;调整模块：由于实现解耦，可以很容易调整，消息入队速率、消息处理速率、增加新的Process；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;其他：
    &lt;ul&gt;
      &lt;li&gt;单次送达：保证MQ中一个message被处理一次，并且只被处理一次，本质：get获取一个message后，这一message即被预定，同一进程不会再次获取这一message，当且仅当进程处理完这一message后，MQ中会delete这个message，否则，过一段时间后，这一message自动解除被预订状态，进程能够重新预定这个message；&lt;/li&gt;
      &lt;li&gt;排序保证：即，满足队列的FIFO，先入先出策略；&lt;/li&gt;
      &lt;li&gt;异步通信：很多场景下，不会立即处理消息，这是，可以在MQ中存储message，并在某一时刻再进行处理；&lt;/li&gt;
      &lt;li&gt;数据流的阶段性能定位：获取用户某一操作的各个阶段（通过message来标识），捕获不同阶段的耗时，可用于定位系统瓶颈。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;常用的消息队列&lt;/h2&gt;

&lt;p&gt;（doing…）&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;消息队列&lt;/code&gt;实现了进程间通信的升级，如下图所示：&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;top 10 uses for message queue：&lt;a href=&quot;http://blog.iron.io/2012/12/top-10-uses-for-message-queue.html&quot;&gt;英文原文&lt;/a&gt;、&lt;a href=&quot;/download/message-queue-intro/top-10-mq.pdf&quot;&gt;pdf版本&lt;/a&gt;、&lt;a href=&quot;http://www.oschina.net/translate/top-10-uses-for-message-queue&quot;&gt;中文译文&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Message_queue&quot;&gt;Message Queue wiki&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bbs.csdn.net/topics/110160741&quot;&gt;http://bbs.csdn.net/topics/110160741&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/yuanyi_wang/archive/2009/12/30/1636178.html&quot;&gt;http://www.cnblogs.com/yuanyi_wang/archive/2009/12/30/1636178.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.php1.cn/article/9865.html&quot;&gt;http://www.php1.cn/article/9865.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>想和自己谈一谈</title>
     <link href="http://ningg.github.com/personal-think"/>
     <updated>2014-09-30T00:00:00+08:00</updated>
     <id>http://ningg.github.com/personal-think</id>
     <content type="html">&lt;p&gt;今天读了&lt;code&gt;***&lt;/code&gt;的博客，特别是每年的个人总结，基本上2004–2013的都看了，希望能从中窥探一点前辈工作十余年的体会，同时也想看看有没有能够借鉴的，万幸，读的较为认真，我也产生了一点想法，借这个时机，我想和自己谈一谈。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;有个打算，有点追求&lt;/h2&gt;

&lt;p&gt;整体上，我需要有些时间较长的总结、计划，比如一年、6个月、3个月的打算；每次打算，需要要涵盖3个方面：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;前一段，我做了什么？有什么可以积累的地方、有什么改进的地方；&lt;/li&gt;
  &lt;li&gt;接下来我需要做什么？计划怎么行动？&lt;/li&gt;
  &lt;li&gt;做这些事情，有一个基本的理念，有吗？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;说到较长时间的打算，我在学校读书的最后一年，曾有过3个月的打算，其实打算开始之前，已经陆续铺垫有3个多月，事后总结，最初的打算完成了60%，当时获得了很大的信心，那种幸福、自信的感觉为自己今后的一些决定带来了内心的支撑，特别是，当出现争议的时候，自己能够在自己进行深入分析后做决断。&lt;/p&gt;

&lt;p&gt;自己做事情，脑袋里有一个基本的理念吗？理念还是有的，整体上几个点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;知识、认识方面的&lt;strong&gt;分享&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;多人一起做事时的&lt;strong&gt;协作&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;所用基础工具的&lt;strong&gt;开源&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;团队、个人核心在于&lt;strong&gt;成长&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从上述理念出发，更能得到自己的共鸣，不要怕对外开放、交流之后，技术上被超越，即使被超越了，对整个社会的进步也是好的。&lt;em&gt;（如果公司对于某些技术有保密要求，这就绝不能对外分享，但是整体的框架，一些设计理念上的东西，也可以在交流中成长）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;能够随身携带的技能，如果可以预见的时间内会用到，那就尽多获取、提早准备，例如，驾驶、外语、急救措施、生活中安全措施。当然，其中最重要是：身体。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;视野放开，保持敏感&lt;/h2&gt;

&lt;p&gt;要将视野放开，保持与时代相接触，对于自己所处行业，需要有自己的认识，这个认识可以是错误的，分享出去，自然会有人忍不住来指正，关注国内和国外最新的科技动态，包括产品、生活、行业；主要途径：业内顶尖人才、顶尖公司动向，领域内主流媒体的动向。&lt;/p&gt;

&lt;p&gt;前沿媒体：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;www.36kr.com&quot;&gt;36Kr&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.infoq.com&quot;&gt;InfoQ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.valleytalk.org&quot;&gt;弯曲评论&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;个人博客、微信：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;coolshell.cn&quot;&gt;酷壳coolshell&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;www.williamlong.info&quot;&gt;月光博客&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;陈利人：&lt;a href=&quot;http://weibo.com/lirenchen&quot;&gt;微博&lt;/a&gt;、&lt;code&gt;微信号：daiziguizhongren&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;梁斌：&lt;a href=&quot;http://weibo.com/pennyliang&quot;&gt;微博&lt;/a&gt;、&lt;code&gt;微信号：pennyjob&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Fenng：&lt;a href=&quot;http://hutu.me/&quot;&gt;小道消息&lt;/a&gt;、&lt;a href=&quot;http://news.dbanotes.net/&quot;&gt;Startup News&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;新兴的潮流网站：对一些如同爆炸性出现的新技术、网站、观点，保持敏感度，进行简要学习其基本规律，了解轮廓。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;上面说了两点，其实就是一个意思：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;有个打算，有点追求：做事情，集中精力，一件一件解决掉；&lt;/li&gt;
  &lt;li&gt;视野放开，保持敏感：做事情，看趋势，3~5年、20年；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;我写blog，大都是分要点，列一个1、2、3，调理还算清晰，但，我总感觉我blog读起来味道怪怪的，相反，读Fenng、caoz、haoel的文章，能感觉到有平铺直叙、娓娓道来，很有味道。写blog，不仅要内容有用，而且，让人读起来也要舒服，这一点，我需要学习改进。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>线框图工具：mockups</title>
     <link href="http://ningg.github.com/mockups-intro"/>
     <updated>2014-09-29T00:00:00+08:00</updated>
     <id>http://ningg.github.com/mockups-intro</id>
     <content type="html">&lt;h2 id=&quot;mockups&quot;&gt;mockups的特点&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;快：提供常用的元素，通过简单托、拉、拽等动作，即可完成线框图绘制；&lt;/li&gt;
  &lt;li&gt;手绘风格：展示效果，朴素、简洁，能够突出内容、层次结构、业务流程；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;基础用法&lt;/h2&gt;

&lt;p&gt;简单的托、拉、拽内置的组件，就能够达到如下的效果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mockups-intro/simple-operate.jpg&quot; alt=&quot;简单的鼠标操作&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;高效用法&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;操作&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;/&lt;/code&gt;/&lt;code&gt;+&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;快速定位到&lt;code&gt;Quick Add&lt;/code&gt;输入框，在其中输入控件名字，即可自动添加一个元素&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;Enter&lt;/code&gt;/&lt;code&gt;F2&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;编辑&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;Ctrl+2&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;锁定&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;Ctrl+Up&lt;/code&gt;/&lt;code&gt;Ctrl+Down&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;上移、下移一层&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code&gt;Ctrl+D&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;复制并粘贴，相当于&lt;code&gt;Ctrl+C&lt;/code&gt; + &lt;code&gt;Ctrl+V&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;几点补充：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;官方提供一些现有的UI模版：&lt;a href=&quot;http://support.balsamiq.com/customer/portal/articles/1311316-how-to-download-from-mockups-to-go&quot;&gt;Mockups To Go&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;通过点击，可实现mockups一个页面跳转到同一目录下的另一个页面；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（官方网站内容还没有看完，doing…）&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.balsamiq.com/#support-documentation&quot;&gt;Mockups官网文档&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://support.balsamiq.com/customer/portal/topics/49503-tutorials&quot;&gt;Mockups官网Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;闲谈&lt;/h2&gt;

&lt;p&gt;扯一点小事：写这篇文章关于mockups操作的文章，起初是无意间受到&lt;code&gt;1-2-3&lt;/code&gt;的博客《&lt;a href=&quot;http://www.cnblogs.com/1-2-3/archive/2009/08/17/Balsamiq-Mockups-Introduction.html&quot;&gt;我喜欢Balsamiq Mockups的三大理由&lt;/a&gt;》和《&lt;a href=&quot;http://www.cnblogs.com/1-2-3/archive/2009/06/30/Balsamiq-Mockups-tips.html&quot;&gt;Balsamiq Mockups 小技巧&lt;/a&gt;》 启发，不过自己当时冷静了一下，脑海里闪烁一个疑问：&lt;code&gt;1-2-3&lt;/code&gt;关于mockups的知识是从哪获取的？哦，官网，最初的知识传播，肯定是某人比较认真，读了官网，然后知识就散播开了，OK，为避免知识在传播过程中有损伤、误解，我直接找根源好了，这样的做法在普通人看来笨拙，却是真正捷径，最简洁、省时、省力的方法。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>产品设计：线框图</title>
     <link href="http://ningg.github.com/wireframe"/>
     <updated>2014-09-28T00:00:00+08:00</updated>
     <id>http://ningg.github.com/wireframe</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;题记：从网上找了一份材料：&lt;a href=&quot;/download/wireframe/wireframe-learning.pptx&quot;&gt;Wireframes.ppt&lt;/a&gt;，下面的内容基本都是从其中整理出来的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section&quot;&gt;项目流程&lt;/h2&gt;

&lt;p&gt;直接贴一张图片：项目流程（中大型产品），这张图片来自于&lt;a href=&quot;/download/wireframe/wireframe-learning.pptx&quot;&gt;Wireframes.ppt&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/wireframe/project-process.png&quot; alt=&quot;project-process.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;说明一点：线框图产生于项目前期，对应到上面的流程中，就是&lt;code&gt;交互设计初稿&lt;/code&gt;的阶段。&lt;/p&gt;

&lt;p&gt;总结一下上面图片中展示的项目流程，一个中大型产品的开发基本过程是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;需求分析&lt;/li&gt;
  &lt;li&gt;需求确定&lt;/li&gt;
  &lt;li&gt;交互设计稿&lt;em&gt;（线框图）&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;视觉设计稿&lt;em&gt;（线框图）&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;项目计划，包括：
    &lt;ul&gt;
      &lt;li&gt;功能需求分析&lt;/li&gt;
      &lt;li&gt;开发设计&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;产品开发，包括：
    &lt;ul&gt;
      &lt;li&gt;前端程序开发&lt;/li&gt;
      &lt;li&gt;后端程序开发&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;测试&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;线框图&lt;/h2&gt;

&lt;p&gt;线框图，就是几张骨架图，两点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;线框、简单的线条，来表现内容；&lt;/li&gt;
  &lt;li&gt;不关心视觉效果和细节；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要做一个用户展示界面的，有几个方面需要考虑：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;需要展示什么内容？&lt;/li&gt;
  &lt;li&gt;这些内容的重要程度怎么划分？&lt;/li&gt;
  &lt;li&gt;内容之间的层次关系？&lt;/li&gt;
  &lt;li&gt;同一页面上，不同内容的位置？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面前两点，需要通过需求分析来确定；后两点，在线框图上能够体现出来。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;线框图的意义&lt;/h3&gt;

&lt;p&gt;线框图在产品的设计过程中，具有不可替代的优势：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;快速创建：设计师不需要考虑太多细节；&lt;/li&gt;
  &lt;li&gt;直观呈现，帮助聚焦：项目前期，去除视觉和细节干扰，确保讨论中，大家将注意力集中在：需求是什么？需要放啥东西？需要怎么样与用户交互？这些内容的层次如何划分？&lt;/li&gt;
  &lt;li&gt;方便修改：轻松修改设计、甚至放弃某一设计；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;几点理论&lt;/h3&gt;

&lt;p&gt;关于线框图，有几点要说明的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每个阶段的线框图都有其特定的价值，并不是精美细致的就是好的；&lt;/li&gt;
  &lt;li&gt;从最简单的设计开始，逐渐添加细节；&lt;/li&gt;
  &lt;li&gt;线框图的最大价值在于讨论和优化；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通常用户进行产品设计时，绘制产品原型的过程，分为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;草图&lt;/li&gt;
  &lt;li&gt;粗略线框图&lt;/li&gt;
  &lt;li&gt;详细线框图&lt;/li&gt;
  &lt;li&gt;交互说明图&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;线框图分类&lt;/h3&gt;

&lt;p&gt;当前不同人员都会使用线框图，主要的有3类设计师：PD、交互、视觉。他们使用线框图的侧重有所不同：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PD：关注，功能、内容、业务流程；&lt;/li&gt;
  &lt;li&gt;交互：关注，业务流程、页面布局、层次结构；&lt;/li&gt;
  &lt;li&gt;视觉：关注，整体风格/气质、颜色、质感，以及其他精细的视觉元素；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结一下，对于中小工程，应该一个工程师搞定所有、一切。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;画线框图的工具&lt;/h2&gt;

&lt;p&gt;选取工具有几个依据：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;保证线框图本身的优势不缺失：快速创建、帮助聚焦、方便修改；&lt;/li&gt;
  &lt;li&gt;工具制作的输出文档要易用：例如，保存为多种形式；&lt;/li&gt;
  &lt;li&gt;想法不受工具的束缚；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更详细一点，产品设计时，选取工具要考虑如下几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Design更容易：线框图、流程图、架构图&lt;/li&gt;
  &lt;li&gt;Present更方便：策划文档、规格书&lt;/li&gt;
  &lt;li&gt;Experience更交互：体验原型、交互设计&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;说两个工具：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mockups&lt;/li&gt;
  &lt;li&gt;Axure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-6&quot;&gt;小结&lt;/h2&gt;

&lt;p&gt;工具能够提升做事的效率，但归根结底，进行设计的时候，是需要想法、需要思考的，工具只是表达；具体在使用线框图时，说两点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从最简单的开始，在迭代中逐步增加细节；&lt;/li&gt;
  &lt;li&gt;留下会议纪要、修改纪要；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;嗳，刚刚回头看了一下上面对线框图的说明，我x，是我故意把事情说复杂了吧，线框图存在的意义，其实，就几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;把想法写出来；&lt;em&gt;（借助工具）&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;从其他人那里得到反馈意见；&lt;/li&gt;
  &lt;li&gt;修改至定稿；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-7&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/download/wireframe/wireframe-learning.pptx&quot;&gt;Wireframes.ppt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>CTO这点事</title>
     <link href="http://ningg.github.com/talk-about-cto"/>
     <updated>2014-09-28T00:00:00+08:00</updated>
     <id>http://ningg.github.com/talk-about-cto</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;题记：本文转载自caoz，原文地址：&lt;a href=&quot;http://zhuanlan.zhihu.com/iamcaoz/19856992&quot;&gt;CTO这点事&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;几乎整个互联网行业都缺CTO，特别是一些草根背景的创业者，这个问题更加显著。从我自己的感受，身边各种朋友委托我找CTO的需求，嗯，算下来超过两位数了，光最近一个月就有3个，而且这三家都是刚拿了A轮的。其他那些公司CTO大部分空缺了一两年，或者其他高管临时暂代过渡。实话说，我觉得每个公司都不错的，但通常也只能遗憾的说，真没有能推荐的。
其实，根据个人的观察，每个互联网团队都喊需要CTO，但是具体诉求却各不相同，如果说共性，就只有一点，那就是，公司老板对技术的期望值与目前技术团队的能力表现，有较大的差异，而这个差异，对于老板来说，就是一个想法，找个合格的CTO，一切就都解决了。其实，真不是这回事。&lt;/p&gt;

&lt;p&gt;今天要说的第一点，就是期望值的控制；很多互联网公司都希望自己走技术驱动的路线，期望小而美，复制美国技术新贵的市场表现；这不能说是一个错误的期望，但是，现实能有多少符合这种需求的人才呢？这样的人才需要技术有前瞻性，对产业格局有判断，对管理有心得，情商还不能低（算了一下，四项里我至少三项不符合。）。整个行业内这样的人有几个？凭什么会跟你？　事实上我身边确实有这样的案例，一个以业务为主的公司，搞定了一个超棒的CTO，很快就转型成以技术为驱动的公司，公司价值极大提升，问题是，这种现象很难具有复制性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/talk-about-cto/chinese-carrier.jpg&quot; alt=&quot;chinese-carrier&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cto&quot;&gt;大家对CTO的期望&lt;/h2&gt;

&lt;p&gt;下面我说一下一个最基本的让人纠结的问题，到底什么是CTO？其实，空谈这个名词的定义毫无意义，从我身边很多朋友公司的实例来看，他们对这个角色的定义和定位是差异非常大的。具体而言，不同创业团队，对CTO需求的真实想法，包括如下层面。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;技术选型&lt;/h3&gt;

&lt;p&gt;这其实是创业公司最纠结的问题；他们往往一上来基于已有的程序员的个人习惯和爱好，选择了一个技术方案，然后到某一天一看，我靠，全是坑（当然，也可能与执行者的能力有关）。而更糟的是，这个技术方案相对冷门，市场上去招聘都很难做。还有就是技术方案成本过高，（不只是钱的问题，特别是时间成本！）结果严重影响到后续的发展速度。　我举个简单例子，最近我给多个创业者提建议，比如做app，很多以内容运营为核心的app，不要用原生态开发，目前一堆第三方的跨平台开发架构，如果选择合适，可以极大减少开发成本，以及降低技术招聘的难度。微信开店开社区，也有一堆第三方平台，这些事情，对于一些创业者来说，就是特纠结的事情，第一，他们并不完全了解这个领域；第二，他们就算有所了解，也很难判断究竟谁家的方案更靠谱？难道要靠百度竞价排名来裁决么？　而对于技术人员，包括相当多技术高手，他们也会有一些自己的打算，比如说，我就熟悉这个，为啥要用我不熟悉的？（没有考虑从公司长远的人力成本和开发效率）。比如说，我本来服务端技术很强，现在用了一个开源服务端框架，比如skynet，我变成写脚本的了，我价值怎么体现？（不知道自己的价值来自于产品的市场表现）。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;万能适配器&lt;/h3&gt;

&lt;p&gt;公司有了一个基本的起步，比如在某个平台上有了一点成绩，然后希望扩展到不同领域，不同平台，然后老板突然发现，哦，原来程序员不是什么都会的。服务端，前端，运维，安全，数据库，甚至SEO优化（你敢说这不是技术？）最好有个牛人都能带起来，这个牛人，就是他们定义的CTO，咳咳。创业公司，人少而事杂，其实，真的可以理解。我认识不少创业者，最终万般无奈下，硬是把自己逼成了万能哥。话说当年我也是创业的时候，万般无奈才自学怎么做运维的，虽然很多都没学会，但是偶尔也能出去忽悠一下。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;性能调优及架构扩展&lt;/h3&gt;

&lt;p&gt;很多小公司到了一定的用户规模和并发规模的时候，一下子就扛不住了，甚至严重影响业务的拓展，某著名淘品牌自己做了一个独立网店，结果一搞秒杀活动就崩溃，（吐个槽，他们运营人员也死心眼，就知道搞秒杀，知道技术架构不行，不会搞点别的形式么）。结果独立网店一直就没搞起来。其实很多这些偏传统的互联网公司，他们所遇到的问题，在高手眼里不值一提，但是这个坎过不去，他们真就发展不起来。说个题外话八卦一下，当年腾讯叫oicq的时候，一堆*icq，只有腾讯牛了，其他的为啥不行？真的是产品原因么？其实都是这个问题，到了１００万用户的时候，就只有腾讯还能发展新用户，其他的技术全都撑不住了。大家看到的都是剩者为王，我看到的是，数不清非常不错的产品，死在这个环节上了。严格的说，这其实不是CTO的工作，按照分工来说，通常涉及系统分析师，系统架构师，以及DBA来做这方面的工作，但是对于创业公司，根本就没这种角色设定，他们遇到这个问题，通常想到的是，有个好的CTO就好了。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;团队管理和效率提升&lt;/h3&gt;

&lt;p&gt;这还真是CTO的本职工作了，我遇到技术出身的创业者，他纠结的问题是，当时自己一个人做开发，连带着做运营，七七八八一堆事，也把东西做起来了，现在公司产品知名度有了，拿到投资了，开发团队也搭起来了，怎么开发效率反而下去了？所以，他的理解是，需要一个CTO。　从技术人才招聘，团队培养，技术培训，到开发模式培训（比如现在流行的敏捷开发），产品质量测评体系，信息安全防护等等，这些林林总总的事情，对于创业者来说，就全部归纳为“需要一个CTO”，嗯。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;数据决策体系&lt;/h3&gt;

&lt;p&gt;自从“大数据”概念火了以后，基本上所有创业者都在谈，我的业务模式，发展了海量用户后，一定是基于数据驱动，可是数据咋驱动？没几个人真想清楚了，但是没关系，只要有CTO，这个问题就解决了。　我知道真有能搞定此事的CTO，真的有，不过，真的属于稀缺品种，目测比大熊猫还少。因为，要理解数据驱动，首先要理解业务，而理解业务，往往不是技术人员的特长。又要自吹一句，我在百度的时候，最牛逼的事情就是在产品部门里搭出一个技术团队，来做数据，为什么要坚持在产品部门编制？（实话说，当年技术部门的薪酬更高一些），我坚持认为重心在于数据目标和解读，而不是技术实现。　所以，这里我多说一句，数据决策体系，先想清楚具体目标和具体展现，再寻求技术支持，而不是一句话推给技术团队去弄，除非你公司里真有这么一只极品稀缺大熊猫。　实际上，再吐个槽，最基本的数据处理的结果，经过正确严谨的解读，就已经有极大的价值了，而很多人往往无视这些，去追求更高深的技术实现，搞一些莫名其妙的算法和逻辑，来彰显技术价值，其实是舍近求远的表现。　&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;重构商业模式&lt;/h3&gt;

&lt;p&gt;这个，咳咳，怎么说呢，公司觉得自己商业模式不够酷，不够吸引资本关注，希望从业务驱动转为技术驱动，嗯，来个牛逼的CTO，这事真可行，嗯，话说，这事真可行，点到为止，不展开了。　&lt;/p&gt;

&lt;p&gt;以上是我观测到的，身边以及业内一些典型的CTO需求，当然，不同公司诉求真的不一样。有的侧重管理，有的侧重解决瓶颈问题，但是对老板而言，只要他们认为目前技术团队不够好，不够给力，不够预期，那么，这就是他们认为缺乏的，CTO。不仅仅是创业公司，上市公司乃至巨头，这个问题也一样存在，比如说，百度的CTO，貌似空缺了好几年呢。是他们的人才不够优秀么？还不是达不到李厂长期望值。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;个人观点&lt;/h2&gt;

&lt;p&gt;下面说我的一些观点&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一&lt;/strong&gt;，相当比例大公司背景的技术牛人，并不适合去创业公司做CTO，原因1：技术再牛，不够全面，这没辙，大公司求深，创业公司求全；原因2：技术再牛，要懂业务。创业公司没有完备严谨的产品经理，没有明确的目标规划，很多工作是一个初步的目标，然后做起来看。如果技术人员不懂业务，很可能就会走弯路，做出很多不伦不类的东西，所以创业公司遇到这样的大牛，要先沟通到位再决定，不要迷信简历，当然，不乏这里有非常出色的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二&lt;/strong&gt;，创始公司老板要想清楚自己当前真正需要的是什么，控制一下目标和欲望，有些事不能强求的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第三&lt;/strong&gt;，要给人才成长的空间，CTO也不是天生的，不要认为外面的和尚一定好念经。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第四&lt;/strong&gt;，遇到真的特别合适的人才，要舍得本钱，理想可以谈，但是让人家舍弃各种丰厚回报的机会来跟你打拼，总要出示一些诚意。要知道这样的人才是稀缺品。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;几点解释&lt;/h2&gt;

&lt;p&gt;最后解释一下 CTO所需要的四个领域的能力&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;技术前瞻性&lt;/strong&gt;，就好比2010年谷歌手机发布了，您还在那里吭哧吭哧跟老板说咱们好好弄塞班。（好多技术牛人，大公司的技术总监都是这样的！！！）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对产业格局有判断&lt;/strong&gt;，举例来说，移动互联网起来了，能知道工具型，娱乐性，生活性的应用大概的爆发周期和爆发规模。 能对移动游戏，电商的发展速度和市场顶点在哪里，这样才能给老板正确的技术投入建议和规划目标建议，别市场喷发的时候不去考虑准备架构扩容，还在角落里扣扣索索的算怎么比同行省点运维成本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;管理心得&lt;/strong&gt;，技术团队的培养和带动，别光顾着自己牛逼，要让团队牛逼。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;情商&lt;/strong&gt;，各种沟通，协调，不解释。&lt;/p&gt;

&lt;p&gt;就这样，有点凌乱，凑活看吧&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>优秀的软件工程师</title>
     <link href="http://ningg.github.com/software-engineer-self-test"/>
     <updated>2014-09-28T00:00:00+08:00</updated>
     <id>http://ningg.github.com/software-engineer-self-test</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;题记：本文转载自邹欣的书籍《&lt;a href=&quot;http://book.douban.com/subject/25965995//&quot;&gt;构建之法&lt;/a&gt;》&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;保持高标准，不要受制于破窗理论(broken windows theory)。
当你看到不靠谱的设计、糟糕的代码、过时的文档和测试用例的时候，不要想“既然别人的代码已经这样了，我的代码也可以随便一点啦。”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主动解决问题。当看到不靠谱的设计，糟糕的代码的时候，不要想“可能别人会来管这个事情” ，或者“我下个月发一个邮件让大家讨论一下”。要主动地把问题给解决了[ii]。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;经常给自己充电，身体训练是运动员生活的一部分，学习是软件工程师职业的伴侣。每半年就要了解和学习一些新的相关技术。通过定期分享（面对面的分享，写技术博客等）来确保自己真正掌握了新技术。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DRY （Don’t Repeat Yourself）——别重复。在一个系统中，每一个知识点都应该有一个无异议的、正规的表现形式。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;消除不相关模块之间的影响，在设计模块的时候，要让它们目标明确并单一，能独立存在，没有不明确的外部依赖。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过快速原型来学习，快速原型的目的是学习，它的价值不在于代码，而在于你通过快速原型学到了什么。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;设计要接近问题领域，在设计的时候，要接近你目标用户的语言和环境。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;估计任务所花费的时间，避免意外。在开始工作的时候，要做出时间和潜在影响的估计，并通告相关人士，避免最后关头意外发生。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;图形界面的工具有它的长处，但是不要忘了命令行工具也可以发挥很高的效率，特别是可以用脚本构建各种组合命令的时候。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;有很多代码编辑器，请把其中一个用得非常熟练。让编辑器可以实现自己的定制，可以用脚本驱动，用起来得心应手。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;理解常用的设计模式，并知道择机而用。设计模式不错，更重要的是知道它的目的是什么，什么时候用，什么时候不用。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;代码版本管理工具是你代码的保障，重要的代码一定要有代码版本管理。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在debug的时候，不要惊慌，想想导致问题的原因可能在哪里。一步一步地找到原因。要在实践中运用工具，善于分析日志（log），从中找到bug。同时，在自己的代码里面加 log.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;重要的接口要用形式化的“合同”来规定。用文档和断言、自动化测试等工具来保证代码的确按照合同来做事，不多也不少。使用断言 (assertion) 或者其他技术来验证代码中的假设，你认为不可能发生的事情在现实世界中往往会发生。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;只在异常的情况下才使用异常 (Exception),  不加判断地过多使用异常，会降低代码的效率和可维护性。记住不要用异常来传递正常的信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;善始善终。如果某个函数申请了空间或其他资源，这个函数负责释放这些资源。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当你的软件有多种技术结合在一起的时候，要采用松耦合的配置模式，而不是要把所有代码都集成到一起。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;把常用模块的功能打造成独立的服务，通过良好的界面 (API) 来调用不同的服务。[YEKA1] &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在设计中考虑对并行的支持，这样你的API 设计会比较容易扩展。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在设计中把展现模块 (View) 和实体模块 (Model) 分开，这样你的设计会更有灵活性。 &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;重视算法的效率，在开始写之前就要估计好算法的效率是哪一个数量级上的（big-O）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在实际的运行场景中测试你的算法，不要停留在数学分析层面。有时候一个小小的实际因素 (是否支持大小写敏感的排序，数据是否支持多语言)会导致算法效率的巨大变化。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;经常重构代码，同时注意要解决问题的根源。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在开始设计的时候就要考虑如何测试 ，如果代码出了问题，有log 来辅助debug 么? 尽早测试，经常测试，争取实现自动化测试，争取每一个构建的版本都能有某些自动测试。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;代码生成工具可以生成一堆一堆的代码，在正式使用它们之前，要确保你能理解它们，并且必要的时候能debug 这些代码。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;和一个实际的用户一起使用软件，获得第一手反馈。 &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在自动测试的时候，要有意引地入bug，来保证自动测试的确能捕获这些错误。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果测试没有做完，那么开发也没有做完。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;适当地追求代码覆盖率：每一行的代码都覆盖了，但是程序未必正确。要确保程序覆盖了不同的程序状态和各种组合条件。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果团队成员碰到了一个有普遍意义的bug,  应该建立一个测试用例抓住以后将会出现的类似的bug。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;测试：多走一步，多考虑一层。如果程序运行了一星期不退出，如果用户的屏幕分辨率再提高一个档次，这个程序会出什么可能的错误?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（带领团队）了解用户的期望值，稍稍超出用户的期望值，让用户有惊喜。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(带领团队) 不要停留在被动地收集需求，要挖掘需求。真正的需求可能被过时的假设、对用户的误解或其他因素所遮挡。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（带领团队）把所有的术语和项目相关的名词、缩写等都放在一个地方。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（带领团队）不要依赖于某个人的手动操作，而是要把这些操作都做成有相关权限的人士都能运行的脚本。这样就不会出现因为某人休假而项目被卡住的情况。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（带领团队）要让重用变得更容易。一个软件团队要创造一种环境，让软件的重用变得更容易。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;（带领团队）在每一次迭代之后，都要总结经验，让下一次迭代的日程安排更可靠。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;备注：&lt;/p&gt;

&lt;p&gt;其他上面的内容，其他地方也有转载和评论：&lt;a href=&quot;http://www.cnblogs.com/xinz/p/3852177.html&quot;&gt;邹欣的教学博客&lt;/a&gt;和&lt;a href=&quot;http://www.zhihu.com/question/19560521/answer/31141681&quot;&gt;知乎出版人周筠&lt;/a&gt;。&lt;/p&gt;

</content>
   </entry>
   
   <entry>
     <title>产品设计：概要</title>
     <link href="http://ningg.github.com/product-design"/>
     <updated>2014-09-28T00:00:00+08:00</updated>
     <id>http://ningg.github.com/product-design</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;最近要做一个系统，包含了三部分：数据收集、分析处理、结果展示交互；其中，系统用户关心的只有&lt;code&gt;结果展示交互&lt;/code&gt;模块；这一模块的设计，直接决定了&lt;code&gt;分析处理&lt;/code&gt;模块中，数据结果的存储形式。现在的问题是，需要先设计好&lt;code&gt;结果展示交互&lt;/code&gt;模块，这样才能保证&lt;code&gt;分析处理&lt;/code&gt;模块开发完成。&lt;/p&gt;

&lt;p&gt;冷静想一想，上面问题的产生，根源是：没有按照正常的互联网产品开发流程，来推进项目。由于系统规模并不大，现在调整为互联网产品的开发流程也不迟。没有相关的人才？我是干什么的，需要什么，就能会什么，搞起。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;产品的诞生流程&lt;/h2&gt;

&lt;p&gt;直接点，当前我对这些产品原型什么的都没有概念，之前在某处的时候，见过产品开发的基本流程：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;产品/设计狮，出一版PSD；&lt;/li&gt;
  &lt;li&gt;研发工程狮，开始开发；说点细节：
    &lt;ul&gt;
      &lt;li&gt;核心攻城狮，敲定表格字段；&lt;/li&gt;
      &lt;li&gt;核心攻城狮，搭起前后端分离的整体架构；&lt;/li&gt;
      &lt;li&gt;所有攻城狮，一起编码实现业务逻辑；&lt;/li&gt;
      &lt;li&gt;Review代码、结伴编程，保证代码质量；&lt;/li&gt;
      &lt;li&gt;夜黑风高时，系统上线（如果是测试环境，则，不用赶在晚上了）；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;整个运营团队、开发团队一边使用系统，一边提交改进意见（包含当前系统截图）；&lt;/li&gt;
  &lt;li&gt;系统细节调整、更新；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从上面的清单中，可以看出步骤2的工作量较大，其实不是这样的，只是自己以前参与了工程的研发，所以知道的研发方面比较细节的东西；实际上，每一步骤，都包含了差不多的工作量，有的脑力多一些，有的体力多一些。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;关注点&lt;/h2&gt;

&lt;p&gt;进行线上产品设计，关注点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;色彩搭配&lt;/li&gt;
  &lt;li&gt;字体选择&lt;/li&gt;
  &lt;li&gt;语言风格&lt;em&gt;（严肃？活泼？）&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;视觉元素&lt;em&gt;（这是什么？）&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;方法&lt;/h2&gt;

&lt;p&gt;当前进行产品设计时，常用的方法如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;手绘草图：设计变得自由；&lt;/li&gt;
  &lt;li&gt;线框图：？&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://udc.weibo.com/&quot;&gt;weibo UDC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mux.baidu.com/&quot;&gt;baidu MUX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ued.taobao.org/&quot;&gt;taobao UED&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.woshipm.com/tag/ued&quot;&gt;woshipm UED&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cued.xunlei.com/&quot;&gt;xunlei CUED&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ued.orzk.com/&quot;&gt;orzk UED&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ued.ctrip.com/blog/&quot;&gt;ctrip UED&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://uedc.163.com/&quot;&gt;163 UEDC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ued.sina.com/&quot;&gt;sina UED&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cdc.tencent.com/&quot;&gt;tencent CDC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mxd.tencent.com/&quot;&gt;tencent MXD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://isux.tencent.com/&quot;&gt;tencent ISUX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://fex.baidu.com/&quot;&gt;baidu FEX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
 
</feed>
